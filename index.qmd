# Proposal and Outline

Results across scientific studies vary drastically even when studying the same underlying phenomena. Sometimes these can be accounted for by some study-level characteristic (i.e., methodology, population, etc.) or it can be accounted for by variations in statistical artifacts such as measurement error or selection effects. Not only does the heterogeneity increase in the presence of statistical artifacts, but artifacts also induce systematic biases that can cause inaccurate results. Artifacts restrict our ability to draw meaningful inferences from scientific results, therefore it is important to apply corrections in order to obtain unbiased estimates.

**Overview.** The goal of the general exam is to turn it into an online open source e-textbook similar to [this one on meta-analysis in R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/) by Mathias Harrer and colleagues. The e-text will be split into two parts: artifact corrections and their application to meta-analysis. Each type of artifact will have a section of its own that details the history, methodology, and its implementation in R and Python. Another section will be dedicated to applying artifact corrections to meta-analysis, which is the type of research where the corrections are most used. The meta-analysis section will also contain the four parts (history, methodology, assumptions, and implementation in R). See [this webpage](https://MatthewBJane.github.io/artifact-corrections) where I cataloged equations and code that will go into this general exam. I also have started writing the [unreliability section](unreliability.qmd#unreliability) so you can start get an idea of how this will look.

**History.** The history part will provide an overview of the literature for that artifact correction. It will note where the correction first was introduced, the adjustments people have made since then, as well as studies where the correction has been applied (most likely in a meta-analysis). Also if applicable, provide examples of where these meta-analyses have utilized such corrections (e.g., Roth et al., 2015) Methodology. The methodological part will review the correction equations (for point estimates and standard errors) and how they were derived for each artifact correction. Artifact corrections will be applied to both correlation coefficients and standardized mean differences (repeated measures and independent samples). The methodological part for the meta-analysis section will discuss how heterogeneity, credibility/confidence intervals, and averages are calculated in the context of artifact corrections. It will also touch on competing approaches (Raju et al., 1983) to the traditional artifact correction approaches (Hunter & Schmidt, 2004).

**Assumptions.** Each artifact correction contains assumptions that must be met in order to obtain unbiased estimate of the true population effect size. This part will discuss each of these assumptions, when each matter, and what simulation studies have found in regard to violation of assumptions. Assumptions at the meta-analytic level of analysis (e.g., independence between artifacts and moderators) will also be discussed.

**Implementation in R and Python.** Artifact corrections are only useful if you can apply them. Since many of the corrections are quite complex, it is important that these can be implemented easily in an open-source software such as R and Python. Each correction will be supplemented with code using R and Python with some additional packages like psychmeta [@dahlke2019]. For statistical analyses and especially for meta-analysis, R has tremendous support in terms of useful packages and a large community that makes it a highly flexible and powerful language. Python is a general-purpose programming language that is a more popular than R. For an example of how this will be done see this section on [estimating reliability in R and Python.](unreliability.qmd#estimating-reliability-code)

