[
  {
    "objectID": "unreliability.html#introduction",
    "href": "unreliability.html#introduction",
    "title": "5  Unreliability",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn general terms, measurement is the process of quantifying an attribute or characteristic of something. In scientific measurement, the measurand is the quantity or the attribute we intend to measure. In the psychological sciences, measurands usually take the form of constructs such as intelligence or anxiety. The goal of measurement is to produce quantities (i.e., scores) that accurately reflect the measurand. It is important to note that measures are not all created equal, some perform better than others. Ideally, measures should produce scores that are consistent and repeatable, this is referred to as the reliability of a measure. A high quality measure should produce highly reliable scores. This section will review what reliability is in theory, how to estimate reliability, and how to adjust effect sizes for measurement error."
  },
  {
    "objectID": "unreliability.html#sec-true-score-theory",
    "href": "unreliability.html#sec-true-score-theory",
    "title": "5  Unreliability",
    "section": "5.2 Reliability in True Score Theory",
    "text": "5.2 Reliability in True Score Theory\nTrue score theory (or classical test theory) is a mathematical formalization of scores obtained from measurements. The true score model assumes that each person (or animal), \\(p\\) has a true score, \\(t_p\\), that stays constant over measurements. Observed scores, \\(x_{pf}\\), can vary between forms (\\(f\\)) of the measure, \\(m\\). This variation is due to measurement-specific error, \\(e_{pf}\\).\n\\[\nx_{pf} = t_p+e_{pf}\n\\]\nThe true score can be defined as the expected value (i.e., the mean) of observed scores over an infinite number of repeated measurements such that, \\(\\mathbb{E}_{f\\rightarrow\\infty}[x_{f}]=t\\). It is also assumed that the expectation of measurement-specific error is zero, \\(\\mathbb{E}_{f\\rightarrow\\infty}[e_{f}]=0\\). It follows from these assumptions that the covariance between errors and true scores is zero (\\(\\sigma_{et}=0\\)) and the covariance between error scores in parallel measurements is zero (\\(\\sigma_{e e'}=0\\)). The independence between true scores and errors provide convenient parsing of the variance in observed scores (\\(\\sigma^2_{x}\\)) into components of variance in true scores (\\(\\sigma_t^2\\)) and errors (\\(\\sigma_{e}^2\\)),\n\\[\n\\sigma_{x}^2 = \\sigma_t^2 + \\sigma_{e}^2\n\\tag{5.1}\\] If \\(\\sigma_{e}^2 &gt; 0\\) then the measurement has imperfect reliability, that is, observed scores are not identical to true scores. In practice, this is almost always the case. Reliability can be defined as the square correlation between observed scores and true scores, \\(r_{xt}^2\\), or the correlation between observed scores in parallel measurements, \\(r_{xx'}=r_{xt}^2\\).\n\n\n\nStructural model illustrating the relationship between true scores, observed scores, and error scores. The pink circle labeled \\(t\\) indicates the true scores, the blue squares labeled with \\(x\\) and \\(x'\\) represent observed scores on parallel measurements, and the red \\(e\\) denotes error. Correlations between \\(t\\), \\(x\\), and \\(x'\\) are in terms of reliability (\\(r_{xx'}\\)). Note that \\(\\sqrt{r_{xx'}}=r_{xt}\\).\n\n\nGiven that errors do not co-vary between parallel measurements and true scores are held constant over measurements, it becomes apparent that the covariance between observed scores produced from parallel measurements must solely be attributable to true score variance, \\(\\sigma_{xx'}=\\sigma_t^2\\). The covariance in observed scores can be standardized to obtain the correlation coefficient between parallel measurements (i.e., the reliability), such that,\n\\[r_{xx'}=\\frac{\\sigma_{xx'}}{\\sigma_x\\sigma_{x'}}  = \\frac{\\sigma_t^2}{\\sigma^2_{x}}\\]\nTherefore reliability can be expressed in a few forms different forms\n\\[\nr_{xx'} =r^2_{xt} = \\frac{\\sigma_t^2}{\\sigma_t^2+\\sigma_{e}^2} = \\frac{\\sigma_t^2}{\\sigma_{x}^2}\n\\tag{5.2}\\]\nIn the literature, the correlation between observed and true scores, \\(r_{xt}\\), is often referred to as the “measure quality index” (Hunter and Schmidt 1990), however measure quality encompasses both reliability and validity. Validity A measure can demonstrate high reliability even though the scores produced by the measure do not accurately reflect the measurand (the quantity that we are intending to measure). For example, if an individual were to step on a weight scale with shoes on, the weight presented on the scale would be highly reliable, namely, if the individual were to repeat this process, they would achieve highly similar results. Nevertheless, the observed weight is systematically biased upward by the weight of the shoes. Therefore if a measure is reliable it does not logically follow that the measure is necessarily valid."
  },
  {
    "objectID": "unreliability.html#estimating-reliability",
    "href": "unreliability.html#estimating-reliability",
    "title": "5  Unreliability",
    "section": "5.3 Estimating Reliability",
    "text": "5.3 Estimating Reliability\nIn practice, reliability must be estimated through indirect methods, since true scores and errors are unknown. Their are many estimators that can be used however, we will go over three of the most common approaches: coefficient alpha, split-half, and test-retest reliability.\n\nInternal Consistency Estimators\nMaybe the most conventionally reported reliability estimator in the psychological sciences is coefficient alpha, also referred to as Cronbach’s alpha or internal consistency. Alpha has the benefit of being computationally convenient, but it also brings along many assumptions that are often violated in practice (Haertel 2006; Sijtsma 2009). Cronbach’s alpha, along with other internal consistency estimators, serves the purpose of assessing the reliability of composite measures comprising multiple components. Taking multiple measurements and then averaging tends to provide a better estimate of true values. For instance, let’s consider the case of Francis Galton (Galton 1907), who conducted a study involving 787 individuals estimating the weight of an ox. On average, each person’s estimate deviated by approximately 37 pounds from the actual weight of the ox, which was recorded as 1198 pounds. However, when all the guesses were averaged together, the combined estimate was 1207 pounds, just a 9 pound difference from the actual value. Averaging a number of noisy estimates provides a much more stable and reliable estimate. So to create a more stable composite score (\\(X\\)), we can take the score (\\(x_m\\)) from \\(k\\) measurements and average them such that, \\[\nX = \\frac{1}{k}(x_1 + x_2 +...+x_k)= \\frac{1}{k}\\sum^k_{m=1}x_m\n\\] Coefficient alpha is represents the reliability of this composite scores. Coefficient alpha only requires three parameters to calculate, the number of measurements (\\(k\\)), the variances of each items ( \\(\\sigma^2_{x_m}\\)), and the variance of the composite score (\\(\\sigma^2_{X}\\)),\n\\[\n_\\alpha r_{X X'} = \\frac{k}{k-1}\\left( 1 - \\frac{\\sum_{m=1}^k \\sigma^2_{x_m}}{\\sigma^2_{X}} \\right)\n\\]\n\n\n\n\n\nWith tighter assumptions (i.e., strictly parallel forms, Haertel 2006), the formula for coefficient alpha can be simplified to just two parameters: the number of measurements and the average correlation between measured scores (\\(\\bar{r}_{x_i x_j}\\), where \\(i\\neq j\\)). This formula is known as Spearman-Brown’s prophecy,\n\\[\n_\\text{sb} r_{XX'}= \\frac{k \\bar{r}_{x_i x_j}}{1+(k-1)\\bar{r}_{x_i x_j}}\n\\]\nThis can be simplified further if we we have two observed scores. This formulation is traditionally called split-half reliability:\n\\[\n_\\text{sh}r_{XX',}= \\frac{2r_{x_1 x_2}}{1+r_{x_1 x_2}}\n\\]\nAll of these reliability estimators measure internal consistency, therefore they do not account for error outside of the measurement-specific error. There are other sources of error that internal consistency reliability estimates do not account for, such as transient error or rater-specific error.\n\n\n\nStructural model illustrating internal consistency. The pink circle labeled \\(t\\) indicates the true scores, the blue squares, \\(x_{1...k}\\), represent the observed scores across multiple measurements, and the red \\(e\\) denotes error. The dark blue hexagon, \\(X\\), indicates a composite score as a sum of the observed scores (\\(x_{1...k}\\)). Note that \\(\\sqrt{r_{XX'}}=r_{Xt}\\).\n\n\n\nCalculating Internal Consistency in R and Python\n\nRPython\n\n\nLet us simulate a data set of 50 individuals where each observed score has the same true score with some error.\n\n#set seed\nset.seed(343)\n\n# set sample size\nn = 50\n\n# simulate data\nt = rnorm(n, 0, 1) # simulate true scores\nx1 = t + rnorm(n, 0, 1) # simulate observed scores for measurement 1\nx2 = t + rnorm(n, 0, 1) # simulate observed scores for measurement 2\nx3 = t + rnorm(n, 0, 1) # simulate observed scores for measurement 3\nx4 = t + rnorm(n, 0, 1) # simulate observed scores for measurement 4\n\n# calculate composite score\nX = x1 + x2 + x3 + x4\n\nCalculate Coefficient Alpha Reliability:\n\n# step 1. calculate variance of observed (measured) scores\nvar_xi = c(var(x1),var(x2),var(x3),var(x4))\n\n# step 2. calculate variance of composite score\nvar_X = var(X)\n\n# step 3. get number of items (k)\nk = length(var_xi)\n\n# step 4. calculate coefficient alpha reliability\nrXX_alpha = k / (k-1) * (1 - sum(var_xi)/var_X)\n\n# display reliability\nprint(round(rXX_alpha,3)) \n\n[1] 0.775\n\n\nCalculate Reliability via Spearman-Brown’s Prophecy:\n\n# step 1. get correlation matrix between all observed scores\ncorr_mat = cor(cbind(x1,x2,x3,x4))\n\n# step 2. average off-diagonal elements of matrix\ndiag(corr_mat) &lt;- NA\nrxixj = mean(corr_mat, na.rm = TRUE)\n\n# step 3. get number of items (k)\nk = dim(corr_mat)[1]\n\n# step 4. calculate Spearman-Brown reliability\nrXX_SB = k * rxixj / (1 + (k-1) * rxixj)\n\n# display reliability\nprint(round(rXX_SB,3)) \n\n[1] 0.775\n\n\nCalculate the Split-Half Reliability:\n\n# step 1. make composite scores for each half of the observed scores\nX1 = x1 + x2\nX2 = x3 + x4\n\n# step 2. calculate the correlation between the scores of both halves\nrX1X2 = cor(X1,X2)\n\n# step 3. calculate the split-half reliability\nrXX_SH = 2*rX1X2 / (1 + rX1X2)\n\n# display reliability\nprint(round(rXX_SH,3)) \n\n[1] 0.824\n\n\nTrue Reliability: Lets see how the results compare to the squared correlation of the true scores and our composite score (true reliability).\n\n# calculate true reliability\nrXt = cor(X,t)\n\n# display true reliability\nprint(round(rXt^2,3)) \n\n[1] 0.753\n\n\nIn this case, the reliability estimates do a fairly good job of estimating the true reliability of the observed scores.\n\n\nSimulate Data: Let us simulate a data set of 50 individuals where each observed score has the same true score with some error. To calculate the necessary statistics, we will import the numpy package.\n\n#import numpy\nimport numpy as np\n\n# set seed\nnp.random.seed(343)\n\n# set sample size\nn = 50\n\n# simulate data\nt = np.random.normal(0, 1, n) # simulate true scores\nx1 = t + np.random.normal(0, 1, n) # simulate observed scores for measurement 1\nx2 = t + np.random.normal(0, 1, n) # simulate observed scores for measurement 2\nx3 = t + np.random.normal(0, 1, n) # simulate observed scores for measurement 3\nx4 = t + np.random.normal(0, 1, n) # simulate observed scores for measurement 4\n\n# calculate sum score\nX = x1 + x2 + x3 + x4\n\nCalculate Coefficient Alpha Reliability:\n\n# step 1. calculate variance of observed (measured) scores\nvar_xm = [np.var(x1),np.var(x2),np.var(x3),np.var(x4)]\n\n# step 2. calculate variance of composite score\nvar_X = np.var(X)\n\n# step 3. get number of items (k)\nk = len(var_xm)\n\n# step 4. calculate coefficient alpha reliability\nrXX_alpha = k / (k-1) * (1 - sum(var_xm)/var_X)\n\nprint(round(rXX_alpha,3)) \n\n0.769\n\n\nCalculate Reliability from Spearman-Brown’s prophecy formula:\n\n# step 1. get correlation matrix between all observed scores\ncorr_mat = np.corrcoef([x1,x2,x3,x4])\n\n# step 2. average off-diagonal elements of matrix\nrxixj = np.mean(corr_mat[~np.eye(k,dtype=bool)])\n\n# step 3. get number of items (k)\nk = len(corr_mat)\n\n# step 4. calculate Spearman-Brown reliability\nrXX_SB = k * rxixj / (1 + (k-1) * rxixj)\n\nprint(round(rXX_SB,3)) \n\n0.772\n\n\nCalculate Split-Half Reliability:\n\n# step 1. make composite scores for each half of the observed scores\nX1 = x1 + x2\nX2 = x3 + x4\n\n# step 2. calculate the correlation between the scores of both halves\nrX1X2 = np.corrcoef(X1,X2)[0,1]\n\n# step 3. calculate the split-half reliability\nrXX_SH = 2*rX1X2 / (1 + rX1X2)\n\n# display reliability\nprint(round(rXX_SH,3)) \n\n0.772\n\n\nLets see how the results compare to the squared correlation of the true scores and our composite score (true reliability).\n\n# calculate true reliability\nrXt = np.corrcoef(X,t)[0,1]\n\n# display reliability\nprint(round(rXt**2,3)) \n\n0.791\n\n\nIn this case, the reliability estimates do a fairly good job of estimating the true reliability of the observed scores. There are also functions within the psych package that allow you to easily calculate coefficient alpha among other reliability estimators\n\n\n\n\n\n\n5.3.1 Test-Retest Stability Estimator\nThere measurement errors that exist outside of the measurement instrument itself. Transient errors represent fluctuations in observed scores over time. These fluctuations, even if they are systematic (e.g., fatigue over the course of a single day), add extraneous within-person variance that can mask true scores (i.e., expectation of observed scores). For example, if a researcher wants to investigate how individuals differ in processing speed, then variation within an individual’s scores across multiple testing sessions would be considered error since the goal of the study is to investigate between-person variation. Considering transient fluctuations as error depends on the research goal, so it is important for researchers to take care in considering which variance components should be considered error in their study. To estimate test-retest reliability, we can compute the pearson correlation coefficient between the measurement at time 1 (\\(x_{T_{1}}\\)) and the second measurement at time 2 (\\(x_{T_{2}}\\)).\n\\[\n_\\text{tr}r_{xx'}= r_{x_{T_1}x_{T_2}}\n\\]\nNote that calculating the pearson correlation coefficient between time-points ignores systematic changes (e.g., practice effects).\n\n\n\n\n\n\nCalculating Test-Retest Reliability in R and Python\n\nRPython\n\n\nLets calculate test-retest reliability in R. First, we can simulate observed scores at two time points, \\(T_1\\) and \\(T_2\\). We can assume that the true scores remain constant between \\(T_1\\) and \\(T_2\\). Second, we can calculate the correlation between the observed scores at each time point (\\(r_{x_{T_1}x_{T_2}}\\)).\n\n# set seed\nset.seed(343)\n\n# set sample size\nn = 70\n\n# simulate true scores\nt = rnorm(n,0,1)\n\n# simulate scores at time 1\nxT1 = t + rnorm(n,0,.5)\n\n# simulate scores at time 2\nxT2 = t + rnorm(n,0,.5)\n\n# calculate test-retest reliability\nrxx = cor(xT1,xT2)\n\n# display reliability\nprint(round(rxx,3))\n\n[1] 0.803\n\n# compare with true reliability\nrxx_true = cor(xT1,t)^2\n\n# display true reliability\nprint(round(rxx_true,3))\n\n[1] 0.768\n\n\n\n\nLets calculate test-retest reliability in R. First, we can simulate observed scores at two time points, \\(T_1\\) and \\(T_2\\). We can assume that the true scores remain constant between \\(T_1\\) and \\(T_2\\). Second, we can calculate the correlation between the observed scores at each time point (\\(r_{x_{T_1}x_{T_2}}\\)).\n\n# import numpy\nimport numpy as np\n\n# set seed\nnp.random.seed(343)\n\n# set sample size\nn = 70\n\n# simulate 70 true scores\nt = np.random.normal(0,1,n)\n\n# simulate scores at time 1\nxT1 = t + np.random.normal(0,.5,n)\n\n# simulate scores at time 2\nxT2 = t + np.random.normal(0,.5,n)\n\n# calculate test-retest reliability\nrxx = np.corrcoef(xT1,xT2)[0,1]\n\n# display reliability\nprint(round(rxx,3))\n\n0.768\n\n# compare with true reliability\nrxx_true = np.corrcoef(xT1,t)[0,1]**2\n\n# display true reliability\nprint(round(rxx_true,3))\n\n0.731\n\n\n\n\n\n\n\n\n5.3.2 Sources of Measurement Error\nThere are many estimators of reliability beyond internal consistency and test-retest that account for different sources of error and hold different assumptions. There are many sources of measurement error that different estimators of reliability account for adapted from table 1 of Wiernik and Dahlke (2020) :\n\nRandom Response Error: Genuine randomness in responses. Examples include: motor errors and variation in response time.\nTime/Environment-Specific (Transient) Error: Fluctuations in scores as a result of the specific time or environment of the measurement. For instance, if researchers administered an ability test to a sample of undergraduate students throughout the course of a day, the student’s who complete the test at the end of the day will likely perform worse than participant’s who completed due to fatigue rather than ability. Errors due to illness, mood, hunger, environmental distractors, etc. all fall under the umbrella of transient errors.\nInstrument-Specific Error: Error due to the specific content or make-up of the measurement instrument. For example, a psychological scale using likert items participant’s idiosyncratic interpretations of questions and response options rather than their standing on the latent construct.\nRater/Observer-Specific Error: Errors induced by idiosyncratic biases of individual raters and rater by ratee interactions (e.g., Teacher A gives higher grades to students who stay after class).\n\nDifferent estimators of reliability account for different sources of measurement error therefore depending on the research design, it is important to carefully choose which reliability is most relevant for your use case. Note that even if two estimators account for the same types of measurement error, they likely hold different assumptions that may be violated in a given research context.\n\nTable 1. List of reliability coefficients and the sources of error they account for.\n\n\n\n\n\n\n\n\n\n\nEstimator\nDescription\nRandom Response Error\nTransient Error\nInstrument-Specific Error\nRater-Specific Error\n\n\n\n\nCoefficient Alpha\nInternal consistency coefficient for composite measures.\n✔️\n\n✔️\n\n\n\nCoefficient Omega\nInternal consistency coefficient for composite measures with specified factor structure.\n✔️\n\n✔️\n\n\n\nSplit-Half\nInternal consistency coefficient for measurements that are split into two halves.\n✔️\n\n✔️\n\n\n\nKuder-Richardson 20\nInternal consistency when observed scores are binary (special case of coefficient alpha).\n✔️\n\n✔️\n\n\n\nItem Response Theory Reliability\nReliability coefficient derived from item response theory (as opposed to classical test theory)\n✔️\n\n✔️\n\n\n\nInter-Rater/Inter-Observer Reliability\nConsistency in scoring between raters/observers.\n✔️\n\n\n✔️\n\n\nTest-Retest\nStability coefficient for repeated measurements across time\n✔️\n✔️\n\n\n\n\nDelayed Coefficient Alpha\nAverage of all possible split-half reliabilities\n✔️\n✔️\n✔️\n\n\n\nG-Coefficient\nReliability coefficient derived from generalizability theory (G-theory). Can incorporate any source of error if enough data is present.\n✔️\n✔️\n✔️\n✔️"
  },
  {
    "objectID": "unreliability.html#bias-in-correlation-coefficients",
    "href": "unreliability.html#bias-in-correlation-coefficients",
    "title": "5  Unreliability",
    "section": "5.4 Bias in Correlation Coefficients",
    "text": "5.4 Bias in Correlation Coefficients\nUnreliability induces systematic bias in effect size estimates such as correlation coefficients Spearman (1904). Lets say we have two observed scores $x$ and $y$,\n\\[\nx=x_t+e\n\\]\n\\[\ny=y_t+e\n\\]\nIn most research contexts, we would like to estimate the correlation between true scores where the correlation between true scores, \\(x_t\\) and \\(y_t\\) is \\[\n\\rho=\\frac{\\sigma_{x_ty_t}}{\\sigma_{x_t} \\sigma_{y_t}}\n\\]\nThe observed correlation differs only in that it standardizes the covariance by the product of the standard deviations of observed scores rather than true scores. The covariance of observed scores will be equivalent to the covariance of true scores assuming \\(\\sigma_{e_x e_y}=0\\) (see Section 5.2).\n\\[\nr=\\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y} = \\frac{\\sigma_{x_ty_t}}{\\sigma_x \\sigma_y}\n\\tag{5.3}\\]\nIn the presence of measurement error, the observed standard deviations (\\(\\sigma_x\\) and \\(\\sigma_y\\)) will be larger than the true standard deviations (\\(\\sigma_x\\) and \\(\\sigma_y\\)). Since the reliability is defined as the ratio of true variance to total observed variance (see Equation 5.2 ), we can see how reliability inflates the true standard deviation\n\\[\n\\sigma^2_x =\\sigma^2_{x_t} \\cdot \\frac{\\sigma^2_{x}}{\\sigma^2_{x_t}} = \\sigma^2_{x_t}\\cdot\\frac{1}{r_{xx'}} = \\frac{\\sigma^2_{xt}}{r_{xx'}}\n\\]\nTherefore,\n\\[\n\\sigma_x = \\frac{\\sigma_{x_t}}{\\sqrt{r_{xx'}}}\n\\]\nIf we plug this back into equation Equation 5.3 then we can see how the observed correlation changes as a function of reliability and how it is biased from the true score correlation \\(\\rho\\):\n\\[\n\\begin{align}\nr &= \\frac{\\sigma_{x_ty_t}}{\\sigma_{x} \\sigma_{y}}\n\\\\ &= \\frac{\\sigma_{x_ty_t}}{\\left[\\frac{\\sigma_{x_t}}{\\sqrt{r_{xx'}}} \\right] \\left[ \\frac{\\sigma_{y_t}}{\\sqrt{r_{yy'}}} \\right] }\n\\\\ &= \\frac{\\sigma_{x_ty_t}}{\\sigma_{x_t}\\sigma_{y_t}} \\cdot \\sqrt{r_{yy'}}\\sqrt{r_{xx'}}\n\\\\ &= \\rho\\sqrt{r_{yy'}}\\sqrt{r_{xx'}}\n\\end{align}\n\\] This equation was first provided by Spearman (1904) For cases where measurement error only affects one of the two variables, the attenuation\n\nrho = .5\nX_true = rnorm(50,0,1)\nY_true = rho * X_true + rnorm(50,0,sqrt(1-rho^2))\n\nh1 &lt;- ggplot(data = NULL, aes(x = X_true, y = Y_true)) +\n    geom_point(color = main_color_blue, fill = main_color_blue) +\n    scale_x_continuous(limits = c(-3,3)) +\n    scale_y_continuous(limits = c(-3,3)) +\n    xlab(\"True x\")+\n    ylab(\"True y\") +\n    ggtitle('True Score Correlation', subtitle = TeX(\"$\\\\rho =$ .50\")) +\n    theme(aspect.ratio = 1,\n          panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank(),\n          title = element_text(color = text_color_blue),\n          panel.background = element_rect(fill = panel_color_blue),\n          panel.border = element_rect(fill = NA, color = border_color_blue,linewidth=1.2),\n          axis.title = element_text(size=14,family = \"Arial\", color = text_color_blue),\n          axis.text.x = element_text(size=12, color = text_color_blue),\n          axis.text.y = element_text(size=12, color = text_color_blue),\n          axis.ticks = element_line(color = border_color_blue,linewidth=1),\n          legend.position = \"none\") \n\nX_obs = X_true + rnorm(50,0, sqrt( 1/.8 - 1) )\nY_obs = Y_true + rnorm(50,0, sqrt( 1/.8 - 1) )\n\nh4 &lt;- ggplot(data = NULL, aes(x = X_obs, y = Y_obs)) +\n    geom_point(color = main_color_blue, fill = main_color_blue) +\n    scale_x_continuous(limits = c(-3,3)) +\n    scale_y_continuous(limits = c(-3,3)) +\n    xlab(\"True x\")+\n    ylab(\"True y\") +\n    ggtitle('True Score Correlation', subtitle = TeX(\"$\\\\rho =$ .50\")) +\n    theme(aspect.ratio = 1,\n          panel.grid.minor = element_blank(),\n          panel.grid.major = element_blank(),\n          title = element_text(color = text_color_blue),\n          panel.background = element_rect(fill = panel_color_blue),\n          panel.border = element_rect(fill = NA, color = border_color_blue,linewidth=1.2),\n          axis.title = element_text(size=14,family = \"Arial\", color = text_color_blue),\n          axis.text.x = element_text(size=12, color = text_color_blue),\n          axis.text.y = element_text(size=12, color = text_color_blue),\n          axis.ticks = element_line(color = border_color_blue,linewidth=1),\n          legend.position = \"none\") \n\nh1 + h2\n\n\n\n\n\n\n\n\n\nUltimately, the inflation of the observed standard deviation As described in Equation 5.2, the reliability can be interpreted as the ratio of total observed variance to\nFortunately, there are corrections that can be applied to effect size estimates to account for bias."
  },
  {
    "objectID": "unreliability.html#corrections-for-bias-in-correlations",
    "href": "unreliability.html#corrections-for-bias-in-correlations",
    "title": "5  Unreliability",
    "section": "5.5 Corrections for Bias in Correlations",
    "text": "5.5 Corrections for Bias in Correlations"
  },
  {
    "objectID": "unreliability.html#correction-for-unreliability-in-correlation-coefficients",
    "href": "unreliability.html#correction-for-unreliability-in-correlation-coefficients",
    "title": "5  Unreliability",
    "section": "5.6 Correction for Unreliability in Correlation Coefficients",
    "text": "5.6 Correction for Unreliability in Correlation Coefficients\n\n5.6.1 Bias in Correlations\nMeasurements that produce scores with low reliability by definition have low correlations between observed scores on parallel forms of the measurement. It should should not be expected that any other variable would.\n\n\n5.6.2 Bias in Standardized Mean Difference\n\n5.6.2.1 Between-Group Standardized Mean Difference\n\n\n5.6.2.2 Repeated Measures Standardized Mean Difference\n(Haertel 2006)\n(Schmidt, Le, and Ilies 2003)\n(Gliem and Gliem 2003)\n(Bobko, Roth, and Bobko 2001)\n(Mendoza and Mumford 1987)\n(Brennan 2010)\n(Viswanathan 2005)\n(Viswesvaran et al. 2014)\n(Sijtsma 2009)\n(Charles 2005)\n(Spearman 1904)\n\n\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001. “Correcting the Effect Size of d for Range Restriction and Unreliability.” Organizational Research Methods - ORGAN RES METHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nBrennan, Robert L. 2010. “Generalizability Theory and Classical Test Theory.” Applied Measurement in Education 24 (1): 1–21. https://doi.org/10.1080/08957347.2011.532417.\n\n\nCharles, Eric. 2005. “The Correction for Attenuation Due to Measurement Error: Clarifying Concepts and Creating Confidence Sets.” Psychological Methods 10 (July): 206–26. https://doi.org/10.1037/1082-989X.10.2.206.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75 (1949): 450–51. https://doi.org/10.1038/075450a0.\n\n\nGliem, Joseph A., and Rosemary R. Gliem. 2003. “Calculating, Interpreting, And Reporting Cronbach’s Alpha Reliability Coefficient For Likert-Type Scales.” https://scholarworks.iupui.edu/handle/1805/344.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nMendoza, Jorge L., and Michael Mumford. 1987. “Corrections for Attenuation and Range Restriction on the Predictor.” Journal of Educational Statistics 12 (3): 282–93. https://doi.org/10.3102/10769986012003282.\n\n\nSchmidt, Frank L., Huy Le, and Remus Ilies. 2003. “Beyond Alpha: An Empirical Examination of the Effects of Different Sources of Measurement Error on Reliability Estimates for Measures of Individual-Differences Constructs.” Psychological Methods 8: 206–24. https://doi.org/10.1037/1082-989X.8.2.206.\n\n\nSijtsma, Klaas. 2009. “On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s Alpha.” Psychometrika 74 (1): 107–20. https://doi.org/10.1007/s11336-008-9101-0.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” International Journal of Epidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nViswanathan, Madhu. 2005. Measurement Error and Research Design. SAGE.\n\n\nViswesvaran, Chockalingam, Deniz S. Ones, Frank L. Schmidt, Huy Le, and In-Sue Oh. 2014. “Measurement Error Obfuscates Scientific Knowledge: Path to Cumulative Knowledge Requires Corrections for Unreliability and Psychometric Meta-Analyses.” Industrial and Organizational Psychology 7 (4): 507–18. https://doi.org/10.1017/S1754942600006799.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "misclassification.html",
    "href": "misclassification.html",
    "title": "6  Misclassification",
    "section": "",
    "text": "(Chyou 2007)\n(Wiernik and Dahlke 2020)\n(Hunter and Schmidt 1990)\n\n\n\n\nChyou, Po-Huang. 2007. “Patterns of Bias Due to Differential Misclassification by Casecontrol Status in a Casecontrol Study.” European Journal of Epidemiology 22 (1): 7–17. https://doi.org/10.1007/s10654-006-9078-x.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "artificial_dichotomization.html",
    "href": "artificial_dichotomization.html",
    "title": "7  Artificial Dichotomization",
    "section": "",
    "text": "(Naggara et al. 2011)\n(Russell, Pinto, and Bobko 1991)\n(Digby 1983)\n(Maxwell and Delaney 1993)\n(Hunter and Schmidt 1990)\n(Vargha et al. 1996)\n(Royston, Altman, and Sauerbrei 2006)\n(Peters and Voorhis 1940)\n(Bonett and Price 2005)\n(Ulrich and Wirtz 2004)\n(Muthén and Hofacker 1988)\n(MacCallum et al. 2002)\n\n\n\n\nBonett, Douglas G., and Robert M. Price. 2005. “Inferential Methods for the Tetrachoric Correlation Coefficient.” Journal of Educational and Behavioral Statistics 30 (2): 213–25. https://www.jstor.org/stable/3701350.\n\n\nDigby, P. G. N. 1983. “Approximating the Tetrachoric Correlation Coefficient.” Biometrics 39 (3): 753–57. https://doi.org/10.2307/2531104.\n\n\nHunter, John, and Frank Schmidt. 1990. “Dichotomization of Continuous Variables: The Implications for Meta-Analysis.” Journal of Applied Psychology 75 (June): 334–49. https://doi.org/10.1037/0021-9010.75.3.334.\n\n\nMacCallum, Robert C., Shaobo Zhang, Kristopher J. Preacher, and Derek D. Rucker. 2002. “On the Practice of Dichotomization of Quantitative Variables.” Psychological Methods 7: 19–40. https://doi.org/10.1037/1082-989X.7.1.19.\n\n\nMaxwell, Scott, and Harold Delaney. 1993. “Bivariate Median Splits and Spurious Statistical Significance.” Psychological Bulletin 113 (January): 181–90. https://doi.org/10.1037/0033-2909.113.1.181.\n\n\nMuthén, Bengt, and Charles Hofacker. 1988. “Testing the Assumptions Underlying Tetrachoric Correlations.” Psychometrika 53 (4): 563–77. https://doi.org/10.1007/BF02294408.\n\n\nNaggara, O., J. Raymond, F. Guilbert, D. Roy, A. Weill, and D. G. Altman. 2011. “Analysis by Categorizing or Dichotomizing Continuous Variables Is Inadvisable: An Example from the Natural History of Unruptured Aneurysms.” American Journal of Neuroradiology 32 (3): 437–40. https://doi.org/10.3174/ajnr.A2425.\n\n\nPeters, Charles C., and Walter R. Van Voorhis. 1940. “Further Methods of Correlation.” In, 362–403. New York, NY, US: McGraw-Hill Book Company. https://doi.org/10.1037/13596-013.\n\n\nRoyston, Patrick, Douglas G. Altman, and Willi Sauerbrei. 2006. “Dichotomizing Continuous Predictors in Multiple Regression: A Bad Idea.” Statistics in Medicine 25 (1): 127–41. https://doi.org/10.1002/sim.2331.\n\n\nRussell, Craig J., Jeffrey K. Pinto, and Philip Bobko. 1991. “Appropriate Moderated Regression and Inappropriate Research Strategy: A Demonstration of Information Loss Due to Scale Coarseness” 15 (3): 257–66. https://doi.org/10.1177/014662169101500305.\n\n\nUlrich, Rolf, and Markus Wirtz. 2004. “On the Correlation of a Naturally and an Artificially Dichotomized Variable.” British Journal of Mathematical and Statistical Psychology 57 (2): 235–51. https://doi.org/10.1348/0007110042307203.\n\n\nVargha, András, Tamás Rudas, Harold D. Delaney, and Scott E. Maxwell. 1996. “Dichotomization, Partial Correlation, and Conditional Independence.” Journal of Educational and Behavioral Statistics 21 (3): 264–82. https://doi.org/10.2307/1165272."
  },
  {
    "objectID": "scale_coarseness.html",
    "href": "scale_coarseness.html",
    "title": "8  Scale Coarseness",
    "section": "",
    "text": "(Symonds 1924)\n(Aguinis, Pierce, and Culpepper 2009)\n(Krieg 1999)\n\n\n\n\nAguinis, Herman, Charles A Pierce, and Steven A Culpepper. 2009. “Scale Coarseness as a Methodological Artifact,” September.\n\n\nKrieg, Edward F. 1999. “Biases Induced by Coarse Measurement Scales.” Educational and Psychological Measurement 59 (5): 749–66. https://doi.org/10.1177/00131649921970125.\n\n\nSymonds, P. M. 1924. “On the Loss of Reliability in Ratings Due to Coarseness of the Scale.” Journal of Experimental Psychology 7 (6): 456–61. https://doi.org/10.1037/h0074469."
  },
  {
    "objectID": "direct_range_restriction.html",
    "href": "direct_range_restriction.html",
    "title": "9  Direct Range Restriction/Enhancement",
    "section": "",
    "text": "(Bobko, Roth, and Bobko 2001)\n(Sackett and Yang 2000)\n(Law, Schmidt, and Hunter 1994)\n(Thorndike 1949)\n(Hunter and Schmidt 1990)\n(Wiernik and Dahlke 2020)\n\n\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001. “Correcting the Effect Size of d for Range Restriction and Unreliability.” Organizational Research Methods - ORGAN RES METHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nLaw, Kenneth, Frank Schmidt, and John Hunter. 1994. “Nonlinearity of Range Corrections in Meta-Analysis: Test of an Improved Procedure.” Journal of Applied Psychology 79 (December): 978–86. https://doi.org/10.1037/0021-9010.79.6.978.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nThorndike, Robert L. 1949. Personnel Selection; Test and Measurement Techniques. Personnel Selection; Test and Measurement Techniques. Oxford, England: Wiley.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "indirect_range_restriction.html",
    "href": "indirect_range_restriction.html",
    "title": "10  Indirect Range Restriction/Enhancement",
    "section": "",
    "text": "(Dahlke and Wiernik 2020)\n(Hunter and Schmidt 1990)\n(Thorndike 1949)\n(Bobko, Roth, and Bobko 2001)\n(Sackett and Yang 2000)\n\n\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001. “Correcting the Effect Size of d for Range Restriction and Unreliability.” Organizational Research Methods - ORGAN RES METHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2020. “Not Restricted to Selection Research: Accounting for Indirect Range Restriction in Organizational Research.” Organizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nThorndike, Robert L. 1949. Personnel Selection; Test and Measurement Techniques. Personnel Selection; Test and Measurement Techniques. Oxford, England: Wiley."
  },
  {
    "objectID": "metaanalysis_intro.html#fixed-effects-model",
    "href": "metaanalysis_intro.html#fixed-effects-model",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.1 Fixed Effects Model",
    "text": "11.1 Fixed Effects Model"
  },
  {
    "objectID": "metaanalysis_intro.html#random-effects-model",
    "href": "metaanalysis_intro.html#random-effects-model",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.2 Random Effects Model",
    "text": "11.2 Random Effects Model\n(Borenstein et al. 2010)\n(Hartung 1999)\n(Cooper, Hedges, and Valentine 2009)\n(DerSimonian and Kacker 2007)\n\n\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and Random-Effects Models for Meta-Analysis.” Research Synthesis Methods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nCooper, Harris M., Larry V. Hedges, and Jeff C. Valentine, eds. 2009. The Handbook of Research Synthesis and Meta-Analysis. 2nd ed. New York: Russell Sage Foundation.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects Model for Meta-Analysis of Clinical Trials: An Update.” NIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nHartung, Joachim. 1999. “An Alternative Method for Meta-Analysis.” Biometrical Journal 41 (8): 901–16. https://doi.org/10.1002/(SICI)1521-4036(199912)41:8&lt;901::AID-BIMJ901&gt;3.0.CO;2-W."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#individual-artifact-correction-model",
    "href": "artifact_correction_metaanalysis.html#individual-artifact-correction-model",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.1 Individual Artifact Correction Model",
    "text": "12.1 Individual Artifact Correction Model"
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#artifact-distribution-model",
    "href": "artifact_correction_metaanalysis.html#artifact-distribution-model",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.2 Artifact Distribution Model",
    "text": "12.2 Artifact Distribution Model"
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#alternative-methods",
    "href": "artifact_correction_metaanalysis.html#alternative-methods",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.3 Alternative Methods",
    "text": "12.3 Alternative Methods\n(Hunter and Schmidt 1990)\n(Wiernik and Dahlke 2020)\n(Schmidt and Hunter 1977)\n(Murphy 2003)\n(Viswesvaran and Ones 1995)\n(Raju and Burke 1983)\n(Callender and Osburn 1980)\n\n\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test of a New Model for Validity Generalization.” Journal of Applied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nMurphy, Kevin R. 2003. Validity Generalization: A Critical Review. Psychology Press.\n\n\nRaju, Nambury, and Michael Burke. 1983. “Two Procedures for Studying Validity Generalization.” Journal of Applied Psychology 68 (August): 382–95. https://doi.org/10.1037/0021-9010.68.3.382.\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General Solution to the Problem of Validity Generalization.” Journal of Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nViswesvaran, Chockalingam, and Deniz S. Ones. 1995. “Theory Testing: Combining Psychometric Meta-Analysis and Structural Equations Modeling.” Personnel Psychology 48 (4): 865–85. https://doi.org/10.1111/j.1744-6570.1995.tb01784.x.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aguinis, Herman, Charles A Pierce, and Steven A Culpepper. 2009.\n“Scale Coarseness as a Methodological Artifact,” September.\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001.\n“Correcting the Effect Size of d for Range Restriction and\nUnreliability.” Organizational Research Methods - ORGAN RES\nMETHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nBonett, Douglas G., and Robert M. Price. 2005. “Inferential\nMethods for the Tetrachoric Correlation Coefficient.” Journal\nof Educational and Behavioral Statistics 30 (2): 213–25. https://www.jstor.org/stable/3701350.\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah\nR. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and\nRandom-Effects Models for Meta-Analysis.” Research Synthesis\nMethods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nBrennan, Robert L. 2010. “Generalizability Theory and Classical\nTest Theory.” Applied Measurement in Education 24 (1):\n1–21. https://doi.org/10.1080/08957347.2011.532417.\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test\nof a New Model for Validity Generalization.” Journal of\nApplied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nCharles, Eric. 2005. “The Correction for Attenuation Due to\nMeasurement Error: Clarifying Concepts and Creating Confidence\nSets.” Psychological Methods 10 (July): 206–26. https://doi.org/10.1037/1082-989X.10.2.206.\n\n\nChyou, Po-Huang. 2007. “Patterns of Bias Due to Differential\nMisclassification by Casecontrol Status in a\nCasecontrol Study.” European Journal of\nEpidemiology 22 (1): 7–17. https://doi.org/10.1007/s10654-006-9078-x.\n\n\nCooper, Harris M., Larry V. Hedges, and Jeff C. Valentine, eds. 2009.\nThe Handbook of Research Synthesis and Meta-Analysis. 2nd ed.\nNew York: Russell Sage Foundation.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R\nPackage for Psychometric Meta-Analysis.” Applied\nPsychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\n———. 2020. “Not Restricted to Selection Research: Accounting for\nIndirect Range Restriction in Organizational Research.”\nOrganizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects\nModel for Meta-Analysis of Clinical Trials: An Update.”\nNIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nDigby, P. G. N. 1983. “Approximating the Tetrachoric Correlation\nCoefficient.” Biometrics 39 (3): 753–57. https://doi.org/10.2307/2531104.\n\n\nFisher, R. A. 1915. “Frequency Distribution of the Values of the\nCorrelation Coefficient in Samples from an Indefinitely Large\nPopulation.” Biometrika 10 (4): 507–21. https://doi.org/10.2307/2331838.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75\n(1949): 450–51. https://doi.org/10.1038/075450a0.\n\n\nGliem, Joseph A., and Rosemary R. Gliem. 2003. “Calculating,\nInterpreting, And Reporting Cronbach’s Alpha Reliability\nCoefficient For Likert-Type Scales.” https://scholarworks.iupui.edu/handle/1805/344.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHartung, Joachim. 1999. “An Alternative Method for\nMeta-Analysis.” Biometrical Journal 41 (8): 901–16. https://doi.org/10.1002/(SICI)1521-4036(199912)41:8&lt;901::AID-BIMJ901&gt;3.0.CO;2-W.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator\nof Effect Size and Related Estimators.” Journal of\nEducational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\n———. 1989. “An Unbiased Correction for Sampling Error in Validity\nGeneralization Studies.” Journal of Applied Psychology\n74 (3): 469–77. https://doi.org/10.1037/0021-9010.74.3.469.\n\n\nHunter, John E, and Ronda F Hunter. n.d. “Validity and Utility of\nAlternative Predictors of Job Performance.”\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of\nmeta-analysis: correcting error and bias in research findings.\nNewbury Park: Sage Publications.\n\n\nHunter, John, and Frank Schmidt. 1990. “Dichotomization of\nContinuous Variables: The Implications for Meta-Analysis.”\nJournal of Applied Psychology 75 (June): 334–49. https://doi.org/10.1037/0021-9010.75.3.334.\n\n\nKrieg, Edward F. 1999. “Biases Induced by Coarse Measurement\nScales.” Educational and Psychological Measurement 59\n(5): 749–66. https://doi.org/10.1177/00131649921970125.\n\n\nLaw, Kenneth, Frank Schmidt, and John Hunter. 1994. “Nonlinearity\nof Range Corrections in Meta-Analysis: Test of an Improved\nProcedure.” Journal of Applied Psychology 79 (December):\n978–86. https://doi.org/10.1037/0021-9010.79.6.978.\n\n\nLin, Lifeng. 2018. “Bias Caused by Sampling Error in Meta-Analysis\nwith Small Sample Sizes.” PLOS ONE 13 (9): e0204056. https://doi.org/10.1371/journal.pone.0204056.\n\n\nMacCallum, Robert C., Shaobo Zhang, Kristopher J. Preacher, and Derek D.\nRucker. 2002. “On the Practice of Dichotomization of Quantitative\nVariables.” Psychological Methods 7: 19–40. https://doi.org/10.1037/1082-989X.7.1.19.\n\n\nMaxwell, Scott, and Harold Delaney. 1993. “Bivariate Median Splits\nand Spurious Statistical Significance.” Psychological\nBulletin 113 (January): 181–90. https://doi.org/10.1037/0033-2909.113.1.181.\n\n\nMendoza, Jorge L., and Michael Mumford. 1987. “Corrections for\nAttenuation and Range Restriction on the Predictor.” Journal\nof Educational Statistics 12 (3): 282–93. https://doi.org/10.3102/10769986012003282.\n\n\nMurphy, Kevin R. 2003. Validity Generalization: A Critical\nReview. Psychology Press.\n\n\nMuthén, Bengt, and Charles Hofacker. 1988. “Testing the\nAssumptions Underlying Tetrachoric Correlations.”\nPsychometrika 53 (4): 563–77. https://doi.org/10.1007/BF02294408.\n\n\nNaggara, O., J. Raymond, F. Guilbert, D. Roy, A. Weill, and D. G.\nAltman. 2011. “Analysis by Categorizing or Dichotomizing\nContinuous Variables Is Inadvisable: An Example from the Natural History\nof Unruptured Aneurysms.” American Journal of\nNeuroradiology 32 (3): 437–40. https://doi.org/10.3174/ajnr.A2425.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of\nCertain Correlation Coefficients.” The Annals of Mathematical\nStatistics 29 (1): 201–11. https://www.jstor.org/stable/2237306.\n\n\nPeters, Charles C., and Walter R. Van Voorhis. 1940. “Further\nMethods of Correlation.” In, 362–403. New York, NY, US:\nMcGraw-Hill Book Company. https://doi.org/10.1037/13596-013.\n\n\nRaju, Nambury, and Michael Burke. 1983. “Two Procedures for\nStudying Validity Generalization.” Journal of Applied\nPsychology 68 (August): 382–95. https://doi.org/10.1037/0021-9010.68.3.382.\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A\nMeta-Analysis.”\n\n\nRoyston, Patrick, Douglas G. Altman, and Willi Sauerbrei. 2006.\n“Dichotomizing Continuous Predictors in Multiple Regression: A Bad\nIdea.” Statistics in Medicine 25 (1): 127–41. https://doi.org/10.1002/sim.2331.\n\n\nRussell, Craig J., Jeffrey K. Pinto, and Philip Bobko. 1991.\n“Appropriate Moderated Regression and Inappropriate Research\nStrategy: A Demonstration of Information Loss Due to Scale\nCoarseness” 15 (3): 257–66. https://doi.org/10.1177/014662169101500305.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range\nRestriction: An Expanded Typology.” Journal of Applied\nPsychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nSchmidt, Frank L., Huy Le, and Remus Ilies. 2003. “Beyond Alpha:\nAn Empirical Examination of the Effects of Different Sources of\nMeasurement Error on Reliability Estimates for Measures of\nIndividual-Differences Constructs.” Psychological\nMethods 8: 206–24. https://doi.org/10.1037/1082-989X.8.2.206.\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General\nSolution to the Problem of Validity Generalization.” Journal\nof Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nSijtsma, Klaas. 2009. “On the Use, the Misuse, and the Very\nLimited Usefulness of Cronbach’s Alpha.”\nPsychometrika 74 (1): 107–20. https://doi.org/10.1007/s11336-008-9101-0.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association\nBetween Two Things.” International Journal of\nEpidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nSymonds, P. M. 1924. “On the Loss of Reliability in Ratings Due to\nCoarseness of the Scale.” Journal of Experimental\nPsychology 7 (6): 456–61. https://doi.org/10.1037/h0074469.\n\n\nThorndike, Robert L. 1949. Personnel Selection; Test and Measurement\nTechniques. Personnel Selection; Test and Measurement Techniques.\nOxford, England: Wiley.\n\n\nUlrich, Rolf, and Markus Wirtz. 2004. “On the Correlation of a\nNaturally and an Artificially Dichotomized Variable.” British\nJournal of Mathematical and Statistical Psychology 57 (2): 235–51.\nhttps://doi.org/10.1348/0007110042307203.\n\n\nVan Aarde, Ninette, Deon Meiring, and Brenton M. Wiernik. 2017.\n“The Validity of the Big Five Personality Traits for Job\nPerformance: Meta-Analyses of South African\nStudies.” International Journal of Selection and\nAssessment 25 (3): 223–39. https://doi.org/10.1111/ijsa.12175.\n\n\nVargha, András, Tamás Rudas, Harold D. Delaney, and Scott E. Maxwell.\n1996. “Dichotomization, Partial Correlation, and Conditional\nIndependence.” Journal of Educational and Behavioral\nStatistics 21 (3): 264–82. https://doi.org/10.2307/1165272.\n\n\nViswanathan, Madhu. 2005. Measurement Error and Research\nDesign. SAGE.\n\n\nViswesvaran, Chockalingam, and Deniz S. Ones. 1995. “Theory\nTesting: Combining Psychometric Meta-Analysis and Structural Equations\nModeling.” Personnel Psychology 48 (4): 865–85. https://doi.org/10.1111/j.1744-6570.1995.tb01784.x.\n\n\nViswesvaran, Chockalingam, Deniz S. Ones, Frank L. Schmidt, Huy Le, and\nIn-Sue Oh. 2014. “Measurement Error Obfuscates Scientific\nKnowledge: Path to Cumulative Knowledge Requires Corrections for\nUnreliability and Psychometric Meta-Analyses.” Industrial and\nOrganizational Psychology 7 (4): 507–18. https://doi.org/10.1017/S1754942600006799.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining\nUnbiased Results in Meta-Analysis: The Importance of Correcting for\nStatistical Artifacts.” Advances in Methods and Practices in\nPsychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  }
]