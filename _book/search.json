[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "",
    "text": "1 Greetings\nWelcome to the living open source textbook Artifact Corrections for Effect Sizes. This textbook covers all the essential equations and code needed to correct for biases in our effect size estimates. It will also hopefully provide readers with a deeper understanding, appreciation, and intuition for these seemingly complex formulas. It also covers how to apply these corrections to meta-analysis.\n\nWhat are Statistical Artifacts?\nIn this book statistical artifacts will be defined broadly as any source of contamination that induces bias in research findings. Artifacts are present in virtually every research study, so it is crucial that we address them.\n\n\nOpen and Living Textbook\nA living textbook is one that constantly updates with new features and is open to changes from others. This book will contain modern methods and cutting-edge techniques for artifact corrections, so in order to keep this book up-to-date it needs to grow as the research grows. New features, such as interactive figures will be added soon\nIt is important that this book is both open-source and open-access. All the figures, code, and documents are available in a github repository. The current maintainer of the book is Matthew B. Jané. This work is under a CC-BY license, therefore if you use any part of this work in your own work, it is important that you acknowledge it and cite it as follows:\n\nAPA\nJané, M. B. (2023). Artifact Corrections for Effect Sizes: Implementation in R and Application to Meta-Analysis. (n.p.). https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/\n\n\nBibTeX\n@book{MatthewBJane2023,\n  title     = \"Artifact Corrections for Effect Sizes: Implementation in R and Application to Meta-Analysis\",\n  author    = \"Jané, Matthew B.\",\n  year      = 2023,\n  publisher = \"(n.p.)\",\n  url      = {https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/}\n}\n\n\n\nContributions\nPlease feel free to contribute to this textbook, if your contribution makes it to the published version of this book, your name will be included in the contributor list below with a description of your work.\n\n\n\nName\nAffiliation\nRole\n\n\n\n\nVelu Immonen, BS\nSolent University\nDesigned cover and twitter preview."
  },
  {
    "objectID": "dedication.html",
    "href": "dedication.html",
    "title": "2  Dedication",
    "section": "",
    "text": "In Loving Memory of Haley Jané\n\n\nMy companion, whose love and presence have filled my life with joy and comfort."
  },
  {
    "objectID": "intro.html#what-are-effect-sizes",
    "href": "intro.html#what-are-effect-sizes",
    "title": "3  Effect Sizes and Artifacts",
    "section": "3.1 What are Effect Sizes?",
    "text": "3.1 What are Effect Sizes?\nEffect sizes are statistics that measure the magnitude of a relationship between variables. It’s important to remember that effect sizes are a valuable tool, enabling researchers to extract meaningful insights from data, rather than being the ultimate objective themselves. Effect sizes aide in researcher’s ability to draw meaningful inferences from data and therefore it is crucial that they are accurate. Correlation coefficients and standardized mean differences are two of the most common effect sizes and so they will be the primary focus of this book."
  },
  {
    "objectID": "intro.html#defining-the-scientific-estimand",
    "href": "intro.html#defining-the-scientific-estimand",
    "title": "3  Effect Sizes and Artifacts",
    "section": "3.2 Defining the Scientific Estimand",
    "text": "3.2 Defining the Scientific Estimand\nIt is important to clearly define the quantity that we aim to estimate (i.e., the estimand). A clearly defined estimand not only shapes our statistical analyses but also establishes a meaningful connection between empirical observations and a theoretical quantity (Lundberg, Johnson, and Stewart 2021). A study can produce effect size estimates that do not accurately reflect the target value. Inaccuracies in effect size estimates can be due to deviations between the current study’s design and the design of a technically perfect study (Rubin 1992). Obtaining a quality estimate of the target effect size requires that the study sample and the measured variables accurately reflect the population and variables of interest. We can illustrate this with the following example:\nLet’s say we want to know the correlation between student motivation and stress among high-school students. Suppose we conduct a study to estimate the correlation by administering a survey to a sample of senior students at a private high school. The survey consists of two questions asking student’s to rate their level of motivation and stress on a scale of 1-10. Let’s now compare how the target population and variables differ from the study population and variables (see Table 3.1):\n\n\nTable 3.1: Comparison of the target and the study.\n\n\n\n\n\n\n\n\n\nTarget\nStudy\nPotential sources of contamination\n\n\n\n\nPOPULATION\nHigh-School Students\nSeniors High-School students\nRange Restriction. Senior student’s at private universities may have a more narrow range of characteristics such as stress and motivation than other classes (e.g., freshmen) and schools (e.g., public).\n\n\nVARIABLES\nMotivation and Stress\nSelf-report questionnaire\nMeasurement Error. The measurement of the target variables of motivation and stress likely will produce errors that do not reflect the true motivation and stress of each individual."
  },
  {
    "objectID": "intro.html#effect-size-errors",
    "href": "intro.html#effect-size-errors",
    "title": "3  Effect Sizes and Artifacts",
    "section": "3.3 Effect Size Errors",
    "text": "3.3 Effect Size Errors\n\n3.3.1 Random (Sampling) Errors\nLet’s start by differentiating between a population effect size and a sample estimate. The population effect size, characterizes the effect size among all possible observations of interest. In contrast, a sample effect size is an estimate of this population value, estimated by a random sample of observations. Throughout the book, population effect sizes will be denoted with Greek letters whereas sample estimates will be denoted with an English letters.\nThe population effect size is a constant, unchanging value that remains fixed across samples. However, if we were to take a random sample from the population and estimate an effect size we would find that the sample estimate varies from sample to sample and does not exactly reflect the population value. This is due to the fact that randomly taking a subset of the population will contain inherent variability in the composition of the sample. Sampling errors describe the random deviations that we observe in effect size estimates from sample to sample (Barraza et al. 2019). Sampling errors are random, however we will see in the next section that not all errors are systematic. We can quantify sampling errors by the variance of the effect size estimator. The variance of an effect size estimator tends to be some function of sample size, where large samples will show less variance compared to small samples. For an illustration of sampling error, see Figure 3.1.\n\n\n\n\n\nFigure 3.1: This figure shows the distribution of sample estimates. The blue diamonds denotes the population effect size, which stays constant across samples. The black dots denote the sample effect size estimate. The grey lines denote random sampling errors, which represent the difference between the estimates and the population value. The sampling distribution on the right shows the probability distribution of estimates across repeated samples, the width of this distribution is described by the variance of the estimator. Note the illustration shows a normally distributed estimator, but this is not a requirement.\n\n\n\n\n\n\n3.3.2 Systematic Errors\nSampling errors produce random errors in effect sizes, however, we can also observe systematic errors. Systematic errors are deviations from the target population value that are consistent across samples and produce bias in effect size estimates. In other words, effect size estimates will be on average larger or smaller than the target population value (Barraza et al. 2019). Random sampling errors, on the other hand, will be larger or smaller than the target population value only by chance. Attenuation describes a type of systematic error where the effect size estimates are smaller than the target population value on average. Inflation on the other hand, is a type of bias that produce effect size estimates that are larger on average. An unbiased effect size would be one where there is no systematic errors and therefore, on average, it is equal to the population effect size. As we will see in future chapters, study artifacts such as selection effects and measurement error can produce effect sizes that contain systematic errors.\nWe can see in Figure 3.2 that the sampling distribution does not become wider or smaller with systematic errors (this may occur indirectly if the sampling variance depends on the effect size itself), instead the whole sampling distribution shifts downward or upward depending on whether the effect size estimates are attenuated or inflated, respectively.\n\n\n\n\n\nFigure 3.2: Three sampling distributions representing estimators that are unbiased, attenuated, and inflated. The blue line indicates the the location of the target population effect size, whereas the black dots show the effect size estimates."
  },
  {
    "objectID": "intro.html#modeling-observed-effect-sizes",
    "href": "intro.html#modeling-observed-effect-sizes",
    "title": "3  Effect Sizes and Artifacts",
    "section": "3.4 Modeling Observed Effect Sizes",
    "text": "3.4 Modeling Observed Effect Sizes\nWe can think of an observed effect size estimate as having three components:\n\n\n\n\nflowchart TD\n   E(\"observed effect &lt;br&gt;size estimate\"):::estimates --- X(\"systematic &lt;br&gt;error (bias)\"):::artifacts\n   E:::estimates --- Z(\"target population&lt;br&gt;effect size (estimand)\"):::target\n   E:::estimates --- Y(\"random (sampling)&lt;br&gt;error\"):::sampling\n   classDef artifacts fill:#d74ea250,stroke-width:0px\n   classDef sampling fill:#00000020,stroke-width:0px\n   classDef target fill:#5fa6bc55,stroke-width:0px\n   classDef estimates fill:#ffffff00,stroke-width:0px\n   linkStyle default stroke-width:2px,fill:none,stroke:black;\n\n\n\n\n\n\nLet’s construct a statistical model for an observed effect size estimate \\(h_i\\) for a given sample, \\(i\\), that accounts for each of these three components. The target population effect size will be denoted by the Greek letter \\(\\theta\\) which does not vary from sample to sample. The artifact attenuation/inflation factor is denoted with the Greek letter \\(\\alpha\\) and accounts for the systematic error that is consistent across samples. Finally the sample error is denoted by the Greek letter \\(\\varepsilon\\) which accounts for the random sampling error across samples (Raju et al. 1991, based on eq. 1),\n\\[\nh_i = {\\color{#cb427b}\\alpha} {\\color{#347e9f}\\theta} +\n{\\color{#797979ff}\\varepsilon}_i.\n\\tag{3.1}\\]\nNote that \\({\\color{#cb427b}\\alpha}\\) and \\({\\color{#347e9f}\\theta}\\) are fixed and does not vary across samples (no subscript \\(i\\)), wheras \\({\\color{#797979ff}\\varepsilon}_i\\) does vary from sample to sample. For the above equation to be true we need to assume that the expectation (i.e., the mean over repeated samples) of \\({\\color{#797979ff}\\varepsilon}_i\\) is zero such that, \\(\\mathbb{E}[{\\color{#797979ff}\\varepsilon}_i] = 0\\). A consequence of this assumption is that the expectation of the observed effect size estimate is \\(\\mathbb{E}[h_i]={\\color{#cb427b}\\alpha} {\\color{#347e9f}\\theta}\\). We will label the term \\({\\color{#cb427b}\\alpha} {\\color{#347e9f}\\theta}\\) as the contaminated population effect size as it is a fixed value that encompasses the target effect size and systematic error.\nThe artifact attenuation/inflation factor, \\({\\color{#cb427b}\\alpha}\\), completely describes the net systematic error in the observed effect size estimates. The value of \\(\\alpha\\) can indicate whether \\(h_i\\) is an unbiased (\\({\\color{#cb427b}\\alpha} = 0\\)), attenuated (\\({\\color{#cb427b}\\alpha} &lt; 1\\)), or inflated estimator (\\({\\color{#cb427b}\\alpha} &gt; 1\\)).\nBecause of random sampling error, the observed effect size estimate will differ from sample to sample (\\(h_1\\neq\\) \\(h_2\\neq\\) \\(h_3\\neq...\\)). The extent to which estimates fluctuate across samples can be quantified by the variance of the effect size estimator, denoted as \\(\\text{var}(h_i)\\). Remember that the contaminated population effect size (\\({\\color{#cb427b}\\alpha} {\\color{#347e9f}\\theta}\\)) fixed across samples, this would result in variation in random sampling errors to be solely responsible for the variation in observed effect sizes such that, \\(\\text{var}(h_i)=\\text{var}({\\color{#797979ff}\\varepsilon}_i)\\)."
  },
  {
    "objectID": "intro.html#correcting-effect-sizes",
    "href": "intro.html#correcting-effect-sizes",
    "title": "3  Effect Sizes and Artifacts",
    "section": "3.5 Correcting Effect Sizes",
    "text": "3.5 Correcting Effect Sizes\nIn principle, if we know the value of the artifact attenuation/inflation factor \\({\\color{#cb427b}\\alpha}\\) then we could correct the observed effect size for systematic errors. However, \\({\\color{#cb427b}\\alpha}\\) is unknown and must be estimated. An estimate of \\({\\color{#cb427b}\\alpha}\\) will be denoted by the English letter \\({\\color{#cb427b}a}\\). According to the model described in Equation 3.1, \\({\\color{#cb427b}\\alpha}\\) is a fixed value that does not vary from sample to sample, however it’s sample estimate will:\n\\[\n{\\color{#cb427b}a}_i = {\\color{#cb427b}\\alpha} + {\\color{#797979ff}\\xi}_i,\n\\tag{3.2}\\]\nwhere \\({\\color{#797979ff}\\xi}_i\\) denotes the sampling error in \\({\\color{#cb427b}a}_i\\). The corrected effect size can thus be calculated by dividing the observed effect size by the estimated artifact factor,\n\\[\n{\\color{#347e9f}h_c}_{_i} = \\frac{h_i}{{\\color{#cb427b}a}_i},\n\\tag{3.3}\\]\nThe corrected effect size will only be an asymptotically (as the sample size approaches infinity) unbiased estimator of the target population value. As we will see in future chapters, corrections will also impose additional distributional assumptions on the data.\nSampling variance of the corrected effect size will depend greatly on the estimation procedure of the effect size and the artifact factor. Sampling variance for corrected effect sizes will be discussed in more detail for each artifact correction chapter."
  },
  {
    "objectID": "intro.html#effect-size-types",
    "href": "intro.html#effect-size-types",
    "title": "3  Effect Sizes and Artifacts",
    "section": "3.6 Effect Size Types",
    "text": "3.6 Effect Size Types\nAlthough there are many types of effect sizes we can use to quantify research findings, we will primarily focus on two: correlations and standardized mean differences (SMD).\n\n3.6.1 Correlations\nA correlation describes the relationship between two continuous variables. The population correlation (\\(\\rho\\)) between variables, \\(X\\) and \\(Y\\), can be defined as the covariance (\\(\\sigma_{XY}\\)) divided by the product of the standard deviations of \\(X\\) (\\(\\sigma_{X}\\)) and \\(Y\\) (\\(\\sigma_{Y}\\)) (Cooper, Hedges, and Valentine 2009, equation 11.21; Jacobs and Viechtbauer 2017, eq. 1),\n\\[\n\\rho = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}}.\n\\tag{3.4}\\]\nA sample estimate of the population correlation can be defined as the sample covariance (\\(S_{XY}\\)) divided by the product of the sample standard deviations (Jacobs and Viechtbauer 2017, eq. 2),\n\\[\nr = \\frac{S_{XY}}{S_XS_Y}\n\\tag{3.5}\\]\nThis is commonly referred to as the Pearson correlation coefficient. The Pearson correlation coefficient was first introduced by Auguste Bravais (1844), then later developed further by Karl Pearson, lending itself to the name. See Figure 3.3 for a visualization of correlated variables.\n\n\n\n\n\nFigure 3.3: Diagram illustrating a correlation between two variables (\\(X\\) and \\(Y\\)). The left panel shows a negative correlation and the right panel shows a positive correlation. The ellipses show the contour of a bivariate normal distribution which the data points are sampled from.\n\n\n\n\nAssuming that the variables follow a bivariate normal distribution, the asymptotic variance in an observed correlation with \\(n\\) observations is (Pearson and Filon 1898, 174; Jacobs and Viechtbauer 2017, eq. 9),\n\\[\n\\text{var}(r_i) \\overset{_\\infty}{=} \\frac{\\left(1 - \\rho^2\\right)^2}{n},\n\\tag{3.6}\\]\nSince the population correlation is unavailable in practice, an accurate estimate of the variance for a given sample \\(i\\) uses the sample correlation instead of population correlation and uses the degrees of freedom, \\(n-1\\), in the denominator instead of the sample size (Cooper, Hedges, and Valentine 2009, eq. 12.27; Jacobs and Viechtbauer 2017, eq. 10),\n\\[\n\\widehat{\\text{var}}(r_i) = \\frac{\\left(1 - r_i^2\\right)^2}{n-1}\n\\tag{3.7}\\]\nWithin the context of a research study that is contaminated with artifacts, we can model an observed correlation for a sample \\(i\\) as similarly to an Equation 3.1,\n\\[\nr_i = {\\color{#cb427b}\\alpha} {\\color{#347e9f}\\rho} +\n{\\color{#797979ff}\\varepsilon}_i.\n\\tag{3.8}\\]\nWhere \\({\\color{#347e9f}\\rho}\\) is the target population correlation (the estimand), \\({\\color{#cb427b}\\alpha}\\) is the artifact attenuation/inflation factor, and \\({\\color{#797979ff}\\varepsilon}_i\\) denotes the sampling errors. The observed correlation is biased relative to the target population correlation as a function of \\({\\color{#cb427b}\\alpha}\\), and therefore a corrected correlation can calculated by,\n\\[\n{\\color{#347e9f}r_c}_{_i} = \\frac{r_i}{{\\color{#cb427b}a}_i}.\n\\tag{3.9}\\]\nFor example, we will see in Chapter 7 that if \\(X\\) and \\(Y\\) are bivariate normal and \\(X\\) undergoes dichotomization through a median split (i.e., dividing observations into two groups based on the median of \\(X\\)) the observed correlation will be attenuated. In this scenario, the artifact attenuation/inflation factor is \\({\\color{#cb427b}a} = 2/\\sqrt{2\\pi} \\approx .80\\), and therefore if the observed correlation is \\(r = .40\\) then the corrected correlation would be estimated as \\(r_c=r/{\\color{#cb427b}a}=.40/.80=.50\\).\n\n\n\n\n\n\nComputing Correlations in R\n\n\n\nLet’s load in the iris data set that contains various physical measurements of three species of plants. We can also subset the data set to only look at the Setosa species:\n\n# load in data\ndata(\"iris\")\n\n# subset rows to only include setosa species\ndf &lt;- subset(iris, iris$species == 'setosa')\n\n# view first 6 plants\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nLet’s say we want to know the correlation between petal length and sepal length. We can use the cor() function in base R to obtain the Pearson correlation coefficient:\n\nr &lt;- cor(iris$Petal.Length,iris$Sepal.Length)\nr\n\n[1] 0.8717538\n\n\nThe result shows a strong positive correlation between petal and sepal length (\\(r = .87\\)). To compute the sampling variance and the standard error, we can use the var_error_r() function in the psychmeta package (Dahlke and Wiernik 2019)\n\n# load in package\nlibrary(psychmeta)\n\n# compute sampling variance\nvar_r &lt;- var_error_r(r, n = nrow(iris), correct_bias = FALSE)\n\n# print results\ndata.frame(r,var_r) \n\n          r        var_r\n1 0.8717538 0.0003867233\n\n\n\n\n\n\n3.6.2 Standardized Mean Difference\nThe standardized mean differences (SMD) is used to quantify the difference between the average value between two groups. In the population, the SMD (\\(\\delta\\)), the difference between the mean of group \\(A\\) (\\(\\mu_A\\)) and group \\(B\\) (\\(\\mu_B\\)) is standardized by the population within group standard deviation (\\(\\sigma\\)) (Cohen 1988, eq. 2.2.1),\n\\[\n\\delta = \\frac{\\mu_A - \\mu_B}{\\sigma}.\n\\tag{3.10}\\]\nDividing by the population within-group standard deviation assumes that the standard deviations within both groups are fixed and equal (i.e., \\(\\sigma=\\sigma_A=\\sigma_B\\)). For a visualization of an SMD see Figure 3.4.\n\n\n\n\n\nFigure 3.4: Diagram illustrating a standardized mean difference in the population between two normal distributions. The mean and standard deviation of group \\(A\\) is \\(\\mu_A=9\\) and \\(\\sigma_A=4\\), respectively. Whereas mean and standard deviation of group \\(B\\) is \\(\\mu_A=12\\) and \\(\\sigma_A=4\\), respectively. Therefore the standardized mean difference is \\(\\delta = (9-12)/4=-.75\\). Note that \\(\\sigma_A=\\sigma_B\\).\n\n\n\n\nCohen (1988) developed a sample estimator of \\(\\delta\\) that is commonly referred to as Cohen’s \\(d\\). Within a sample, we can estimate \\(\\delta\\) with the sample estimator (Cooper, Hedges, and Valentine 2009, eq. 11.96),\n\\[\nd = \\frac{\\overline{Y}_A-\\overline{Y}_B}{S_p},\n\\tag{3.11}\\]\nwhere \\(S_p\\) is the pooled sample standard deviation and the bars indicate the sample mean. The pooled standard deviation computes a weighted average (weighted by the within-group degrees of freedom, e.g., \\(n_{A}-1\\)) of the within-group sample variance and then takes the square root (Cooper, Hedges, and Valentine 2009, eq. 12.12),\n\\[\nS_p = \\sqrt{\\frac{(n_A-1)S_A^2 + (n_B-1)S_B^2}{n-2}}\n\\tag{3.12}\\]\nWhere \\(n_A\\), \\(n_B\\), and \\(n\\) represent the sample size within group \\(A\\), group \\(B\\), and the total sample size (\\(n = n_A+n_B\\)), respectively. Assuming that \\(Y\\) is normally distributed within each group and the within-group variances are equal, the asymptotic sampling variance of an SMD is (Hedges and Olkin 1985, eq. 14),\n\\[\n\\text{var}(d_i) \\overset{_\\infty}{=} \\frac{n}{n_An_B} + \\frac{\\delta^2}{2n}.\n\\tag{3.13}\\]\nSince the population SMD is unknown, we can estimate it with the sample \\(d\\) value. Therefore the estimated variance can be computed as (Hunter and Schmidt 2015, eq. 7.23),\n\\[\n\\widehat{\\text{var}}(d_i) = \\left(\\frac{n - 1}{n - 3}\\right)\\left(\\frac{n}{n_An_B} + \\frac{\\delta^2}{2n}\\right).\n\\tag{3.14}\\]\nNote that the multiplier, \\((n_A+n_B - 1)/(n_A+n_B - 3)\\), is to account for slight underestimation of the sampling variance in small sample sizes (Goulet-Pelletier and Cousineau 2018).\nIn a study contaminated by artifacts, we can model an observed SMD for a sample \\(i\\) as similarly to an Equation 3.1,\n\\[\nd_i = {\\color{#cb427b}\\alpha} {\\color{#347e9f}\\delta} +\n{\\color{#797979ff}\\varepsilon}_i.\n\\tag{3.15}\\]\nWhere \\({\\color{#347e9f}\\delta}\\) is the target population SMD (the estimand), \\({\\color{#cb427b}\\alpha}\\) is the artifact attenuation/inflation factor, and \\({\\color{#797979ff}\\varepsilon}_i\\) denotes the sampling errors. The observed SMD is biased relative to the target population SMD as a function of \\({\\color{#cb427b}\\alpha}\\). However, \\({\\color{#cb427b}\\alpha}\\) is not available in practice and so therefore the observed SMD can be corrected by dividing by an estimate of \\({\\color{#cb427b}\\alpha}\\),\n\\[\n{\\color{#347e9f}d_c}_{_i} = \\frac{d_i}{{\\color{#cb427b}a}_i}.\n\\tag{3.16}\\]\nOccasionally, corrections can not be applied to the contaminated \\(d\\) value directly. In such cases, we may have to correct \\(d\\) by first converting to a point-biserial correlation (i.e., a Pearson correlation between a dichotomous group variable and continuous variable), correcting the correlation, and then converting back to a (corrected) \\(d\\) value (Hunter and Schmidt 2015). The three-step procedure can be applied as follows:\n\nConvert \\(d\\) to \\(r\\) using the proportion of subjects in group \\(A\\) or group \\(B\\) (\\(p=n_A/n\\) or \\(p=n_B/n\\)) (Wiernik and Dahlke 2020, equation 9),\n\n\\[\nr_i = \\frac{d_i}{\\sqrt{\\frac{1}{p_i(1-p_i)}+d_i}}.\n\\tag{3.17}\\]\n\nCorrect the point-biserial correlation,\n\n\\[\n{\\color{#347e9f}r_c}_{_i} = \\frac{r_i}{{\\color{#cb427b}a}_i}.\n\\]\n\nConvert \\(r_c\\) back to a \\(d\\) value. If the proportions of group membership are contaminated by artifacts, then we need the true group proportions in the target population (\\(p_i^*\\)) to convert back to \\(d\\). If the the true group proportions are unavailable, then we can use the observed proportions as estimates (Wiernik and Dahlke 2020, equation 10)\n\n\\[\n{\\color{#347e9f}d_c}_{_i} =  \\frac{{\\color{#347e9f}r}_{{\\color{#347e9f}c}_i}}{\\sqrt{p_i^*(1-p_i^*)(1-{\\color{#347e9f}r}_{{\\color{#347e9f}c}_i}^2)}} \\approx \\frac{{\\color{#347e9f}r}_{{\\color{#347e9f}c}_i}}{\\sqrt{p_i(1-p_i)(1-{\\color{#347e9f}r}_{{\\color{#347e9f}c}_i}^2)}}.\n\\tag{3.18}\\]\nThe sampling variance of the corrected SMD can be quite complicated when there is sampling error in the artifact factor.\n\n\n\n\n\n\nComputing SMDs in R\n\n\n\nLet’s load in a data set for a plant growth experiment:\n\n# load in data\ndata(\"PlantGrowth\")\n\n# view first 6 plants\nhead(PlantGrowth)\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n\nWe see that the data set contains two variables, weight of the plant and the experimental group. There are three two experimental groups present: a control group and two treatment groups. If we want to obtain the SMD between the each treatment group and the control group, we can use the cohen.d function psych package (William Revelle 2023):\n\nlibrary(psych)\n\n# estimate SMD for first treatment group\nd1 &lt;- cohen.d(weight ~ group,\n             data = subset(PlantGrowth, group == 'ctrl' | group == 'trt1'))\nd1$cohen.d\n\n          lower     effect     upper\nweight -1.44938 -0.5615655 0.3411078\n\n# estimate SMD for first treatment group\nd2 &lt;- cohen.d(weight ~ group,\n             data = subset(PlantGrowth, group == 'ctrl' | group == 'trt2'))\nd2$cohen.d\n\n            lower   effect    upper\nweight 0.05850126 1.005987 1.929048\n\n\nWe see that the first treatment showed a negative effect on plant growth (\\(d=-0.56\\)) and the second treatment shows a positive effect (\\(d=1.01\\)). For our purposes we want the estimate and the sampling variance of the estimator, therefore we can extract that information from the output of cohen.d():\n\ndata.frame(d = d1$cohen.d[,'effect'], \n           var.d = d1$se^2)\n\n           d     var.d\n1 -0.5615655 0.8014621\n\ndata.frame(d = d2$cohen.d[,'effect'], \n           var.d = d2$se^2)\n\n         d     var.d\n1 1.005987 0.8747367\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarraza, Felipe, Marcelo Arancibia, Eva Madrid, and Cristian Papuzinski. 2019. “General Concepts in Biostatistics and Clinical Epidemiology: Random Error and Systematic Error.” Medwave 19 (7): e7687. https://doi.org/10.5867/medwave.2019.07.7687.\n\n\nBravais, A. 1844. Analyse mathématique sur les probabilités des erreurs de situation d’un point. Impr. Royale.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. Academic Press.\n\n\nCooper, Harris M., Larry V. Hedges, and Jeff C. Valentine, eds. 2009. The Handbook of Research Synthesis and Meta-Analysis. 2nd ed. New York: Russell Sage Foundation.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nGoulet-Pelletier, Jean-Christophe, and Denis Cousineau. 2018. “A Review of Effect Sizes and Their Confidence Intervals, Part i: The Cohen’s d Family.” The Quantitative Methods for Psychology 14 (4): 242–65. https://doi.org/10.20982/tqmp.14.4.p242.\n\n\nHedges, Larry V., and Ingram Olkin. 1985. Statistical Methods for Meta-Analysis. Academic press. https://books.google.com/books?hl=en&lr=&id=7GviBQAAQBAJ&oi=fnd&pg=PP1&dq=info:e6P1zfh2T6QJ:scholar.google.com&ots=Dx-YqN6_9B&sig=-39HgbYdWPp_BwSTzA9cRODs2Q0.\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of meta-analysis: correcting error and bias in research findings (third). Third. Thousand Oaks, California: Sage Publications.\n\n\nJacobs, Perke, and Wolfgang Viechtbauer. 2017. “Estimation of the Biserial Correlation and Its Sampling Variance for Use in Meta-Analysis.” Research Synthesis Methods 8 (2): 161–80. https://doi.org/10.1002/jrsm.1218.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 86 (3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nPearson, Karl, and L. N. G. Filon. 1898. “Mathematical Contributions to the Theory of Evolution. IV. On the Probable Errors of Frequency Constants and on the Influence of Random Selection on Variation and Correlation. [Abstract].” Proceedings of the Royal Society of London 62: 173–76. https://www.jstor.org/stable/115709.\n\n\nRaju, Nambury, Michael Burke, Jacques Normand, and George Langlois. 1991. “A New Meta-Analytic Approach.” Journal of Applied Psychology 76 (June): 432–46. https://doi.org/10.1037/0021-9010.76.3.432.\n\n\nRubin, Donald B. 1992. “Meta-Analysis: Literature Synthesis or Effect-Size Surface Estimation?” Journal of Educational Statistics 17 (4): 363–74. https://doi.org/10.3102/10769986017004363.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611.\n\n\nWilliam Revelle. 2023. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University. https://CRAN.R-project.org/package=psych."
  },
  {
    "objectID": "small_samples.html#introduction",
    "href": "small_samples.html#introduction",
    "title": "4  Small Samples",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThe purpose of sample statistics is to draw meaningful inferences about the population. However, effect size estimators such as Pearson’s correlation coefficient and Cohen’s \\(d\\) are biased in small sample sizes. This small sample bias is an artifact and can be adjusted with the appropriate correction factor."
  },
  {
    "objectID": "small_samples.html#when-correcting-alongside-other-artifacts",
    "href": "small_samples.html#when-correcting-alongside-other-artifacts",
    "title": "4  Small Samples",
    "section": "4.2 When Correcting alongside other Artifacts",
    "text": "4.2 When Correcting alongside other Artifacts\nThe small sample bias should always be corrected for prior to applying any other artifact correction. It is independent of all other artifact corrections and therefore the corrected effect sizes in this section can be treated as the uncorrected effect sizes in other sections."
  },
  {
    "objectID": "small_samples.html#correcting-standardized-mean-differences-for-small-sample-bias",
    "href": "small_samples.html#correcting-standardized-mean-differences-for-small-sample-bias",
    "title": "4  Small Samples",
    "section": "4.3 Correcting Standardized Mean Differences for Small Sample Bias",
    "text": "4.3 Correcting Standardized Mean Differences for Small Sample Bias\n\n4.3.1 Defining the Target Quantity\nOur quantity of interest is the population standardized mean difference, \\(\\delta\\), between groups \\(A\\) and \\(B\\). We can model the relationship between the population standardized mean difference and the estimate (\\(d\\)),\n\\[\nd = a\\delta+e.\n\\]\nWhere \\(a\\) is an attenuation/inflation factor and \\(e\\) is our sampling error term. Ultimately, we can obtain an unbiased estimate of the population standardized mean difference by correcting the sample standardized mean difference as follows,\n\\[\nd_c = \\frac{d}{\\hat{a}}.\n\\]\n\n\n4.3.2 Artifact Correction for d\nAs the sample size approaches infinity, Cohen’s estimator of the standardized mean difference is unbiased (Hedges 1981; Cohen 2013). However, in small sample sizes Cohen’s estimator is inflated, that is, on average, it overestimates the population standardized mean difference. To see why this is the case, we can first define the population standardized mean difference between group \\(A\\) and group \\(B\\) such that,\n\\[\n\\delta = \\frac{\\mu_A-\\mu_B}{\\sigma}.\n\\]\nWhere \\(\\bar{y}_A\\) and \\(\\bar{y}_B\\) are the observed arithmetic means of group \\(A\\) and group \\(B\\), respectively. A sample estimate of the standardized mean difference is,\n\\[\nd = \\frac{\\overline{Y}_A-\\overline{Y}_B}{S_p}\n\\tag{4.1}\\]\nWhere \\(S_p\\) is the pooled standard deviation (i.e., weighted average within-group standard deviation). The estimator, \\(d\\), is an asymptotically unbiased estimate of \\(\\delta\\). We can denote this asymptotic relationship as,\n\\[\n\\mathbb{E}_i[d_i] \\overset{_n}{\\asymp} \\delta.\n\\]\nHowever, \\(d\\) is a biased estimator of \\(\\delta\\) when the sample size is finite. Particularly, the smaller the sample size, the larger the bias. We can see that in Figure 4.1, \\(d\\) tends to over-estimate \\(\\delta\\). Therefore, we can apply an artifact inflation factor, \\(a\\), to capture this over-estimation,\n\\[\n\\mathbb{E}_i[d_i] = a\\delta.\n\\]\nThe reason for this bias in \\(d\\) values is two-fold:\n\nStandard deviations tend to be attenuated in small sample sizes. This is due to the fact that although variance (squared standard deviation) is an unbiased estimator of the population variance, the square root of the variance (i.e., the standard deviation, \\(S_p\\)) is a biased estimate of the population standard deviation (\\(\\mathbb{E}_i[\\sqrt{S_{p_i}}] \\neq \\sqrt{\\sigma}\\), Holtzman 1950).\nA ratio is biased in small sample sizes (Kempen and Vliet 2000), therefore the ratio between the mean difference and the standard deviation (see Equation 4.1) will likewise be biased.\n\nTo obtain an unbiased estimate of the population standardized mean difference, we need to first estimate the artifact inflation factor, \\(a\\). In this case, the artifact inflation factor has been mathematically derived previously by Hedges (1989). For other types of artifacts, \\(a\\) is unknown in practice and must be estimated, however, for small sample bias the exact value of \\(a\\) is known. The precise value of \\(a\\) is a function of sample size (equation 6e, Hedges 1989),\n\\[\na = \\frac{\\Gamma\\left(\\frac{n-3}{2}\\right)\\sqrt{\\frac{n-2}{2}}}{\\Gamma\\left(\\frac{n-2}{2}\\right)}.\n\\] Where \\(\\Gamma(\\cdot)\\) denotes the gamma function. The gamma function is factorial function generalized to non-integers (note that a factorial function on integers would look something like: \\(3! = 3 \\cdot 2 \\cdot 1\\), Taboga 2021). There is also an approximation of \\(a\\) that is more computationally trivial (re-arrangement of the first formula on pp. 114, Hedges 1989):\n\\[\na \\approx \\frac{4n-9}{4n-12}\n\\]\nHowever, with the advent of computers, this approximation formula is unnecessary. We can see in Figure 4.2 that there is notable bias when sample size is below 20. Furthermore, the bias is most pronounced when the sample \\(d\\) value is larger (there is no bias at \\(d=0\\)).\n\n\n\n\n\nFigure 4.1: Plot showing the bias in the standardized mean difference computed in small samples. The X-axis is the sample size (\\(n\\), the vertical bars are indicative of each integer). The Y-axis is the the estimated standardized mean difference (\\(d\\)). The dark pink coloring indicates more bias.\n\n\n\n\nUsing \\(a\\), we can correct the \\(d\\) value such that,\n\\[\nd_c = \\frac{d}{a} = \\frac{d}{ \\left[\\frac{\\Gamma\\left(\\frac{n-3}{2}\\right)\\sqrt{\\frac{n-2}{2}}}{\\Gamma\\left(\\frac{n-2}{2}\\right)}\\right]}.\n\\tag{4.2}\\]\nTo obtain the standard error of \\(d_c\\) we can apply the same correction as above to the standard error of \\(d\\) (\\(se\\)),\n\\[\nse(d_c) = \\frac{se(d)}{a} = \\frac{se(d)}{ \\left[\\frac{\\Gamma\\left(\\frac{n-3}{2}\\right)\\sqrt{\\frac{n-2}{2}}}{\\Gamma\\left(\\frac{n-2}{2}\\right)}\\right]}.\n\\tag{4.3}\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say we want to compute the mean difference of test scores between two classes. Class 1 has 11 students and class 2 has 10 students. We then obtain a standardized mean difference between the two classes of \\(d=.43\\). In R, we can use the correct_d_bias() function to correct the point estimate (using Equation 4.2) and then var_error_g function to correct the error variance and thus the standard error (using Equation 4.3). Both of these functions are found in the psychmeta package (Dahlke and Wiernik 2019).\n\nlibrary(psychmeta)\n\nd &lt;- .43\nn1 &lt;- 11\nn2 &lt;- 10\n\n# correct\ndc &lt;- correct_d_bias(d = d,\n                     n = n1+n2)\n\nvar_corrected &lt;- var_error_g(g = dc, \n                             n1 = n1, \n                             n2 = n2)\n\n# print results\ncbind(dc=dc, se = sqrt(var_corrected))\n\n            dc       se\n[1,] 0.4144264 0.424262\n\n\nThe output shows a corrected standardized mean difference of \\(d_c = 0.41\\) (\\(se(d_c) = 0.42\\))"
  },
  {
    "objectID": "small_samples.html#correcting-for-small-sample-bias-in-correlations",
    "href": "small_samples.html#correcting-for-small-sample-bias-in-correlations",
    "title": "4  Small Samples",
    "section": "4.4 Correcting for Small Sample Bias in Correlations",
    "text": "4.4 Correcting for Small Sample Bias in Correlations\n\n4.4.1 Defining the Estimand\nOur quantity of interest is the population correlation, \\(\\rho\\). We can model the relationship between the population correlation and our sample estimate (\\(r\\)) with,\n\\[\nr = a\\rho+e\n\\]\nWhere \\(a\\) is our small sample biasing factor and \\(e\\) is our sampling error term. Ultimately, we can obtain an unbiased estimate of the population correlation by correcting the observed correlation as follows,\n\\[\nr_c = \\frac{r}{a}\n\\]\n\n\n4.4.2 Artifact Correction for r\nLet’s first define the correlation in the population as the covariance between \\(X\\) and \\(Y\\) (\\(\\sigma_{XY}\\)) standardized by the product of the standard deviation of \\(X\\) (\\(\\sigma_X\\)) and \\(Y\\) (\\(\\sigma_Y\\)):\n\\[\n\\rho = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_Y}\n\\]\nThe sample estimate can be defined as (\\(S\\) denoting the sample estimates of \\(\\sigma\\)),\n\\[\nr = \\frac{S_{XY}}{S_{X}S_Y}\n\\]\nAsymptotically, the expectation of a sample correlation is equal to the population correlation,\n\\[\n\\mathbb{E}_i[r_i] \\overset{_n}{\\asymp} \\rho.\n\\]\nHowever, similar to standardized mean differences, correlations are also biased in small sample sizes (Olkin and Pratt 1958). Unlike standardized mean differences, Pearson correlations are under-estimated, rather than over-estimated. Therefore, an artifact attenuation factor, \\(a\\), can account for this bias,\n\\[\n\\mathbb{E}_i[r_i] = a\\rho.\n\\]\nBecause the attenuation factor, \\(a\\), has been mathematically derived directly Olkin and Pratt (equation 2.3, 1958), there is no need to obtain a noisy estimate of \\(a\\). Therefore \\(a\\) can be defined exactly as,\n\\[\na = \\frac{1}{F\\left(\\frac{1}{2},\\frac{1}{2};\\frac{n-1}{2};1-r^2\\right)}\n\\tag{4.4}\\]\nWhere \\(F(\\cdot)\\) is the hypergeometric function (for background see the Wikipedia page: ). The hypergeometric function is a complicated and iterative function which which can be defined in terms of \\(\\Gamma(\\cdot)\\) functions (plugging in values into equation 2.2, Olkin and Pratt 1958)\n\\[\nF\\left(\\frac{1}{2},\\frac{1}{2};\\frac{n-1}{2};1-r^2\\right) = \\sum^{\\infty}_{z=0} \\frac{\\Gamma\\left(\\frac{1}{2} + z\\right)\\Gamma\\left(\\frac{n-1}{2}\\right)\\left(1-r^2\\right)^z}{\\Gamma\\left(\\frac{1}{2}\\right)\\Gamma\\left(\\frac{n-1}{2}+z\\right)z!}\n\\]\nAlthough this formula is complex, it can be easily done in R so do not worry! There is also an approximation of \\(a\\) that is much simpler than Equation 4.4,\n\\[\na \\approx \\frac{2(n-3)}{2n-r^2-5}\n\\]\nThen we can correct the point-estimate the sampling variance for small sample bias. I will emphasize that approximations are not necessary if a computer is available. We can see in Figure 4.2 that there is notable bias when sample size is below 15. Furthermore, the bias is most pronounced when the sample correlation around .60 (there is no bias at \\(r=0\\) and \\(r=\\pm 1\\)).\n\n\n\n\n\nFigure 4.2: Plot showing the bias in the correlations computed in small samples. The X-axis is the sample size (\\(n\\), the vertical bars are indicative of each integer). The Y-axis is the the estimated Pearson correlation (\\(r\\)). The dark pink coloring indicates more bias.\n\n\n\n\nTo correct for small sample bias, we can divide the sample correlation \\(r\\) by the attenuation factor \\(a\\),\n\\[\nr_c = \\frac{r}{a} = \\frac{r}{\\left[\\frac{1}{F\\left(\\frac{1}{2},\\frac{1}{2};\\frac{n-1}{2};1-r^2\\right)}\\right]}\n\\tag{4.5}\\]\nWhere the standard error of \\(r_c\\) can be obtained by correcting the observed standard error (\\(se\\)),\n\\[\nse(r_c) = \\frac{se(r)}{a} = \\frac{se(r)}{\\left[\\frac{1}{F\\left(\\frac{1}{2},\\frac{1}{2};\\frac{n-1}{2};1-r^2\\right)}\\right]}\n\\tag{4.6}\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say we compute a correlation of \\(r=.43\\) between test scores and self-reported motivation in a small sample of students. In R, we can use the escalc function in the metafor package (Viechtbauer 2010). Setting measure = \"UCOR\" will apply the small sample correction for the point estimate (Equation 4.5) and the error variance (i.e., square of the standard error, Equation 4.6). Note in order to run this function you must have the gsl package already installed.\n\n# install.packages('gsl')\nlibrary(metafor)\n\nr &lt;- .43\nn &lt;- 11\n\n# correct the correlation\nescalc(measure = 'UCOR',\n       ri = r,\n       ni = n,\n       var.names = c(\"rc\", \"se.2\"),\n       digits = 3)\n\n\n     rc  se.2 \n1 0.454 0.063 \n\n\nThe output shows a corrected correlation between test scores and motivation of \\(r_c=.454\\) (\\(se(r)^2=.063\\)).\n\n\n\n\n\n\n\nCohen, Jacob. 2013. Statistical Power Analysis for the Behavioral Sciences. Academic Press.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator of Effect Size and Related Estimators.” Journal of Educational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\n———. 1989. “An Unbiased Correction for Sampling Error in Validity Generalization Studies.” Journal of Applied Psychology 74 (3): 469–77. https://doi.org/10.1037/0021-9010.74.3.469.\n\n\nHoltzman, Wayne H. 1950. “The Unbiased Estimate of the Population Variance and Standard Deviation.” The American Journal of Psychology 63 (4): 615–17. https://doi.org/10.2307/1418879.\n\n\nKempen, G.m.p. van, and L.j. van Vliet. 2000. “Mean and Variance of Ratio Estimators Used in Fluorescence Ratio Imaging.” Cytometry 39 (4): 300–305. https://doi.org/10.1002/(SICI)1097-0320(20000401)39:4&lt;300::AID-CYTO8&gt;3.0.CO;2-O.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of Certain Correlation Coefficients.” The Annals of Mathematical Statistics 29 (1): 201–11. https://www.jstor.org/stable/2237306.\n\n\nTaboga, Marco. 2021. “Gamma Function.” https://www.statlect.com/mathematical-tools/gamma-function.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting meta-analyses in R with the metafor package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03."
  },
  {
    "objectID": "unreliability.html#introduction",
    "href": "unreliability.html#introduction",
    "title": "5  Unreliability",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn general terms, measurement is the process of quantifying an attribute or characteristic of something. In scientific measurement, the measurand is the quantity or the attribute we intend to measure. In the psychological sciences, measurands usually take the form of constructs such as intelligence or anxiety. Often the goal of measurement is to produce quantities (i.e., scores) that accurately reflect the measurand. However, quantities that do not reflect a real attribute can still have useful predictive value (e.g., socio-economic status). It is important to note that measures are not all created equal, some perform better than others. Ideally, measures should produce scores that are consistent and repeatable, this is referred to as the reliability of a measure. A high quality measure should produce highly reliable scores. This section will review what reliability is in theory, how to estimate reliability, and how to correct effect sizes for measurement error."
  },
  {
    "objectID": "unreliability.html#sec-true-score-theory",
    "href": "unreliability.html#sec-true-score-theory",
    "title": "5  Unreliability",
    "section": "5.2 Reliability in True Score Theory",
    "text": "5.2 Reliability in True Score Theory\nTrue score theory (or classical test theory) is a mathematical formalization of observed scores obtained from a measurement procedure. Observed scores, \\(X_m\\), is defined as a score obtained upon measurement \\(m\\). The true score model assumes that each individual, has a true score, \\(T\\), that stays constant over repeated measurements. Variation in observed scores over repeated measurements is due to measurement-specific error, \\(E_{m}\\),\n\\[\nX_{m} = T+E_{m}.\n\\]\nHere, measurements are strictly parallel. Strictly parallel measurements have the following four properties (p. 69, Haertel 2006):\n\nMeasurements have identical specifications. That is, each measurement uses the same measurement procedure.\nThe distribution of observed scores for each measurement are identical: \\(f(X_1) = f(X_2) = \\ldots\\).\nAny set of two measurements are assumed to covary the same as any other set of two measurements: \\(\\sigma_{X_1 X_2} = \\sigma_{X_2 X_3} = \\sigma_{X_1 X_3} = \\ldots\\).\nEach measurement equally covaries with any other variable: \\(\\sigma_{X_1 Y} = \\sigma_{X_2 Y} = \\ldots\\).\n\nTrue scores can be defined as the expected value (i.e., the mean) of observed scores over repeated measurements such that, \\(\\mathbb{E}_m[X_{m}]=T\\). Given this assumption, it follows that the average of the resultant errors is zero across repeated measurements, \\(\\mathbb{E}_m[E_{m}]=0\\). It also follows that the covariance between observed scores from measurement to measurement must only be attributable to the variation in true scores (\\(\\sigma_{XX'}=\\sigma^2_{T}\\)) and therefore true scores and errors are independent (\\(\\sigma_{ET}=0\\)). The independence between true scores and errors provide convenient parsing of the variance in observed scores,\n\\[\n\\sigma_{X}^2 = \\sigma_T^2 + \\sigma_{E}^2.\n\\tag{5.1}\\]\nIn practice, the goal is to obtain observed scores that closely resemble true scores, therefore it is important to minimize measurement error variance (\\(\\sigma^2_E\\)). If \\(\\sigma^2_E = 0\\), then the scores can be said to have perfect reliability, that is, observed scores do not vary upon repeated measurements and are thus identical to true scores. In practice, this is virtually never the case. In true score theory, reliability can be defined as the correlation between observed scores in parallel measurements,\n\\[\n\\rho_{XX'}=\\frac{\\sigma_{XX'}}{\\sigma_X\\sigma_{X'}}  = \\frac{\\sigma_T^2}{\\sigma^2_X}.\n\\]\nThe reliability can also be expressed by the square of the correlation between observed scores and true scores. To understand why this is the case, note that the covariance between parallel forms of a measure is equivalent to the covariance between observed scores and true scores, \\(\\sigma_{XT}=\\)\\(\\sigma_{(T+E)T}=\\)\\(\\sigma^2_T + \\sigma_{TE}=\\)\\(\\sigma^2_T = \\sigma_{XX'}\\) (Haertel 2006),\n\\[\n\\rho_{XX'} = \\frac{\\sigma_T^2}{\\sigma_X^2} = \\frac{\\left(\\sigma_T^2\\right)^2}{\\sigma_X^2 \\sigma_T^2}= \\frac{\\sigma_{XT}^2}{\\sigma^2_X\\sigma^2_T} = \\rho^2_{XT}.\n\\tag{5.2}\\]\nTo visualize how reliability relates true scores and observed scores see the structural diagram Figure 5.1.\nIt is important to emphasize that true scores are expected values over repeated observations and they do not necessarily correspond to an actual, tangible attribute of interest (Borsboom and Mellenbergh 2002). As a result, every measurement has a true score, regardless of whether it gauges a concrete attribute or not. For example, if we construct a test by summing the responses to the items: “how many languages can you confidently hold a conversation in?” and “Estimate the number of photos you’ve taken in the last year across all devices”. Even in such a nonsensical measure, the test’s composite score retains a true score, but this true score does not mirror a tangible reality.\n\n\n\nFigure 5.1: Structural diagram illustrating the relationship between true scores, observed scores, and error scores. The pink circle labeled \\(t\\) indicates the true scores, the blue squares labeled with \\(X\\) and \\(X'\\) represent observed scores on parallel measurements, and the red \\(E\\) denotes error. Correlations between \\(T\\), \\(X\\), and \\(X'\\) are in terms of reliability (\\(\\rho_{XX'}\\)). Note that \\(\\rho_{XX'}=\\rho_{XT}^2\\)."
  },
  {
    "objectID": "unreliability.html#reliability-vs-validity",
    "href": "unreliability.html#reliability-vs-validity",
    "title": "5  Unreliability",
    "section": "5.3 Reliability vs Validity",
    "text": "5.3 Reliability vs Validity\nReliability and validity are distinct properties in measurement. Validity pertains to whether the scores produced by a measure reflects the quantities it is intended to measure (p. 14 Kelley 1927). According to Borsboom, Mellenbergh, and Van Heerden (2004), a measure is valid if both of the following statements are true:\n\nThe attribute exists.\nVariations in the attribute causally produce variations in the outcomes of the measurement procedure.\n\nBorsboom’s formulation of validity is simpler and more practical than other formulations such as Cronbach and Meehl’s (1955) nomological network approach to validity. It is important to note that even if an attribute does not exist (statement 1), scores may still provide predictive utility. For example, socio-economic status (SES) is a formative quantity that is constructed from a composite of education, income, occupation status, etc. Although SES is not causal to these indicators, SES can still be used as a predictor of important life outcomes."
  },
  {
    "objectID": "unreliability.html#estimating-reliability",
    "href": "unreliability.html#estimating-reliability",
    "title": "5  Unreliability",
    "section": "5.4 Estimating Reliability",
    "text": "5.4 Estimating Reliability\nIn practice, reliability must be estimated through indirect methods, this is due to the fact that true scores and errors are unknown. There are many reliability estimators that can be used, however we will go over a selection of internal consistency estimators as well as test-retest stability estimators.\n\n5.4.1 Internal Consistency Estimators\nTaking multiple measurements and then averaging tends to provide a more stable estimate of true values. For instance, let’s consider the case of Francis Galton (1907), who conducted a study involving 787 individuals estimating the weight of an ox. On average, each person’s estimate deviated by approximately 37 pounds from the actual weight of the ox, which was recorded as 1198 pounds. However, when all the guesses were averaged together, the combined estimate was 1207 pounds, just a 9 pound difference from the true value. This principle can be extended to broader applications, such as measuring psychological constructs. If we were to assess someone’s level of extraversion using ratings from their mother, father, friend, and sibling, the average of their combined assessments would yield a more reliable score compared to relying solely on a single evaluator. So to create a more stable composite score (\\(X\\)), we can take the score from \\(\\kappa\\) items (\\(x\\)) and sum them such that,\n\\[\nX = x_1 + x_2 +...+x_\\kappa.\n\\]\nThe most commonly reported reliability estimator in the psychological sciences is coefficient alpha, also referred to as Cronbach’s alpha. Coefficient alpha, along with other internal consistency estimators, serves the purpose of assessing the reliability of composite scores comprising multiple item scores. Coefficient alpha was first derived by Lee Cronbach (see equation 13, Cronbach 1951) only requires three parameters to calculate, the number of measurements (\\(\\kappa\\)), the sample variances of each item (\\(S^2_{x_m}\\)), and the variance of the composite score (\\(S^2_{X}\\)). Coefficient alpha will estimate the reliability of the composite observed score (\\(r_{XX'}\\)),\n\\[\n_\\alpha r_{XX'} = \\frac{\\kappa}{\\kappa-1}\\left( 1 - \\frac{\\sum_{m=1}^\\kappa S^2_{x_m}}{S^2_{X}} \\right).\n\\tag{5.3}\\]\n\n\n\n\n\nFigures showing the observed scores upon 10 repeated measurements and the composite observed score for a single person (the true score is denoted with the dashed line). The left panel shows 10 observed scores with a lot of variation (i.e., low reliability). The composite score (dark red dot with error bars), shows wide error bars illustrating the low precision of the observed score score. The right panel also shows 10 observed scores with little variation (i.e., high reliability). The composite score (dark blue dot with error bars), shows narrow error bars illustrating the high precision of the observed score.\n\n\n\n\nWith tighter assumptions (see Haertel 2006), the formula for coefficient alpha can be simplified to just two parameters: the number of measurements (\\(\\kappa\\)) and the average correlation between measured scores (\\(\\bar{r}_{x_i x_j}\\), where \\(i\\neq j\\)). This formula is known as Spearman-Brown’s prophecy (see equation III of Charles Spearman 1910; or the last equation on page 299 of Brown 1910),\n\\[\n_\\text{sb} r_{XX'}= \\frac{\\kappa \\bar{r}_{x_i x_j}}{1+(\\kappa-1)\\bar{r}_{x_i x_j}}\n\\tag{5.4}\\]\nThis can be simplified further if we have two observed item scores. This formulation is a variation of split-half reliability:\n\\[\n_\\text{sh}r_{XX'}= \\frac{2r_{x_1 x_2}}{1+r_{x_1 x_2}}\n\\tag{5.5}\\]\nAll of these reliability estimators measure internal consistency, therefore they do not account for error outside of the measurement-specific error. There are other sources of error that internal consistency reliability estimates do not account for, such as transient error or rater-specific error.\n\n\n\nStructural model illustrating internal consistency. The pink circle labeled \\(T\\) indicates the true scores, the blue squares, \\(x_{1...\\kappa}\\), represent the observed sub-scores across multiple measurements, and the red \\(e\\) denotes error. The dark blue hexagon, \\(X\\), indicates a composite score as a sum of the observed scores (\\(\\mathbb{x}_{1...\\kappa}\\)). Note that \\(\\rho_{XX'}=\\rho_{XT}\\).\n\n\n\n\n\n\n\n\nApplied Example\n\n\n\nUsing the anxiety data set from the irr package (Gamer et al. 2019), we can estimate the reliability of three raters evaluating the anxiety in a sample of 20 individuals. Let us first load in the data:\n\nlibrary(irr)\n\n# load in data\ndata(\"anxiety\", package = \"irr\")\n\n# print data\nhead(anxiety, 4)\n\n  rater1 rater2 rater3\n1      3      3      2\n2      3      6      1\n3      3      4      4\n4      4      6      4\n\n\nWe can also use the alpha() function from the psych package (William Revelle 2023) to estimate coefficient alpha too. The output of alpha provides a lot of useful information, and it reports two types of alphas: standardized and raw. The standardized alpha is based on the correlations between items rather than the covariance between items, which is useful if items are on completely different scales (standardized alpha equivalent to the Spearman-Brown prophecy formula found in Equation 5.4). Also you will notice that the standardized alpha and ). If the ratings are on the same scale then the raw_alpha is preferred.\n\nlibrary(psych)\n\nreliability &lt;- alpha(anxiety)\n\nrXX &lt;- reliability$total$raw_alpha\n\nrXX\n\n[1] 0.4525862\n\n\nThe output shows a very low reliability between raters (\\(r_{XX'}=.45\\)).\n\n\n\n\n5.4.2 Test-Retest Stability Estimator\nTransient errors represent fluctuations in observed scores over time. These fluctuations, even if they are systematic (e.g., fatigue over the course of a single day), add extraneous within-person variance that can mask individual differences. Considering transient fluctuations as error depends on the research goal, so it is important for researchers to take care in considering which variance components should be considered error in their study (see Section 5.4.3). To estimate test-retest reliability, we can compute the correlation between the measurement at time 1 (\\(X_{t_1}\\)) and the second measurement at time 2 (\\(X_{t_2}\\)),\n\\[\n_\\text{tr}r_{XX'}= \\text{corr}(X_{t_1}X_{t_2}).\n\\]\nNote that calculating the pearson correlation coefficient between time-points ignores systematic changes (e.g., practice effects). We can visualize test-retest reliability in Figure 5.2 where the top panels show the correlations between time points and the bottom panels show the within-person change between time-points for scores with high and low reliability.\n\n\n\n\n\nFigure 5.2: Illustrating test-retest reliability. Top-left and top-right panels show the correlation between observed scores at both time-points for a measure that has low and high reliability, respectively. Bottom-left and bottom-right panels show the within-person change from time-point 1 to time-point 2 for scores with low and high reliability, respectively.\n\n\n\n\n\n\n5.4.3 Sources of Measurement Error\nMeasurement error can itself be broken down into multiple sources of error. Depending on the study, different sources of error may be more relevant than others. Different reliability estimators will account for different sources of error therefore it is important for researchers to choose the right reliability estimator for their study. A description of four of the most common sources of error is adapted from table 1 of Wiernik and Dahlke (2020):\n\nRandom Response Error: Genuine randomness in responses. Examples include: motor errors and variation in response time.\nTime/Environment-Specific (Transient) Error: Fluctuations in scores as a result of the specific time or environment of the measurement. For instance, if researchers administered an ability test to a sample of undergraduate students throughout the course of a day, the student’s who complete the test at the end of the day will likely perform worse than participant’s who completed due to fatigue rather than ability. Errors due to illness, mood, hunger, environmental distractors, etc. all fall under the umbrella of transient errors.\nInstrument-Specific Error: Error due to the specific content or make-up of the measurement instrument. For example, a psychological scale using Likert items may show participant’s idiosyncratic interpretations of questions and response options rather than their standing on the latent construct.\nRater/Observer-Specific Error: Errors induced by idiosyncratic biases of individual raters and rater by ratee interactions (e.g., Teacher A gives higher grades to students who stay after class).\n\nDifferent estimators of reliability account for different sources of measurement error therefore depending on the research design, it is important to carefully choose which reliability is most relevant for your use case. Note that even if two estimators account for the same types of measurement error, they likely hold different assumptions that may be violated in a given research context.\n\nList of reliability coefficients and the sources of error they account for. The sources of error are denoted by the columns labeled 1-4, corresponding to each of the four sources of error.\n\n\n\n\n\n\n\n\n\n\nEstimator\nDescription\n1\n2\n3\n4\n\n\n\n\nCoefficient Alpha\nInternal consistency coefficient for composite measures.\nX\n\nX\n\n\n\nCoefficient Omega\nInternal consistency coefficient for composite measures with specified factor structure.\nX\n\nX\n\n\n\nSplit-Half\nInternal consistency coefficient for measurements that are split into two halves.\nX\n\nX\n\n\n\nKuder-Richardson 20\nInternal consistency when observed scores are binary (special case of coefficient alpha).\nX\n\nX\n\n\n\nItem Response Theory Reliability\nReliability coefficient derived from item response theory (as opposed to classical test theory)\nX\n\nX\n\n\n\nInter-Rater/Inter-Observer Reliability\nConsistency in scoring between raters/observers.\nX\n\n\nX\n\n\nTest-Retest\nStability coefficient for repeated measurements across time\nX\nX\n\n\n\n\nDelayed Alpha\nAverage of all possible split-half reliabilities\nX\nX\nX\n\n\n\nG-Coefficient\nReliability coefficient derived from generalizability theory (G-theory). Can incorporate any source of error if enough data is present.\nX\nX\nX\nX"
  },
  {
    "objectID": "unreliability.html#sec-correcting-correlations-me",
    "href": "unreliability.html#sec-correcting-correlations-me",
    "title": "5  Unreliability",
    "section": "5.5 Correction for Bias in Correlations",
    "text": "5.5 Correction for Bias in Correlations\n\n5.5.1 Defining the Target Quantity\nContinuing with our emphasis on clearly defining our quantity of interest prior to applying any corrections, let us define it here. Our target is the population correlation between true scores of our independent and dependent variables. We can define the observed scores of the independent and dependent variables \\(X\\) and \\(Y\\) as,\n\\[\nX=T+E_X\n\\]\n\\[\nY=U+E_Y.\n\\]\nWhere \\(T\\) and \\(U\\) are the true scores for the independent and dependent variables, respectively. The population true score correlation can thus be be denoted by, \\(\\rho_{TU}\\), and can be defined as the standardized covariance,\n\\[\n\\rho_{TU} = \\frac{\\sigma_{TU}}{\\sigma_{T}\\sigma_{U}}.\n\\]\nIn a given study, we will only have access to the observed scores of the independent and dependent variables, therefore the study correlation is \\(r_{XY}\\). The relationship between the observed correlation and the true population correlation can be defined as,\n\\[\nr_{XY} = a\\rho_{TU} + \\varepsilon.\n\\]\nWhere \\(a\\) is the artifact attenuation factor (we will see that measurement error attenuates rather than inflates the correlation). An unbiased estimate of the true score population correlation (\\(\\rho_{TU}\\)) can then be calculated by dividing the observed score correlation by an estimate of the artifact attenuation factor,\n\\[\nr_{TU} = \\frac{r_{XY}}{\\hat{a}}.\n\\]\nThe measurement model can be visualized in Figure 5.3.\n\n\n\nFigure 5.3: This figure shows the relationship between the true scores, observed scores, and error scores. The true score correlation is denoted by the curved arrow connecting the circles indicating true score variables, \\(T\\) and \\(U\\).\n\n\n\n\n5.5.2 Artifactual Correction for Unreliability\nMeasurement error induces systematic bias in effect size estimates such as correlation coefficients C. Spearman (1904). In the population, let us assume there is some factor \\(a\\) that accounts for the systematic bias in observed score correlations (\\(\\rho_{XY}\\)) relative to true score correlations (\\(\\rho_{TU}\\)), such that\n\\[\n\\rho_{XY} = a \\rho_{TU}.\n\\]\nSince the correlation is defined as the covariance standardized by the standard deviations, the population correlation between true scores, \\(T\\) and \\(U\\), is defined as,\n\\[\n\\rho_{TU}=\\frac{\\sigma_{TU}}{\\sigma_{T} \\sigma_{U}}.\n\\]\nLikewise the correlation between the observed scores, \\(X\\) and \\(Y\\), would be the observed covariance divided by the observed standard deviations,\n\\[\n\\rho_{XY} =\\frac{\\sigma_{XY}}{\\sigma_{X} \\sigma_{Y}}.\n\\]\nHowever, if we assume that there is no covariance between errors in \\(X\\) and \\(Y\\) (\\(\\sigma_{E_X E_Y} = 0\\)), then the covariance between observed scores is only attributable to the covariance between true scores, therefore \\(\\sigma_{XY} = \\sigma_{TU}\\). This means that the observed score correlation can be expressed as,\n\\[\n\\rho_{XY} =\\frac{\\sigma_{TU}}{\\sigma_{X} \\sigma_{Y}}.\n\\tag{5.6}\\]\nNow the only difference between the observed score correlation and the true score correlation is the standard deviations in the denominator. In the presence of measurement error, the observed score standard deviations (\\(\\sigma_X\\) and \\(\\sigma_Y\\)) will be larger than the true score standard deviations (\\(\\sigma_{T}\\) and \\(\\sigma_{U}\\)). Using the definition of reliability, we can show how the observed variance is inflated compared to the true variance as a function of reliability. Since the reliability is defined as the ratio of true variance to total observed variance (see Equation 5.2), we can see how reliability inflates the observed variance,\n\\[\\begin{aligned}\n\\sigma^2_X &=\\sigma^2_{T} \\left(\\frac{\\sigma^2_{X}}{\\sigma^2_{T}} \\right)\\\\[.3em]\n&= \\sigma^2_{T}\\left(\\frac{1}{\\rho_{XX'}} \\right)\\\\[.3em]\n&= \\frac{\\sigma^2_{T}}{\\rho_{XX'}}.\n\\end{aligned}\\]\nTherefore the observed standard deviation is,\n\\[\n\\sigma_X = \\frac{\\sigma_{T}}{\\sqrt{\\rho_{XX'}}}.\n\\tag{5.7}\\]\nSince the reliability, and its square root, will be less than 1, then the observed score variance will be larger than the If we use the definition of an observed score correlation (Equation 5.6), then we can replace \\(\\sigma_X\\) and \\(\\sigma_Y\\) with \\(\\frac{\\sigma_{T}}{\\sqrt{\\rho_{XX'}}}\\) and \\(\\frac{\\sigma_{U}}{\\sqrt{\\rho_{YY'}}}\\), respectively. Now we can see how the observed score correlation differs from the true score correlation:\n\\[\\begin{aligned}\n\\rho_{XY} &= \\frac{\\sigma_{T U}}{\\left[\\frac{\\sigma_{T}}{\\sqrt{\\rho_{XX'}}} \\right] \\left[ \\frac{\\sigma_{U}}{\\sqrt{\\rho_{YY'}}} \\right] } \\\\[.3em]\n&= \\frac{\\sigma_{T U}}{\\sigma_{T}\\sigma_{U}}  \\sqrt{\\rho_{XX'}}\\sqrt{\\rho_{YY'}}\\\\[.3em]\n&= \\rho_{TU} \\sqrt{\\rho_{XX'}}\\sqrt{\\rho_{YY'}}\n\\end{aligned}\\]\nThis attenuation formula was first derived by Charles Spearman (1904). See Figure 5.4 for a visualization of the attenuation. Note that this formulation requires that there is no correlation between \\(E_X\\) and \\(E_Y\\) (\\(\\rho_{E_X E_Y}=0\\)).\n\n\n\n\n\nFigure 5.4: Visualizing the attenuation of an observed score correlation (\\(\\rho_{XY}\\)) due to measurement error. Top left panel shows a scatter plot of the correlation between the true scores of the independent (\\(T\\)) and dependent variable (\\(U\\)). Top right panel shows a scatter plot of the correlation between the observed scores of the independent (\\(X\\)) and dependent variable (\\(Y\\)). The bottom left panel shows the reliability of a single variable (\\(X\\)) and it’s relationship with the observed score correlation (\\(\\rho_{XY}\\)) while varying the true score correlation (true score correlations are represented as the red dots, i.e., when reliability is perfect; \\(\\rho_{TU}=\\{0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0\\}\\)). The bottom right panel shows attenuation of a a true score correlation of \\(\\rho_{TU}=.5\\), when both \\(X\\) and \\(Y\\) are affected by measurement error. The darker red indicates a larger observed correlation, where it peaks at an observed score correlation of .50 (when reliability of \\(X\\) and \\(Y\\) is 1) which is equivalent to the true score correlation\n\n\n\n\nLet us recall that we can express the observed score correlation as a function of the true score population correlation (i.e., the target quantity, \\(\\rho_{TU}\\)),\n\\[\nr_{XY} = a\\rho_{TU} + e.\n\\]\nWhere \\(e\\) is the sampling error and \\(a\\) is the artifact attenuation factor. \\(a\\) can now be replaced with the square root of the reliabilities of \\(X\\) and \\(Y\\).\n\\[\nr_{XY} = \\left(\\sqrt{\\rho_{XX'}}\\sqrt{\\rho_{YY'}}\\right) \\rho_{TU} + e.\n\\]\nTherefore we can correct the observed score study correlation by dividing by an estimate the attenuation factor using,\n\\[\nr_{TU} = \\frac{r_{XY}}{\\hat{a}} = \\frac{r_{XY}}{\\sqrt{r_{XX'}}\\sqrt{r_{YY'}}}\n\\]\nThe corrected correlation coefficient is an unbiased estimator of the target quantity, \\(\\rho_{TU}\\). We also need to obtain the standard error of the corrected correlation coefficient. The standard error of the uncorrected correlation, \\(se\\), can be adjusted directly with one of two formulas:\n\nIf the reliability coefficients are estimated from the same sample as the correlation, then the standard error formula is,\n\n\\[\nse_c =  \\frac{se}{\\sqrt{r_{XX'}}\\sqrt{r_{YY'}}}\n\\]\n\nIf the reliability coefficients and the correlation are all estimated from different samples (\\(n\\) will denote the sample size of the standardized mean difference, \\(n_{r_{XX'}}\\) and \\(n_{r_{YY'}}\\) and will denote the sample size of the respective reliability coefficients) then the standard error of the corrected correlation is approximately (for the derivation, see the appendix of this section),\n\n\\[\nse(r_{TU}) = \\sqrt{\\frac{se(r_{XY})^2}{r_{XX'}r_{YY'}} +\n\\frac{r^2_{XY}\\left(1-r^2_{XX'}\\right)^2}{4(n_{r_{XX'}}-1)r_{XX'}^3r_{YY'}} +\n\\frac{r^2_{XY}\\left(1-r^2_{YY'}\\right)^2}{4(n_{r_{YY'}}-1)r_{XX'}r_{YY'}^3}}\n\\tag{5.8}\\]\nThe two ways of calculating standard error are due to the fact that when the reliability and correlation coefficient are computed from the same sample, their sampling errors will be correlated (Bobko and Rieck 1980). Note that if you only want to correct for unreliability in one variable, than you can set the reliability of the other variable to 1 in all the equations above.\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say we wanted to estimate the correlation between academic motivation and test performance among a sample of \\(n=100\\) students. Motivation scores are obtained using a self-report measure with a reliability of \\(r_{XX'}=.80\\). Test performance is the sum of correctly answered multiple choice questions and the total scores were also fairly reliable (\\(r_{YY'}=.80\\)). The correlation between observed motivation and performance scores is \\(r_{XY}=.40\\). To correct the correlation for measurement error, we can use the correct_r function in the psychmeta package (Dahlke and Wiernik 2019).\n\nlibrary(psychmeta)\n\nrXY &lt;- .40  # correlation between X and Y\nrXX &lt;- .80  # reliability of X\nrYY &lt;- .80  # reliability of Y\nn &lt;- 100    # sample size\n\n# correct correlation\ncorrect_r(correction = 'meas',\n          rxyi = rXY,\n          rxx = rXX,\n          ryy = rYY,\n          n = n)\n\nCorrelations Corrected for Measurement Error:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1   0.5    0.276    0.691 100        51.5\n\n\nThe estimate of the true score correlation is \\(r_{TU} = .50\\;[.28, .69]\\)."
  },
  {
    "objectID": "unreliability.html#correction-for-bias-in-standardized-mean-differences-d",
    "href": "unreliability.html#correction-for-bias-in-standardized-mean-differences-d",
    "title": "5  Unreliability",
    "section": "5.6 Correction for Bias in Standardized Mean Differences (d)",
    "text": "5.6 Correction for Bias in Standardized Mean Differences (d)\n\n5.6.1 Defining the Target Quantity\nPrior to correcting for measurement error let us define our target. Our target is the difference in the means of group \\(A\\) and \\(B\\) with respect to the true scores, \\(U\\), of our dependent variable, \\(Y\\),\n\\[\nY_A = U_A + E_A\n\\]\n\\[\nY_B = U_B + E_B.\n\\]\nWhere \\(U_A\\) and \\(U_B\\) are the true scores for group \\(A\\) and group \\(B\\), respectively. The true score standardized mean difference can thus be be denoted by, \\(\\delta_{gU}\\) (\\(g\\) indicates the grouping variable, and \\(U\\) denotes the continuous true score outcome), and can be defined as the mean difference divided by the within-group standard deviation,\n\\[\n\\delta_{gU} = \\frac{\\mu_{U_A} - \\mu_{U_B}}{\\sigma_{U}}.\n\\]\nWhere \\(\\mu_U\\) indicates the population mean of true scores. The relationship between the population true score standardized mean difference (\\(\\delta_{gU}\\)) can be related to the observed study standardized mean difference with the following formulation:\n\\[\nd_{gY} = a\\delta_{gU}+e.\n\\]\nWhere \\(a\\) is the attenuation factor induced by measurement error and \\(e\\) denotes sampling error. To obtain an unbiased estimate of true score standardized mean difference, \\(\\delta_{gU}\\), we can correct the observed standardized mean difference by dividing by an estimate of \\(a\\),\n\\[\nd_{gU} = \\frac{d_{gY}}{\\hat{a}}.\n\\]\n\n\n5.6.2 Artifact Correction for Unreliability\nThe population mean of true scores and observed scores are identical since measurement error only affects the variance in scores. Therefore the population mean difference is equal between true and observed scores:\n\\[\n\\mu_{U_A} - \\mu_{U_B} = \\mu_{Y_A} - \\mu_{Y_B}\n\\]\nNote that this means that an unstandardized mean difference is not biased by measurement error. Since the mean difference in true scores and observed scores are the same, the observed score population standardized mean difference can also be expressed as the mean difference of true scores standardized by the observed score standard deviation,\n\\[\n\\delta_{gY} = \\frac{\\mu_{Y_A} - \\mu_{Y_B}}{\\sigma_Y}  = \\frac{\\mu_{U_A} - \\mu_{U_B}}{\\sigma_Y}\n\\]\nWe know from Equation 5.2 that the standard deviation of true scores, \\(\\sigma_U\\), can be expressed as a function of reliability and the observed score standard deviation,\n\\[\n\\sigma_U = \\sigma_Y\\sqrt{\\rho_{YY'}}\n\\tag{5.9}\\]\nTherefore the true score standardized mean difference could be corrected for by dividing the observed score standardized mean difference by the reliability,\n\\[\n\\delta_{gU} = \\frac{\\mu_{U_A} - \\mu_{U_B}}{\\sigma_Y\\sqrt{\\rho_{YY'}}} = \\frac{\\delta_{gY}}{\\sqrt{\\rho_{YY'}}}\n\\]\nThe reliability in this case is the within-group reliability (\\(\\rho_{YY'}=\\rho_{YY'_A}=\\rho_{YY'_B}\\)) which is assumed to be equal between groups. Similarly, a sample estimator of the observed score standardized mean difference can be expressed as,\n\\[\nd_{gY} = \\frac{\\overline{Y}_A - \\overline{Y}_B}{S_{Y_P}}  = \\frac{\\overline{U}_A - \\overline{U}_B}{S_{Y_P}}\n\\]\nWhere \\(\\overline{Y}\\) and \\(\\overline{U}\\) are sample means of observed scores and true scores, respectively. The within-group standard deviation of observed scores can be estimated by pooling the standard deviation from both groups,\n\\[\nS_{Y_P} = \\sqrt{\\frac{(n_A+1)S^2_{Y_A}+(n_B+1)S^2_{Y_B}}{n_A + n_B - 2}}.\n\\]\nWhere \\(n_A\\) and \\(n_B\\) are the sample sizes within group \\(A\\) and group \\(B\\), respectively.\n\nPooled Reliability\nTo obtain the pooled standard deviation of true scores, we can correct the observed score standard deviations for measurement error such that, \\(S^2_{Y}=r_{YY'}S^2_{U}\\) (similar to what we see in Equation 5.9). Therefore we can compute the pooled true score standard deviation (\\(S_{U_P}\\)) with,\n\\[\nS_{U_P} = \\sqrt{\\frac{(n_A+1)\\left(S_{Y_A} \\sqrt{r_{YY'_A}}\\right)^2+(n_B+1)\\left(S_{Y_B} \\sqrt{r_{YY'_B}}\\right)^2}{n_A + n_B - 2}}.\n\\]\nAlternatively, we can pool the reliability and the true score standard deviations separately,\n\\[\nS_{Y_P} = \\sqrt{\\frac{(n_A+1)S_{Y_A}^2+(n_B+1)S_{Y_B}^2}{n_A + n_B - 2}}\n\\]\n\\[\nr_{YY'_P} = \\sqrt{\\frac{(n_A+1) r_{YY'_A}^2+(n_B+1)r_{YY'_B}^2}{n_A + n_B - 2}}.\n\\]\nThen we can express \\(S_{U_P}\\) as the pooled standard deviation and the pooled reliability,\n\\[\nS_{U_P} = S_{Y_P}\\sqrt{r_{YY'_P}}.\n\\]\nNow we can calculate the corrected sample standardized mean difference, so that it estimates the standardized mean difference in true scores rather than observed scores:\n\\[\nd_{gU} = \\frac{\\overline{U}_A - \\overline{U}_B}{S_{U_P}} =  \\frac{\\overline{U}_A - \\overline{U}_B}{S_{Y_P}\\sqrt{r_{YY'_P}}} = \\frac{d_{gY}}{\\sqrt{r_{YY'_P}}}\n\\]\nThe corrected (true score) standardized mean difference, \\(d_{gU}\\), will also need it’s standard error to be adjusted from the observed score estimate. The standard error can be computed one of two ways:\n\nIf the reliability coefficient and the standardized mean difference are computed from the same sample then the standard error can be estimated by, \\[\nse(d_{gU}) = \\frac{se(d_{gY})}{\\sqrt{r_{YY'_P}}}.\n\\]\nIf the reliability coefficient and the standardized mean difference are computed from separate samples (\\(n\\) will denote the sample size of the standardized mean difference, \\(n_{r_{YY'}}\\) and will denote the sample size of the reliability coefficient), we can use the following formulation (see appendix for derivation),\n\n\\[\nse(d_{gU}) = \\sqrt{\\frac{se(d_{gY})^2}{r_{YY'_P}} +\\frac{d^2_{gY}\\left(1-r^2_{YY'_P}\\right)^2}{4\\left(1-n_{r_{YY'_P}}\\right)r^3_{YY'_P}}}\n\\tag{5.10}\\]\n\n\nTotal Sample Reliability\nIt is common that studies will only report the full sample reliability and not the reliability within each group. If the groups differ substantially on the dependent variable, \\(Y\\), then the total sample reliability will over-estimate the within-group reliability. When the total sample reliability is all that is available, we can correct the standardized mean difference by first converting \\(d_{gY}\\) to a point-biserial correlation, \\(r_{gY}\\). To do this we also need the observed proportion of subjects in group \\(A\\) or \\(B\\) (\\(p_g\\) ; it does not matter which group is chosen, as long as it is consistent throughout).\n\\[\nr_{gY} = \\frac{d_{gY}}{\\sqrt{\\frac{1}{p_g(1-p_g)}+d_{gY}^2}}.\n\\]\nThen we can the correlation coefficient as we did in Section 5.5.2. Taking the reliability of \\(Y\\), the \\(r_{gY}\\) for the total sample reliability (see Section 5.5 for details),\n\\[\nr_{gU} = \\frac{r_{gY}}{\\sqrt{r_{YY'}}}\n\\]\nThen we can obtain the corrected (true score) standardized mean difference by converting \\(r_{gU}\\) back into \\(d_{gU}\\),\n\\[\nd_{gU} = \\frac{r_{gU}}{\\sqrt{p_g(1-p_g)(1-r_{gU}^2)}}\n\\]\nThe standard error of the corrected value will also need to be adjusted. The same process can be done for the sampling variance as well, but instead we can put it all into one equation,\n\\[\nse_{c} = \\frac {se\\left(\\frac{r_{gU}}{r_{gY}}\\right)} {\\sqrt{\\left(1+d_{gY}^2p_g[1-p_g]\\right)^3\\left(1-r_{gU}^2\\right)^3}}\n\\]\nNote that this formula is specifically for the case where the standardized mean difference and the reliability coefficient are estimated from the same sample. If they are estimated from separate samples, then we can convert the standard error of the standardized mean difference to a standard error of a point-biserial correlation and then use Equation 5.8. Once the point-biserial standard error is corrected, then it can be converted back to a correlation coefficient.\n\n\n\n\n\nFigure 5.5: Visualizing the attenuation of the standardized mean differences in observed scores (bottom panel) from true scores (top panel). The true score standardized mean difference is \\(\\delta_{gU}=0.60\\), but in the presense of measurement error (\\(\\rho_{YY'}=.70\\)) the observed score standardized mean difference is \\(\\delta_{gY}=0.50\\). This occurs due to the increased standard deviation in observed scores relative to true scores (the mean difference does not change).\n\n\n\n\n\n\n\n\n\n\nApplied Example in R\n\n\n\nImagine we want to test the effectiveness of a drug that is meant to alleviate stress. Therefore we conduct a study and randomize a sample of 150 people into a control group (\\(n=75\\)) and a treatment group (\\(n=75\\)). After the intervention, the subjects report their stress levels on a self-report scale. The scale was shown to have a reliability of \\(r_{YY'}=.80\\). We can correct the standardized mean differences on observed scores with the correct_d function in the psychmeta package (Dahlke and Wiernik 2019).\n\nlibrary(psychmeta)\n\ndgY &lt;- .40\nrYY &lt;- .80\nn1 &lt;- n2 &lt;- 75\n\ncorrect_d(correction = 'meas',\n          d = dgY,\n          ryy = rYY,\n          n1 = n1,\n          n2 = n2)\n\nd Values Corrected for Measurement Error:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.449   0.0828    0.838 150         118\n\n\nThe output of psychmeta shows a slightly larger (\\(d_{gU} = 0.45\\,[0.08,0.84]\\))."
  },
  {
    "objectID": "unreliability.html#sec-lim-information",
    "href": "unreliability.html#sec-lim-information",
    "title": "5  Unreliability",
    "section": "5.7 Estimating Reliability with Limited Information",
    "text": "5.7 Estimating Reliability with Limited Information\nReliability estimates should preferably be calculated from within the study’s sample, however there are a couple of ways to estimate reliability when this information is not provided. A common way to obtain an estimate of the reliability is to look in meta-analyses or a test manuals. If the number of items in a study differs from the test manual, you can approximate the reliability of the study’s test, with a re-arrangement of the Spearman-Brown prophecy formula,\n\\[\nr_{xx'_{study}} \\approx \\frac{1}{\\frac{\\kappa_{\\text{ref}}}{\\kappa_{\\text{study}}} \\left(\\frac{1}{r_{xx'_{study}}} - 1\\right) + 1}.\n\\]\nWhere \\(\\kappa_{\\text{ref}}\\) and \\(\\kappa_{\\text{study}}\\) denote the number of items in the reference test and the test used in the study, respectively."
  },
  {
    "objectID": "unreliability.html#appendix",
    "href": "unreliability.html#appendix",
    "title": "5  Unreliability",
    "section": "5.8 Appendix",
    "text": "5.8 Appendix\n\nDeriving the standard error of a corrected correlation with independent sampling errors\nEquation 5.8 - This Taylor series approximation derivation can also be found in the Mathematica script, Derivations.nb, denoted as (SE.1).\n\\[\\begin{aligned}\n\\tiny{se_c} &\\tiny{\\approx \\sqrt{\\frac{\\partial}{\\partial r_{XY}}\\left[\\frac{r_{XY}}{\\sqrt{r_{XX'}}\\sqrt{r_{YY'}}}\\right]^2 \\widehat{var}(r_{XY}) + \\frac{\\partial}{\\partial r_{XX'}}\\left[\\frac{r_{XY}}{\\sqrt{r_{XX'}}\\sqrt{r_{YY'}}}\\right]^2  \\widehat{var}(r_{YY'}) + \\frac{\\partial}{\\partial r_{XX'}}\\left[\\frac{r_{XY}}{\\sqrt{r_{XX'}}\\sqrt{r_{YY'}}}\\right]^2 \\widehat{var}(r_{YY'})}}\\\\[.3em]\n&\\tiny{\\approx \\sqrt{ \\left(\\frac{1}{r_{XX'}r_{YY'}}\\right) se^2 + \\left( -\\frac{r_{XY}}{2\\sqrt{r_{XX'}^3r_{YY'}}} \\right)^2\\frac{\\left(1-r^2_{XX'}\\right)^2}{n_{r_{XX'}}-1}+  \\left(  -\\frac{r_{XY}}{2\\sqrt{r_{XX'}r_{YY'}^3}} \\right)^2\\frac{\\left(1-r^2_{YY'}\\right)^2}{n_{r_{YY'}}-1}}}\\\\[.3em]\n&\\tiny{\\approx \\sqrt{\\frac{se^2}{r_{XX'}r_{YY'}} +\n\\frac{r^2_{XY}\\left(1-r^2_{XX'}\\right)^2}{4(n_{r_{XX'}}-1)r_{XX'}^3r_{YY'}} +\n\\frac{r^2_{XY}\\left(1-r^2_{YY'}\\right)^2}{4(n_{r_{YY'}}-1)r_{XX'}r_{YY'}^3}}}\n\\end{aligned}\\]\n\n\nDeriving the standard error of a corrected correlation with independent sampling errors\nEquation 5.10 - Found in Derivations.nb, denoted as (SE.2).\n\n\n\n\nBobko, Philip, and Angela Rieck. 1980. “Large Sample Estimators for Standard Errors of Functions of Correlation Coefficients.” Applied Psychological Measurement 4 (3): 385–98. https://doi.org/10.1177/014662168000400309.\n\n\nBorsboom, Denny, and Gideon J Mellenbergh. 2002. “True Scores, Latent Variables, and Constructs: A Comment on Schmidt and Hunter.”\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap Van Heerden. 2004. “The Concept of Validity.” Psychological Review 111 (4): 1061–71. https://doi.org/10.1037/0033-295X.111.4.1061.\n\n\nBrown, William. 1910. “Some Experimental Results in the Correlation of Mental Abilities1.” British Journal of Psychology, 1904-1920 3 (3): 296–322. https://doi.org/10.1111/j.2044-8295.1910.tb00207.x.\n\n\nCronbach, Lee J. 1951. “Coefficient Alpha and the Internal Structure of Tests.” Psychometrika 16 (3): 297–334. https://doi.org/10.1007/BF02310555.\n\n\nCronbach, Lee J., and Paul E. Meehl. 1955. “Construct Validity in Psychological Tests.” Psychological Bulletin 52 (4): 281–302. https://doi.org/10.1037/h0040957.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75 (1949): 450–51. https://doi.org/10.1038/075450a0.\n\n\nGamer, Matthias, Jim Lemon, Ian Fellows, and Puspendra Singh. 2019. Irr: Various Coefficients of Interrater Reliability and Agreement. https://CRAN.R-project.org/package=irr.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of meta-analysis: correcting error and bias in research findings (third). Third. Thousand Oaks, California: Sage Publications.\n\n\nKelley, Truman Lee. 1927. Interpretation of Educational Measurements. World Book Company.\n\n\nMendoza, Jorge L., and Michael Mumford. 1987. “Corrections for Attenuation and Range Restriction on the Predictor.” Journal of Educational Statistics 12 (3): 282–93. https://doi.org/10.3102/10769986012003282.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” International Journal of Epidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nSpearman, Charles. 1910. “Correlation Calculated from Faulty Data.” British Journal of Psychology 3 (3): 271295. https://www.proquest.com/docview/1293688112/citation/7E133DC1091D4E47PQ/1.\n\n\nViswesvaran, Chockalingam, Deniz S. Ones, Frank L. Schmidt, Huy Le, and In-Sue Oh. 2014. “Measurement Error Obfuscates Scientific Knowledge: Path to Cumulative Knowledge Requires Corrections for Unreliability and Psychometric Meta-Analyses.” Industrial and Organizational Psychology 7 (4): 507–18. https://doi.org/10.1017/S1754942600006799.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611.\n\n\nWilliam Revelle. 2023. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University. https://CRAN.R-project.org/package=psych."
  },
  {
    "objectID": "misclassification.html#introduction",
    "href": "misclassification.html#introduction",
    "title": "6  Group Misclassification",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nGroup misclassification describes a situation where true group membership (e.g., people with a disorder) does not perfectly match the observed group membership (e.g., people diagnosed with a disorder). Group misclassification can be considered a type of measurement error where instead of accounting for errors in continuous variables (i.e., unreliability), group misclassification accounts for errors in categorical variables."
  },
  {
    "objectID": "misclassification.html#defining-group-misclassification",
    "href": "misclassification.html#defining-group-misclassification",
    "title": "6  Group Misclassification",
    "section": "6.2 Defining Group Misclassification",
    "text": "6.2 Defining Group Misclassification\nMisclassification can be defined as any deviations between true group membership and observed group membership. Let us imagine two groups, group \\(A\\) and group \\(B\\). In order to identify members of group \\(A\\) and group \\(B\\), we have to use some measurement procedure. We can also suppose that this measurement instrument produces imperfect group assignments, that is, people who are actually in group \\(A\\) are sometimes assigned group \\(B\\) and vice versa. We can visualize the performance of the classification procedure with a contingency table (see Table 6.1) between actual group membership (\\(G\\)) and observed group membership (\\(g\\)):\n\n\n\n\n\n\nTable 6.1: Contingency table between assigned group membership and actual group membership.\n\n\n\n\\(\\boldsymbol{G=A}\\)\n\\(\\boldsymbol{G=B}\\)\n\n\n\\(\\boldsymbol{g=A}\\)\n\\(n_{AA}\\)\n\\(n{BA}\\)\n\n\n\\(\\boldsymbol{g=B}\\)\n\\(n_{AB}\\)\n\\(n_{BB}\\)\n\n\n\n\n\n\n\n\nWe can see from the contingency table that individual’s who were correctly classified, would be labeled in the cell block \\(AA\\) or \\(BB\\) and those who were misclassified would belong to cells \\(BA\\) and \\(AB\\). Therefore we can define the proportion of individuals that are accurately classified as \\(p_{\\text{acc}} = \\frac{n_{AA} + n_{BB}}{n_{AA} + n_{BB} + n_{AB} + n_{BA}}\\) whereas the proportion of people misclassified can be defined as \\(p_{\\text{mis}} = \\frac{n_{AB} + n_{BA}}{n_{AA} + n_{BB} + n_{AB} + n_{BA}}\\). A high-quality classifier would would minimize \\(p_{\\text{mis}}\\) and maximize \\(p_{\\text{acc}}\\). Note that the proportion of people misclassified is inversely proportional to the proportion of people accurately classified such that, \\(p_{\\text{mis}} = 1-p_{\\text{acc}}\\)."
  },
  {
    "objectID": "misclassification.html#classification-reliability",
    "href": "misclassification.html#classification-reliability",
    "title": "6  Group Misclassification",
    "section": "6.3 Classification Reliability",
    "text": "6.3 Classification Reliability\nSimilar to quantifying reliability in continuous variables by calculating the correlation in parallel sets of observed scores, the same can be done in categorical variables. Instead of a contingency table between observed (\\(g\\)) and true (\\(G\\)) group membership, we will instead create a contingency table of two measurements producing two sets of observed group assignments (\\(g\\) and \\(g'\\)). Measurements often will take the form of inter-rater assessments, for example, two clinician’s diagnosis of Major Depressive Disorder (MDD) in the same sample of patients.\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{g=A}\\)\n\\(\\boldsymbol{g=B}\\)\n\n\n\\(\\boldsymbol{g'=A}\\)\n\\(n_{AA}\\)\n\\(n_{BA}\\)\n\n\n\\(\\boldsymbol{g'=B}\\)\n\\(n_{AB}\\)\n\\(n_{BB}\\)\n\n\n\n\n\n\n\nTo obtain the reliability of the group assignments, we can calculate the correlation coefficient between \\(g\\) and \\(g'\\). Since both variables are categorical, a Pearson correlation coefficient would be equivalent to a phi coefficient. The phi coefficient is often referred to as Matthew’s correlation coefficient and is most frequently used as an index of performance of a binary classifier in machine learning. Let’s denote the reliability (i.e., the correlation between \\(g\\) and \\(g'\\)) as \\(\\rho_{gg'}\\). Remember that reliability from the chapter on unreliability can be defined as the square of the correlation between true scores and observed scores. As is the case here, we can define classification reliability as the square of the correlation between assigned group membership and actual group membership,\n\\[\n\\rho_{gg'} = \\rho^2_{gG}\n\\]\nThere are a few ways to obtain a sample estimate of \\(\\rho_{gg'}\\) (\\(r_{gg'}\\)). The first way is to calculate the sample estimate directly from a contingency table,\n\\[\nr_{gg'} = \\frac{n_{AA}n_{BB}-n_{AB}n_{BA}}{\\sqrt{(n_{AA}+n_{BA})(n_{AB}+n_{BB})(n_{AA}+n_{AB})(n_{BA}+n_{BB})}}.\n\\]\nWhere \\(n_{AA}\\), \\(n_{BB}\\), \\(n_{AB}\\), and \\(n_{BA}\\) are the number of subjects within their respective cells of the contingency table. If the values of the contingency table are not available, we can calculate the reliability from the \\(\\chi^2\\)-statistic,\n\\[\nr_{gg'} = \\sqrt{\\frac{\\chi^2}{n}}.\n\\]\nWhere \\(n\\) is the total sample size (sum of all cells). If the \\(\\chi^2\\)-statistic is unavailable, we can approximate the reliability from the accuracy (\\(p_{\\text{acc}}\\)) or the proportion of people misclassified (\\(p_{\\text{mis}}\\)),\n\\[\nr_{gg'} = (2p_{\\text{acc}}-1)^2 = (1-2p_{\\text{mis}})^2.\n\\]\nThis approximation assumes that the group sizes are approximately equal and the misclassification rates are approximately equal between groups. Otherwise, \\(r_{gg'}\\) will be overestimated (Wiernik and Dahlke 2020).\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say that a researcher wants to calculate the reliability of a clinician’s diagnoses of major depressive disorder. They hire two clinicians to assign a sample of 100 patients to a control group or a major depressive disorder group. The researcher runs a \\(\\chi^2\\)-test to test the association between the clinicians group assignments and it returns \\(\\chi^2=54.5\\). We can then calculate the reliability of classification using base R.\n\nchi2 &lt;- 54.5\nn &lt;- 100\n\n# calculate reliability from chi squared statistic\nrgg &lt;- sqrt(chi2/n)\n\n# print reliability\nrgg\n\n[1] 0.7382412\n\n\nThe clinicians show a fair level of agreement with a classification reliability of \\(r_{gg'}=.74\\)."
  },
  {
    "objectID": "misclassification.html#correcting-for-group-misclassification-in-standardized-mean-difference",
    "href": "misclassification.html#correcting-for-group-misclassification-in-standardized-mean-difference",
    "title": "6  Group Misclassification",
    "section": "6.4 Correcting for Group Misclassification in Standardized Mean Difference",
    "text": "6.4 Correcting for Group Misclassification in Standardized Mean Difference\n\n6.4.1 Defining our Target Quantity\nOur quantity of interest is the true score population standardized mean difference, \\(\\delta_{GU}\\), between actual members of group \\(A\\) and group \\(B\\) on the true scores of the dependent variable, \\(U\\). However, the observed sample standardized mean difference (\\(d_{gY}\\)) is estimating the difference in observed scores between individuals who are assigned group to \\(A\\) and group \\(B\\). Non-differential error in the assignment of groups (i.e., group misclassification) will bias the observed correlation. We can model the observed standardized mean difference as a function of the target quantity, \\(\\delta_{GU}\\),\n\\[\nd_{gY} = a\\delta_{GU} + e.\n\\]\nWhere \\(a\\) is the artifact attenuation factor and \\(e\\) denotes the sampling error. Therefore an unbiased estimate of the target standardized mean difference can be obtained by dividing the observed standardized mean difference by an estimate of the artifact attenuation factor,\n\\[\nd_{GU} = \\frac{d_{gY}}{\\hat{a}}.\n\\]\n\n\n6.4.2 Artifact Correction for Standardized Mean Difference\nThe standardized mean differences will become biased when subject’s assigned groups differ from their actual group. This is partially due to the fact that the means of each group are driven closer to one another. Let us suppose that, on average, group \\(A\\) and group \\(B\\) score differently on some outcome, \\(Y\\). When some subjects are erroneously assigned to the incorrect group, the observed mean within each group will reflect a weighted average true means of both groups. This is due to the fact that the misclassified individuals are being drawn from a population with a different mean. To calculate the mean of the observed groups we must incorporate the true mean of the correctly classified subjects and the misclassified subjects,\n\\[\n\\overline{Y}^\\text{obs}_A = \\left(\\frac{n_{AA}}{n_{AA}+n_{BA}}\\right)\\overline{Y}^\\text{true}_A + \\left(\\frac{n_{BA}}{n_{AA}+n_{BA}}\\right)\\overline{Y}^\\text{true}_B\n\\]\n\\[\n\\overline{Y}^\\text{obs}_A = \\left(\\frac{n_{BB}}{n_{BB}+n_{AB}}\\right)\\overline{Y}^\\text{true}_B + \\left(\\frac{n_{AB}}{n_{BB}+n_{AB}}\\right)\\overline{Y}^\\text{true}_A.\n\\]\nFrom the above equations, it becomes evident that as the number of misclassified individuals increases (\\(n_{AB}\\) and \\(n_{BA}\\)), the observed means of each group gradually converge towards each other. As the means converge, the standardized mean difference will correspondingly shift toward zero. To illustrate this phenomenon, Figure 6.1 shows the distributions for groups \\(A\\) and \\(B\\) without any misclassification. In this case, there is no attenuation of the standardized mean difference.\n\n\n\n\n\nFigure 6.1: Distributions of scores without misclassification. True mean difference and observed mean differ only due to sampling error. Red squares denote actual group \\(A\\) members, blue circles denote actual group \\(B\\) members.\n\n\n\n\nIf some individuals are assigned to the incorrect group, then we will see attenuation in the standardized mean difference as the means converge. Figure 6.2 is showing what happens when the group misclassification rate is 10%. A group misclassification rate of 10% is equivalent to a classification reliability of \\(r_{gg'}=.64\\).\n\n\n\n\n\nFigure 6.2: Distributions of scores with a 10% misclassification rate. Observed standardized mean differences are biased toward the null (i.e., \\(\\delta\\) = 0). Red squares denote actual group \\(A\\) members, blue circles denote actual group \\(B\\) members. Note that a few members of group \\(A\\) (red squares) are within assigned group \\(B\\) and vice versa (indicative of misclassification).\n\n\n\n\nIt is important to note that for many of the corrections converting the standardized mean difference to a point-biserial correlation is often a necessary step. However, once the corrected point-biserial correlation is obtained, the correlation can then be converted back into a standardized mean difference. To correct for bias induced by misclassification we first need to convert the observed standardized mean difference to a point-biserial correlation coefficient by using the observed proportion of the sample that has been assigned to either group \\(A\\) or group \\(B\\) (\\(p_g\\)). The group proportion \\(p_g\\) in the following equations will only show up in the term \\(p_g(1-p_g)\\) so it will not matter which group is used to calculate the proportion. Converting \\(d_{gY}\\) to \\(r_{gY}\\):\n\\[\nr_{gY} = \\frac{d_{gY}}{\\sqrt{\\frac{1}{p_g(1-p_g)}-d_{gY}^2}}.\n\\]\nWe can then correct the point-biserial correlation for group misclassification by dividing by the square root of the classification reliability. Since we also want to correct for measurement error in the continuous dependent variable, \\(Y\\), we can simultaneously apply the correction for unreliability:\n\\[\nr_{GU} = \\frac{r_{gY}}{\\sqrt{r_{gg'}}\\sqrt{r_{YY'}}}.\n\\]\nNow we can convert the corrected point-biserial correlation into a corrected standardized mean difference (\\(d_{GU}\\)). When converting back to a standardized mean difference, we need to use the true group proportions, \\(p_G\\). Although if we are to assume equal misclassification rates between groups, then the observed proportion can be used \\(p_g\\):\n\\[\nd_{GU} = \\frac{r_{GU}}{\\sqrt{p_G\\left(1-p_G\\right)\\left(1-r_{GU}^2\\right)}}.\n\\]\nThis process of converting, correcting, and then back-converting must also be done for the standard error. To avoid redundancy, we can incorporate each step into a single equation:\n\\[\n\\small{se(d_{GU}) = \\frac {se(d_{gY})\\times r_{GU}} {r_{gY}\\sqrt{\\left(1+d_{gY}^2p[1-p]\\right)^2\\left(d_{gY}^2+\\frac{1}{p_g(1-p_g)}\\right)p_G(1-p_G)(1-r_{GU}^2)^3}}.}\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nA researcher wants to compare the academic performance (measured by a standardized test) of people with and without Major Depressive Disorder (MDD). The researcher reports a classification reliability of \\(r_{gg'}=.80\\) and a reliability of the standardized test as \\(r_{YY'}=.85\\). The researcher than finds a standardized mean difference of \\(d_{gY}=.30\\) favoring controls (i.e., controls had a higher average on the test). Using the correct_d function in the psychmeta package, we can obtain an unbiased estimate of the target standardized mean difference.\n\nlibrary(psychmeta)\n\ncorrect_d(correction = \"meas\",\n          d = .30,   # observed standardized mean difference\n          ryy = .85, # reliability of dependent variable\n          rGg = sqrt(.80), # sqrt of classification reliability\n          n1 = 100, # sample size in controls\n          n2 = 100) # sample size in people with MDD\n\nd Values Corrected for Measurement Error:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.366   0.0238    0.726 200         133\n\n\nThe corrected standardized mean difference is \\(d_{GU}=0.37\\, [0.02,\\, .73]\\).\n\n\n\n\n\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "artificial_dichotomization.html#introduction",
    "href": "artificial_dichotomization.html#introduction",
    "title": "7  Artificial Dichotomization",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nResearchers occasionally split naturally continuous variables into two discrete groups to increase interpretability or conduct specific analyses (e.g., t-tests). However, artificially dichotomizing variables introduces measurement error variance thus attenuating effect size estimates Maxwell and Delaney (1993). The obvious solution to this problem is to simply not dichotomize variables, however if only summary data is available to us, then we may not have this luxury. Dichotomization can also be practical in some instances. For example, clinical disorder diagnoses such as generalized anxiety disorder, are examples of dichotomization where individuals are separated into either having the disorder or not even though individual differences in anxiety exist as a continuum."
  },
  {
    "objectID": "artificial_dichotomization.html#artificial-dichotomization-induced-measurement-error",
    "href": "artificial_dichotomization.html#artificial-dichotomization-induced-measurement-error",
    "title": "7  Artificial Dichotomization",
    "section": "7.2 Artificial Dichotomization Induced Measurement Error",
    "text": "7.2 Artificial Dichotomization Induced Measurement Error\nVariables that are dichotomized contain measurement error. This can be demonstrated by the simple fact that dichotomized scores are not perfectly correlated with their underlying continuous scores. To demonstrate this, we can draw a sample of scores and then split the data into high and low scorers and then calculate the correlation coefficient between the two (see Figure 7.1).\n\n\n\n\n\nFigure 7.1: Visualizing the loss of precision when artificially dichotomizing. Left panel shows a normally distributed variable split (at the median/mean) into a high scoring group and a low scoring group.\n\n\n\n\nEven with a perfectly reliable measure, dichotomization will introduce measurement error. Dichotomization occurs when data is split into two groups (low and high groups will be denoted as \\(L\\) and \\(H\\), respectively) depending on whether they are above or below some cut-point \\(C_X\\). We can define artificially dichotomized scores as,\n\\[\nX_D=\n\\begin{cases}\n    H,& \\text{if } X\\geq C_X\\\\\n    L,& \\text{if } X&lt;C_X\n\\end{cases}\n\\]"
  },
  {
    "objectID": "artificial_dichotomization.html#correcting-correlations-for-artificial-dichotomization",
    "href": "artificial_dichotomization.html#correcting-correlations-for-artificial-dichotomization",
    "title": "7  Artificial Dichotomization",
    "section": "7.3 Correcting Correlations for Artificial Dichotomization",
    "text": "7.3 Correcting Correlations for Artificial Dichotomization\n\n7.3.1 Defining the Target Quantity\nWe want to obtain the population correlation between continuous scores of the independent (\\(X\\)) and dependent variable (\\(Y\\)), \\(\\rho_{XY}\\).\nThere are two cases of dichotomization that may occur in a given study: the univariate case where only one variable (either \\(X\\) or \\(Y\\)) is dichotomized and the bivariate case where both variables are dichotomized. In either case, dichotomization will have a biasing effect on the study correlation coefficient. The study correlation on dichotomized data (using the bivariate case, \\(r_{X_D Y_D}\\)) can be modeled as a function of the population correlation on continuous scores (\\(\\rho_{XY}\\); i.e., the target), an artifact attenuation factor \\(a\\), and sampling error, \\(e\\),\n\\[\nr_{X_DY_D} = a\\rho_{XY}+e.\n\\]\nAn unbiased estimate of \\(\\rho_{XY}\\) can be calculated by dividing the study correlation by an estimate of the artifact attenuation factor, \\(a\\),\n\\[\nr_{XY} = \\frac{r_{X_D Y_D}}{a}.\n\\]\n\n\n7.3.2 Artifact Correction for Correlations\nCorrelations can suffer from dichotomization in one variable (univariate case) or both variables (bivariate case). This section will discuss the procedure for obtaining an unbiased estimate of the correlation coefficient for both cases. For a comparative visualization of a correlation on with no dichotomization, univariate dichotomization, and bivariate dichotomization, see Figure 7.2.\n\n\n\n\n\nFigure 7.2: Scatter plots showing the correlation under dichotomization. The first panel (left to right) shows the correlation with no dichotomization (color and shapes of points denote where the split in the dichotomized cases will occur). The middle panel shows the univariate case where only the independent variable, \\(X\\), is dichotomized. The last panel shows the bivariate case where both independent (\\(X\\)) and dependent (\\(Y\\)) variables are dichotomized.\n\n\n\n\n\nThe Univariate Case\nIn the simplest case of dichotomization, only one variable is dichotomized and the other is left continuous. A Pearson correlation computed between a dichotomous variable and a continuous variable is known as point-biserial correlation. However, if the variable is naturally continuous, we can estimate the correlation of the underlying continuous scores by computing a biserial correlation. If all we have access to is the dichotomized data, then we need to assume the shape of the underlying distribution, in this case, the biserial correlation assumes normality.\nIn the population, the study correlation \\(\\rho_{X_D Y}\\) is biased by some artifact biasing factor, \\(a\\),\n\\[\n\\rho_{X_D Y} = a\\rho_{XY}.\n\\]\nTo estimate the attenuation factor \\(a\\), we must first figure out where the split of the data occured. To do this, we must first calculate the proportion of the sample in the assigned to the low or high scoring group:\n\\[\np_X  := p(X_i \\geq C_X) = \\frac{ n_{\\text{H}} }{n_{\\text{H}} + n_{\\text{L}}}\n\\]\nor\n\\[\np_X := p(X_i &lt; C_X) = \\frac{ n_{\\text{L}} }{n_{\\text{H}} + n_{\\text{L}}}.\n\\]\nWhere \\(n\\) indicates the sample size within the low, \\(L\\), and high, \\(H\\), scoring groups.\n\n\n\n\n\n\nNote\n\n\n\nIt will not matter whether you calculate the proportion of the sample in the high scoring group or the low scoring group for \\(p_X\\). Once you decide on one, do not change it.\n\n\nWe can use the quantile function (\\(\\phi^{-1}[\\cdot]\\), i.e., the inverse of the cumulative density of the standard normal distribution) to find where the split would have occured on a standard normal distribution, \\(s_X=\\phi^{-1}[p_X]\\). Using the location of the split on the standard normal, we can compute the artifact attenuation factor [an adaptation of equation 2, Hunter and Schmidt (1990)],\n\\[\n\\hat{a} =\\frac{\\varphi\\left(s_X\\right)}{\\sqrt{p_X(1-p_X)}}.\n\\tag{7.1}\\]\nWhere \\(\\varphi(\\cdot)\\) is the normal ordinate function (i.e., probability density function of a standard normal distribution). Figure 7.3 visually demonstrates how each of these relate to a standard normal distribution.\n\n\n\n\n\nFigure 7.3: This figure shows a normal distribution of scores split into a high scoring and low scoring group. The cut-point of the standard normal distribution is computed with the quantile function, \\(\\phi^{-1}[p_X]\\). The ordinate of the normal distribution at that cut-point is calculated with the normal ordinate function, \\(\\varphi\\left(\\phi^{-1}[p_X]\\right)\\).\n\n\n\n\nIn the case of a median split, where the cut-point would be placed at zero of a standard normal (splitting the distribution in equal halves), the attenuation factor would simplify to \\(\\hat{a} =\\frac{\\varphi(0)}{\\sqrt{.5(.5)}}\\) \\(=\\frac{2}{\\sqrt{2\\pi}}\\).\nWe can correct the study correlation using the estimated artifact factor, \\(\\hat{a}\\), therefore the full correction equation is,\n\\[\nr_{XY} = \\frac{r_{X_DY}}{\\hat{a}} = \\frac{r_{X_DY}}{\\left[\\frac{\\varphi\\left(s_X\\right)}{\\sqrt{p_X(1-p_X)}}\\right]}.\n\\tag{7.2}\\]\n\n\n\n\n\n\nConverting to \\(\\boldsymbol{\\mathsf{r_{X_{D}Y}}}\\) from means and standard deviations\n\n\n\nIt is common that studies do not report the correlation between dichotomized and continuous scores, \\(r_{X_DY}\\) (i.e., the point-biserial correlation), instead they may report the means and standard deviations instead. To obtain the point-biserial correlation, \\(r_{X_D Y}\\), we need the the mean of the high scoring group (\\(\\overline{Y}_H\\)), mean of the low scoring groups (\\(\\overline{Y}_L\\)), the standard deviation across all individuals (\\(S_Y\\); not to be confused with the pooled/average standard deviation within each group), and the sample sizes within each group (\\(n_H\\) and \\(n_L\\)) to calculate \\(r_{X_DY}\\),\n\\[\nr_{X_D Y} =\\frac{\\overline{Y}_H-\\overline{Y}_L}{S_Y} \\sqrt{p_X(1-p_X)}.\n\\]\nIf the standard deviations are reported only for within the high and low groups, you can estimate the total standard deviation with the following formula,\n\\[\nS_X = \\sqrt{\\frac{n_HS_{X_H} + n_L S_{X_L}}{n_H + n_L} +\\frac{n_Hn_L}{n_H + n_L}(\\overline{X}_H - \\overline{X}_L)^2}.\n\\]\n\n\nHunter and Schmidt (1990) suggested that one should correct the standard error by dividing the uncorrected standard error by the artifact attenuation factor (see equation 6, Hunter and Schmidt 1990). However simulations have found that this computation does not work as well as Soper’s exact method (Soper 1914; Jacobs and Viechtbauer 2017). Therefore the standard error of the corrected (biserial) correlation can be estimated with the following formula (equation 12, Jacobs and Viechtbauer 2017),\n\\[\n\\small{se(r_{XY}) = \\sqrt{\\frac{1}{n-1} \\left(r_{XY}^4+r_{XY}^2\\left(\\frac{p_X(1-p_X)s_X^2}{\\varphi\\left(s_X\\right)^2} + \\frac{2p_X - 1}{\\varphi\\left(s_X\\right)} -\\frac{5}{2}\\right)+\\frac{p_X(1-p_X)}{\\varphi\\left(s_X\\right)^2}\\right)}}.\n\\]\nSoper (1914) also developed an approximation of the above formula,\n\\[\nse(r_{XY}) \\approx \\sqrt{\\frac{1}{n-1}} \\left(\\frac{\\sqrt{p_X(1-p_X)}}{\\varphi\\left(s_X\\right)}-r_{XY}^2\\right).\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say we want to assess the relationship between a sales person’s score on a job knowledge test and their job performance estimated as the number of sales made per week. However the researchers of the study chose to dichotomize sales people into high sales performers and low sales performers by splitting the sample into two equally sized groups. They then reported the means and standard deviations of job knowledge test scores of both groups:\n\nLow sales performers: Mean = 22 (SD = 4, n = 50)\nHigh sales performers: Mean = 24 (SD = 4, n = 50)\n\nTo calculate the corrected correlation we can use escalc() function from the metafor package (Viechtbauer 2010). Using the argument measure='RBIS' will return the biserial correlation coefficient which is equivalent to the corrected correlation.\n\nlibrary(metafor)\n\nescalc(measure = 'RBIS',\n       m1i = 24, # High performer mean \n       m2i = 22, # Low performer mean\n       sd1i = 4, # High performer SD\n       sd2i = 4, # Low performer SD\n       n1i = 50, # High performer sample size\n       n2i = 50, # Low performer sample size\n       var.names = c('rXY','se.2'),\n       digits = 3)\n\n\n    rXY  se.2 \n1 0.307 0.014 \n\n\nTherefore the estimated correlation on continuous scores is \\(r_{XY}=.31\\).\nIf the study reported a Pearson correlation (point-biserial) between the dichotomized variable and the continuous variable of \\(r_{X_D Y}=.245\\), then\n\nlibrary(psychmeta)\n\ncorrect_r_dich(r = .245, # study point-biserial correlation\n               px = .50, # proportion of sample in low or high group\n               n = 100) # total sample size\n\n  r_corrected var_e_corrected    n_adj\n1    0.307062      0.01401901 59.51455\n\n\nThe corrected correlation for continuous scores is equal to the calculation using the means and standard deviations. Note that the estimated sampling variance of the corrected correlation, se.2 (in the psychmeta example) and var_e_corrected (in the metafor example), are both equal to the squared corrected standard error (\\(se(r_{XY})^2\\)).\n\n\n\n\nThe Bivariate Case\nIn some cases, both independent and dependent variables are dichotomized. A Pearson correlation calculated on these two dichotomized (binary) variables would be equal to the phi coefficient (or also known as Matthew’s correlation coefficient) and we can denote it with our notation for dichotomized variables, \\(\\rho_{X_D X_D}\\). Dichotomized data can be structured in a contingency table (see Table 7.1).\n\n\n\n\n\n\nTable 7.1: Contingency table.\n\n\n\n\\(X_D=\\text{Low}\\)\n\\(X_D=\\text{High}\\)\n\n\n\\(Y_D=\\text{Low}\\)\n\\(n_{LL}\\)\n\\(n_{HL}\\)\n\n\n\\(Y_D=\\text{High}\\)\n\\(n_{LH}\\)\n\\(n_{HH}\\)\n\n\n\n\n\n\n\n\nFigure 7.4 illustrates how this contingency table relates to an underlying continuous bivariate normal distribution.\n\n\n\nFigure 7.4: The ellipse indicates the bivariate normal distribution of \\(X\\) and \\(Y\\) with a strong positive correlation. If \\(X\\) and \\(Y\\) are positively correlated then we should more individuals populating the high-high and low-low cells rather than the high-low and low-high cells which can be seen by the area of the ellipse located in each quadrant.\n\n\nThe corrected correlation coefficient for two dichotomized variables is commonly referred to as the tetrachoric correlation coefficient. The tetrachoric correlation estimates the correlation on continuous scores assuming a bivariate normal distribution.\nOne of the difficulties of computing a dichotomization corrected (tetrachoric) correlation (\\(r_{XY}\\)) is that the relationship between binary variables are reported in different ways across studies, we will describe how to obtain a dichtomization corrected correlation in four different cases:\n\nThe full contingency table is provided, including the sample sizes for each cell.\nThe odds ratio is reported as well as the marginal proportions (proportions in low and high groups for each variable).\nThe Phi coefficient is reported.\n\n\nCase 1: Full contingency table is reported\nIf the full contingency table is reported, then the tetrachoric correlation can be calculated directly. Due to the complexity of the calculation, we will use R.\nThe escalc() function in the metafor package (Viechtbauer 2010) can take on values from a contingency table and compute a tetrachoric correlation using the measure='RTET' argument. The function uses the method described by Kirk (1973).\n\n# Example Contingency Table\n#    XL   XH\n# YL 43   23\n# YH 27   38  \n\nlibrary(metafor)\n\nescalc(measure = 'RTET',\n       ai = 43,\n       bi = 23,\n       ci = 27,\n       di = 38,\n       var.names = c('rXY','se.2'))\n\n\n     rXY   se.2 \n1 0.3637 0.0155 \n\n\nThe results show a dichotomization corrected correlation of \\(r_{XY} = .36\\) and an estimated sampling variance of \\(se_c^2=.016\\).\n\n\nCase 2: odds ratio is reported\nIf the odds ratio is all that is available, then we can use the tetrachoric correlation approximation described by Bonett and Price (2005). Using the estimated odds ratio (\\(OR=(n_{HH}n_{LL})/(n_{HL}n_{LH})\\)) and the marginal proportions (\\(p_X\\) and \\(p_Y\\); for this case, both proportions should be with respect to the high scoring group) we can approximate the dichotomization corrected (tetrachoric) correlation (see equation 4, Bonett and Price 2005),\n\\[\nr_{XY} \\approx \\cos\\left( \\frac{\\pi}{1+OR^\\Omega}\\right)\n\\] Where \\(\\Omega\\) is\n\\[\n\\Omega = 1 - \\frac{\\left|p_X -p_Y\\right|}{5} - \\left[\\frac{1}{2}-\\min(p_X,p_Y)\\right]^2.\n\\]\nNote that \\(\\min(p_X,p_Y)\\) is the smallest marginal proportion. The standard error of the estimated correlation can be computed with the following formula [see equation 9, Bonett and Price (2005); note this equation is slightly changed in order to account for the fact that we only have access to the marginal sample sizes and proportions]\n\\[\n\\small{se(r_{XY}) =  \\frac{\\pi\\times \\Omega\\times OR^\\Omega\\times\\sin\\left(\\frac{\\pi}{1+OR^\\Omega}\\right)}{\\left(1+OR^\\Omega\\right)^2}\\times\\sqrt{\\frac{4}{n}}}\n\\]\nWhere \\(n\\) is the total sample size. Using base R, we can convert the odds ratio to a tetrachoric correlation.\n\npX &lt;- .4 # proportion of individuals in high group in XD\npY &lt;- .5 # proportion of individuals in high group in YD\nn &lt;- 150 # total sample size\nOR &lt;- 1.43 # odds ratio\nomega &lt;- 1 - abs(pX-pY) / 5 - (1/2 - min(c(pX,pY)))^2\n\n# calculate corrected correlation\nrXY &lt;- cos(pi/(1+OR^omega))\n\n# calculate standard error\nse &lt;- pi*omega*OR^omega*sin(pi/(1+OR^omega)) / (1+OR^omega)^2 * sqrt(4/n)\n\ncbind(rXY, se)\n\n           rXY      se\n[1,] 0.2665276 0.11637\n\n\n\n\nCase 3: Correlation between dichotomized variables\nA Pearson correlation coefficient calculated between two binary variables is most commonly referred to as a Phi coefficient or Matthew’s correlation coefficient. However, the Phi coefficient underestimates the correlation on the underlying continuous scores (assuming bivariate normality). Therefore we can approximate the correlation on continuous scores by using a similar correction to the univariate case. We can define an artifact attenuation factor that is similar to Equation 7.1, but with the added attenuation of \\(Y_D\\),\n\\[\n\\hat{a} =\\left[\\frac{\\varphi\\left(\\phi^{-1}[p_X]\\right)}{\\sqrt{p_X(1-p_X)}}\\right]\\times\\left[\\frac{\\varphi\\left(\\phi^{-1}[p_Y]\\right)}{\\sqrt{p_Y(1-p_Y)}}\\right].\n\\]\nTherefore we can correct \\(r_{X_DY_D}\\) for dichotomization in both variables by dividing by the attenuation factor, \\[\nr_{XY} = \\frac{r_{X_DY_D}}{\\hat{a}}=\\frac{r_{X_DY_D}}{\\left[\\frac{\\varphi\\left(\\phi^{-1}[p_X]\\right)}{\\sqrt{p_X(1-p_X)}}\\right]\\times\\left[\\frac{\\varphi\\left(\\phi^{-1}[p_Y]\\right)}{\\sqrt{p_Y(1-p_Y)}}\\right]}\n\\] This correction is only an approximation, however it performs fairly well when the correlation is below .8 (Hunter and Schmidt 1990). The standard error of the corrected correlation can be calculated similarly (adaptation of equation 9, Hunter and Schmidt 1990),\n\\[\nse(r_{XY})=\\frac{se(r_{X_DY_D})}{\\left[\\frac{\\varphi\\left(\\phi^{-1}[p_X]\\right)}{\\sqrt{p_X(1-p_X)}}\\right]\\times\\left[\\frac{\\varphi\\left(\\phi^{-1}[p_Y]\\right)}{\\sqrt{p_Y(1-p_Y)}}\\right]}\n\\]\nUsing the correct_r_dich() function in the psychmeta package (Dahlke and Wiernik 2019), we can correct the observed study correlation for dichotomization.\n\nlibrary(psychmeta)\n\ncorrect_r_dich(r = .20,  # observed study correlation\n               px = .50, # proportion of sample in high group of XD\n               py = .50, # proportion of sample in high group of YD\n               n = 100)  # sample size\n\n  r_corrected var_e_corrected    n_adj\n1   0.3141593      0.02296926 36.36678\n\n\nThe output shows the corrected correlation to be \\(r_{XY}=.31\\) and it’s estimated sampling variance \\(se_c^2=.023\\)."
  },
  {
    "objectID": "artificial_dichotomization.html#correcting-standardized-mean-differences-for-artificial-dichotomization",
    "href": "artificial_dichotomization.html#correcting-standardized-mean-differences-for-artificial-dichotomization",
    "title": "7  Artificial Dichotomization",
    "section": "7.4 Correcting Standardized Mean Differences for Artificial Dichotomization",
    "text": "7.4 Correcting Standardized Mean Differences for Artificial Dichotomization\n\n7.4.1 Defining our Target Quantity\nThe target quantity is the standardized mean difference between groups of a naturally continuous variable. Our target can thus be defined as the population standardized mean difference between groups \\(A\\) and \\(B\\) on continuous scores of the dependent variable, \\(\\delta_{gY}\\). For a given study the dichotomized standardized mean difference (\\(d_{gY_D}\\)) can be defined as\n\\[\nd_{gY_D} = a\\delta_{gY} + e\n\\]\nTherefore to obtain an unbiased estimate of the target quantity, we can correct the observed study standardized mean difference by dividing by the attenuation factor,\n\\[\nd_{gY} = \\frac{d_{gY_D}}{\\hat{a}}\n\\]\n\n\n7.4.2 Artifact Correction for Standardized Mean Differences\nThe simplest way to correct for dichotomization in a standardized mean difference is to first convert the observed \\(d\\) value of the dichotomized dependent variable to a correlation coefficient. When converting to a correlation coefficient, it’s important to note the binary nature of both variables, leading us to estimate the phi coefficient rather than the point-biserial correlation that we would be estimating if the dependent variable was continuous. To calculate the phi coefficient from a \\(d\\) value we can use the proportion of group membership in group \\(A\\) or group \\(B\\) (\\(p_g\\); it does not matter which one is chosen, as long as it is consistent for every instance of \\(p_g\\)),\n\\[\nr_{gY_D} = \\frac{d_{gY_D}}{\\sqrt{d_{gY_D}^2+\\frac{1}{p(1-p)}}}\n\\]\nWe can then correct the correlation similar to how we did in Section 7.3.2,\n\\[\nr_{gY} = \\frac{r_{gY_D}}{\\left[\\frac{\\varphi\\left(\\phi^{-1}[p_Y]\\right)}{\\sqrt{p_Y(1-p_Y)}}\\right]}.\n\\]\nThen we can convert the corrected correlation back into a standardized mean difference,\n\\[\nd_{gY} = \\frac{r_{gY}}{\\sqrt{p_g\\left(1-p_g\\right)\\left(1-r_{gY}^2\\right)}}.\n\\]\nWhere \\(d_{gY}\\) is our corrected correlation (i.e., the estimated correlation on continuous variables). The observed sampling variance must also be corrected using the same three step procedure. For simplicity, we will consolidate this into one formula,\n\\[\nse(d_{gY}) = \\frac {se(d_{gY_D}) \\times r_{XY}} {r_{x_DY_D}\\sqrt{\\left(1+d_{gY_D}^2p_g[1-p_g]\\right)^3(1-r_{gY}^2)^3}}.\n\\]\n\nObtaining Standardized Mean Difference from Odds Ratio\nIn most cases, difference in dichotomized outcomes between two groups is unlikely to be reported as a standardized mean difference, instead it will be more commonly reported as an odds ratio (\\(OR\\)). The odds ratio is asymmetric about 1 (i.e., the null), but we can make it symmetric by log transforming it (\\(\\log(OR)\\)). The standard error of the log odds ratio can be defined as,\n\\[\nse(\\log(OR)) = \\sqrt{\\frac{1}{n_{AH}} + \\frac{1}{n_{AL}} + \\frac{1}{n_{BH}} + \\frac{1}{n_{BL}}}\n\\]\nThe equation above requires the full contingency table to compute. From there we can use the cox-logit method to convert the odds ratio to a standardized mean difference (Cox 1989; Haddock, Rindskopf, and Shadish 1998). Them method is quite simple as it just divides the log odds ratio by 1.65,\n\\[\nd_{gY} = \\frac{\\log(OR)}{1.65}\n\\tag{7.3}\\]\nand the corresponding sampling variance of the \\(d\\) value is,\n\\[\nse(d_{gY}) = \\frac{se(\\log(OR))}{1.65}.\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s consider a hypothetical scenario where we want to examine the relationship between caffeine consumption and the occurrence of heart palpitations in a population. Our target quantity in this case is the standardized mean difference of coffee consuption between people with and without heart palpitations. The variable of interest, caffeine consumption, is continuous (measured in milligrams per day). However, the researcher decides to dichotomize this variable into two groups: “High Caffeine Consumers” and “Low Caffeine Consumers.”\nSuppose we have a sample of 500 individuals, and we dichotomize their caffeine consumption into “High Caffeine Consumers” (more than 250mg per day) and “Low Caffeine Consumers” (less than or equal to 250mg per day). We also observe the occurrence of heart palpitations in these individuals.\n\n\n\n\nHeart Palpitations: Yes\nHeart Palpitations: No\n\n\n\n\nHigh Consumers\n60\n140\n\n\nLow Consumers\n20\n280\n\n\n\nWe can calculate the dichotomization corrected standardized mean difference by calculating the log odds ratio with escalc() function and then applying Equation 7.3 to estimate \\(d_{gY}\\).\n\n# calculate log-odds ratio\nOR &lt;- escalc(measure = 'OR',\n             ai = 60,\n             bi = 140,\n             ci = 20,\n             di = 280,\n             var.names = c('logOR','se.2'))  \n\n# convert to standardized mean difference\ndgY &lt;- OR$logOR / 1.65\nse &lt;- sqrt(OR$se.2) / 1.65\n\n# print results\ncbind(dgY,se)\n\n          dgY        se\n[1,] 1.085915 0.1685905\n\n\nWe can see that the standardized mean difference is estimated to be \\(d_{gY}=1.09\\) and the corrected standard error is \\(se_c=.169\\)\n\n\n\n\n\n\n\nBonett, Douglas G., and Robert M. Price. 2005. “Inferential Methods for the Tetrachoric Correlation Coefficient.” Journal of Educational and Behavioral Statistics 30 (2): 213–25. https://www.jstor.org/stable/3701350.\n\n\nCox, D. R. 1989. Analysis of Binary Data. 2nd ed. New York: Routledge. https://doi.org/10.1201/9781315137391.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nHaddock, C. Keith, David Rindskopf, and William R. Shadish. 1998. “Using Odds Ratios as Effect Sizes for Meta-Analysis of Dichotomous Data: A Primer on Methods and Issues.” Psychological Methods 3 (3): 339–53. https://doi.org/10.1037/1082-989X.3.3.339.\n\n\nHunter, John, and Frank Schmidt. 1990. “Dichotomization of Continuous Variables: The Implications for Meta-Analysis.” Journal of Applied Psychology 75 (June): 334–49. https://doi.org/10.1037/0021-9010.75.3.334.\n\n\nJacobs, Perke, and Wolfgang Viechtbauer. 2017. “Estimation of the Biserial Correlation and Its Sampling Variance for Use in Meta-Analysis.” Research Synthesis Methods 8 (2): 161–80. https://doi.org/10.1002/jrsm.1218.\n\n\nKirk, David B. 1973. “On the Numerical Approximation of the Bivariate Normal (Tetrachoric) Correlation Coefficient.” Psychometrika 38 (2): 259–68. https://doi.org/10.1007/BF02291118.\n\n\nMaxwell, Scott, and Harold Delaney. 1993. “Bivariate Median Splits and Spurious Statistical Significance.” Psychological Bulletin 113 (January): 181–90. https://doi.org/10.1037/0033-2909.113.1.181.\n\n\nSoper, H. E. 1914. “On the Probable Error of the Bi-Serial Expression for the Correlation Coefficient.” Biometrika 10 (2/3): 384–90. https://doi.org/10.2307/2331789.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting meta-analyses in R with the metafor package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03."
  },
  {
    "objectID": "scale_coarseness.html#introduction",
    "href": "scale_coarseness.html#introduction",
    "title": "8  Scale Coarseness",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nScale coarseness describes a situation where a variable that is naturally continuous (e.g., happiness) is binned into discrete values (e.g., happiness measured on a scale of 1-10). This situation is quite common in the social and psychological sciences where Likert items or dichotomous yes/no responses are aggregated to form a coarse total score for a naturally continuous construct. When coarseness is present, measurement error is introduced into the observed scores and those scores lose information. Unlike dichotomization, coarseness is an artifact that occurs due to the design of the study rather than during the analysis phase (Aguinis, Pierce, and Culpepper 2009). Particularly, dichotomization occurs after scores are obtained (e.g., splitting a group into high scorers and low scorers), whereas coarseness occurs as an artifact of the measurement procedure itself. The primary issue with coarseness is that it limits the set of possible values a score can be which introduces error when the variable is naturally continuous. This can be visualized by correlating coarse scores with their underlying continuous scores (see Figure 8.1). You will notice that the correlation between coarse and continuous scores is not perfect, indicating that the coarse scores do not perfectly capture the underlying continuous scores.\n\n\n\n\n\nFigure 8.1: Scatterplot showing the correlation between coarse scores (on a 5-point scale) and the underlying continuous scores."
  },
  {
    "objectID": "scale_coarseness.html#correcting-for-coarseness-in-correlations",
    "href": "scale_coarseness.html#correcting-for-coarseness-in-correlations",
    "title": "8  Scale Coarseness",
    "section": "8.2 Correcting for Coarseness in Correlations",
    "text": "8.2 Correcting for Coarseness in Correlations\n\n8.2.1 Defining our Target Quantity\nOur quantity of interest is the population correlation, \\(\\rho\\), between continuous independent variable, \\(X\\), and continuous dependent variable, \\(Y\\). However, in a given study the measurement procedure may produce coarse scores for \\(X\\) and \\(Y\\). We will denote coarse scores with the subscript \\(C\\) We can model the relationship between the observed sample correlation on coarse scores and the true population correlation,\n\\[\nr_{X_CY_C} = a\\rho_{XY}+e.\n\\]\nWhere \\(a\\) is our artifact attenuation factor and \\(e\\) is our sampling error term. We can obtain an unbiased estimate of the continuous population correlation, \\(\\rho_{XY}\\), by dividing the observed correlation by an estimate of the attenuation factor,\n\\[\nr_{XY} = \\frac{r_{X_CY_C}}{\\hat{a}}.\n\\]\n\n\n8.2.2 Artifact Correction for Coarseness\nCorrelating a coarse score with another variable will cause attenuation of the correlation (MacCallum et al. 2002). Furthermore if we correlate a coarse score with another coarse score than we will observe even more attenuation (see Figure 8.2). There are two cases that we can run into: 1) the univariate case where only one variable is coarse and 2) the bivariate case where both variables are coarse.\n\n\n\n\n\nFigure 8.2: First plot (left to right) shows both variables as continuous and normal. The second plot shows coarseness (5-point scale) only on X, leaving Y continuous (\\(r_{X_CY}=.47\\)). Last plot shows coarseness on both variables (\\(r_{X_CY_C}=.47\\)).\n\n\n\n\nTo correct the correlation between coarse scores for \\(X\\) and \\(Y\\), we need to know the correlation between coarse scores and their underlying continuous scores (\\(r_{XX_C}\\) or \\(r_{YY_C}\\)). The calculation of the correlation will require us two important assumptions:\n\nThe shape of the underlying distribution (i.e., normal or uniform).\nThe intervals between scale-points are equal.\n\nBased on these assumptions, Peters and Voorhis (1940) constructed a table of correlations between coarse and continuous scores that is also reported more recently by Aguinis, Pierce, and Culpepper (2009). Table 8.1 is adapted from Peters and Voorhis (1940) and displays the correlation values for uniform and normal distributions for a given number of scale points. For the normal distribution correction, its been shown that even in cases of extreme skew, these correction factors perform well (Wylie 1976).\n\n\nTable 8.1: Correlations between continuous and coarse scores (\\(r_{XX_C}\\)) from Peters and Voorhis (1940)\n\n\n\n\n\n\n\nScale Points\nContinuous-Coarse score Correlation (normal)\nContinuous-Coarse score Correlation (uniform)\n\n\n\n\n2\n.816\n.866\n\n\n3\n.859\n.943\n\n\n4\n.916\n.968\n\n\n5\n.943\n.980\n\n\n6\n.960\n.986\n\n\n7\n.970\n.990\n\n\n8\n.977\n.992\n\n\n9\n.982\n.994\n\n\n10\n.985\n.995\n\n\n11\n.988\n.996\n\n\n12\n.990\n.997\n\n\n13\n.991\n.997\n\n\n14\n.992\n.997\n\n\n15\n.994\n.998\n\n\n\n\nThe correlations between coarse and continuous scores (\\(r_{XX_C}\\) and \\(r_{YY_C}\\)) can be used to correct the correlation coefficient\n\\[\nr_{XY} = \\frac{r_{X_CY_C}}{r_{XX_C}r_{YY_C}}.\n\\]\nWhere \\(r_{XX'}\\) and \\(r_{YY'}\\) are the appropriate correction factors from the table for \\(x\\) and \\(y\\). We must also adjust the sampling variance as well,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o}}{a^2_x a^2_y}.\n\\]\n\n\n8.2.3 Correcting Correlations in R\nTo correct scale coarseness in R, we can use the table in Section 8.2.2. Lets start by simulating a coarse data (5 scale-points for x and 7 scale-points for y) set of 500 individuals with a true population correlation of \\(\\rho = .50\\).\n\n\n\n\n\n\nApplied Example in R\n\n\n\nImagine that a researcher wants to relate depression and age. They collect a sample of 1,000 people from the general population and administer a very quick survey. Depression is measured based on a single item from the patient health questionnaire (PHQ, Kroenke, Spitzer, and Williams 2003) and age is measured in 5 age ranges:\n\n\nOver the last 2 weeks, how often have you been bothered by the following problems?\n\n\n\nWhat is your age?\n\n\n\n\n\nNot at all\nSeveral days\nMore than half the days\nNearly every day\n\n\n\n\n\n1-20 years\n21-40 years\n41-60 years\n61-80 years\n81+ years\n\n\n\nLet’s say we obtain a correlation of \\(r_{X_C Y_C}=-.20\\). Since the correlation is computed on coarse scores, it is likely attenuated relative to the correlation on each variables continuous underlying scores. Therefore we can use the correct_r_coarseness() function in the psychmeta package (Dahlke and Wiernik 2019) to correct the correlation.\n\nlibrary(psychmeta)\n\ncorrect_r_coarseness(r = -.20, # observed correlation\n                     kx = 5, # 5 age range bins\n                     ky = 4,  # 4 PHQ item options\n                     n = 1000, # sample size\n                     dist_x = \"unif\", # assumed X distribution\n                     dist_y = \"norm\") # assumed Y distribution\n\n  r_corrected var_e_corrected    n_adj\n1  -0.2230339     0.001147251 788.0867\n\n\nWe see a slight increase in the magnitude of the correlation with the estimated correlation on continuous scores being \\(r_{XY}=-.22\\) and a standard error of \\(se(r_{XY})=\\sqrt{.0011}=0.034\\)."
  },
  {
    "objectID": "scale_coarseness.html#correcting-for-coarseness-in-d-values",
    "href": "scale_coarseness.html#correcting-for-coarseness-in-d-values",
    "title": "8  Scale Coarseness",
    "section": "8.3 Correcting for Coarseness in d values",
    "text": "8.3 Correcting for Coarseness in d values\n\n8.3.1 Defining our Target Quantity\nOur quantity of interest is the population standardized mean difference, \\(\\delta_{gY}\\), between groups \\(A\\) and \\(B\\) on continuous variable, \\(Y\\). We can define the standardized mean difference on coarse scores (\\(d_{gY_C}\\)) as,\n\\[\nd_{gY_C} = a\\delta_{gY}+\\varepsilon.\n\\]\nWhere \\(a\\) is our coarseness biasing factor and \\(\\varepsilon\\) is our sampling error term. Ultimately, we can obtain an unbiased estimate of the population standardized mean difference on continuous scores by correcting the observed standardized mean difference as follows,\n\\[\nd_{gY} = \\frac{d_{gY_C}}{\\hat{a}}.\n\\]\n\n\n8.3.2 Artifact Correction for Coarseness\nTo correct a standardized mean difference for coarseness in dependent variable, we can use the correlation between coarse scores and continuous scores from Table 8.1,\n\\[\nd_{gY} = \\frac{d_{gY_C}}{r_{YY_C}}.\n\\]\nWhere \\(r_{YY_C}\\) is the appropriate correction factor from the table. We must also adjust the standard error,\n\\[\nse(d_{gY}) = \\frac{se(d_{gY_C})}{r_{YY_C}}.\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say that a researcher wants to investigate gender differences in depression. The researcher administers a survey to a sample of men and women from the general population. Depression is measured based on a single item from the patient health questionnaire (PHQ, Kroenke, Spitzer, and Williams 2003):\nOver the last 2 weeks, how often have you been bothered by the following problems?\n\nNot at all\nSeveral days\nMore than half the days\nNearly every day\n\nLet’s say we obtain a standardized mean difference of \\(d_{g Y_C}=.25\\), slightly favoring women. Since there is currently no correct_d_coarseness() function in psychmeta, we can simply correct the correlation with base R. again however we will need t in the psychmeta package (Dahlke and Wiernik 2019) to correct the correlation.\n\nlibrary(psychmeta)\n\ndgYc &lt;- .25 # standardized mean difference on coarse scores\nse.dgYc &lt;- .10 # standardized mean difference on coarse scores\nrYYc &lt;- .916\n\ndgY &lt;- dgYc / rYYc # correct d\nse.dgY &lt;- se.dgYc / rYYc # correct se\n\n# print results\ncbind(dgY, se.dgY)\n\n           dgY    se.dgY\n[1,] 0.2729258 0.1091703\n\n\nWe see a slight increase in the magnitude of the correlation with the estimated correlation on continuous scores being \\(d_{gY}=.27\\) and a standard error of \\(se(d_{gY})=.109\\).\n\n\n\n\n\n\n\nAguinis, Herman, Charles A Pierce, and Steven A Culpepper. 2009. “Scale Coarseness as a Methodological Artifact,” September.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nKroenke, Kurt, Robert L. Spitzer, and Janet B. W. Williams. 2003. “The Patient Health Questionnaire-2: Validity of a Two-Item Depression Screener.” Medical Care 41 (11): 1284–92. https://www.jstor.org/stable/3768417.\n\n\nMacCallum, Robert C., Shaobo Zhang, Kristopher J. Preacher, and Derek D. Rucker. 2002. “On the Practice of Dichotomization of Quantitative Variables.” Psychological Methods 7: 19–40. https://doi.org/10.1037/1082-989X.7.1.19.\n\n\nPeters, Charles C., and Walter R. Van Voorhis. 1940. “Further Methods of Correlation.” In, 362–403. New York, NY, US: McGraw-Hill Book Company. https://doi.org/10.1037/13596-013.\n\n\nWylie, Peter B. 1976. “Effects of Coarse Grouping and Skewed Marginal Distributions on the Pearson Product Moment Correlation Coefficient.” Educational and Psychological Measurement 36 (1): 1–7. https://doi.org/10.1177/001316447603600101."
  },
  {
    "objectID": "direct_range_restriction.html#introduction",
    "href": "direct_range_restriction.html#introduction",
    "title": "9  Direct Selection",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nDirect selection occurs when subjects are explicitly selected based on some eligibility criterion on the variables of interest (rather than a third variable). Range restriction is a form of selection bias that describes a situation where there is less variation in our sample then there is in the population. Whereas range enhancement indicates that there is more variation in a sample then there is in the population. Direct range restriction/enhancement biases the variances and effect size estimates."
  },
  {
    "objectID": "direct_range_restriction.html#an-example-of-direct-range-restriction",
    "href": "direct_range_restriction.html#an-example-of-direct-range-restriction",
    "title": "9  Direct Selection",
    "section": "9.2 An Example of Direct Range Restriction",
    "text": "9.2 An Example of Direct Range Restriction\nImagine a tech company that wants to assess the correlation between years of experience and programming proficiency for their software engineers. They have two primary divisions: Division A and Division B. Division A primarily hires entry-level software engineers, with less than 3 years of experience. Division B, on the other hand, hires experienced software engineers with more than 3 years of experience. The company decides to conduct a study to assess the correlation between years of experience and programming proficiency. However, they only collect data from Division A due to logistical reasons, assuming that the relationship found there would be represent the entire company. In this scenario, direct range restriction occurs because the sample used for the study (Division A) represents a narrow range of years of experience (0-3 years) compared to the broader range present in the entire company (0+ years). Consequently, the standard deviation will be smaller in the sample then it would if we had sampled from the entire company. As we will see in later sections of this chapter, the observed correlation between years of experience and programming proficiency would be attenuated, underestimating the true correlation."
  },
  {
    "objectID": "direct_range_restriction.html#a-direct-selection-function",
    "href": "direct_range_restriction.html#a-direct-selection-function",
    "title": "9  Direct Selection",
    "section": "9.3 A Direct Selection Function",
    "text": "9.3 A Direct Selection Function\nA selection function described here is a type of indicator function that represents the mechanism of which observations are selected into a given sample. In the case of direct selection, the selection will be a function of the of the variable of interest, \\(X\\). We will denote a selection function as \\(\\mathcal{S}(X)\\). The output of the selection function will be a binary: either the individual is selected \\(\\mathcal{S}(X)=1\\) or rejected \\(\\mathcal{S}(X)=0\\) (see Figure 9.1).\n\n\n\nFigure 9.1: Diagram illustrating the direct selection function. The variable, \\(X\\), in the population is selected based on a function of itself. Therefore the variable under selection is conditional on the selection procedure \\(X|\\mathcal{S}(X)\\).\n\n\nThe functional form of the selection function is going to be described by some inequality of \\(X\\). The inequality will be dependent on the research context (e.g., the sampling procedure).\n\n\n\n\n\n\nA Simple Example\n\n\n\nLet’s define the selection function as\n\\[\\mathcal{S}(X) = \\begin{cases}1 & \\text{if }X&gt;3 \\text{ and } X&lt;8 \\\\ 0 & \\text{if }X\\leq3 \\text{ and } X\\geq8  \\end{cases}\\] then let’s say \\(X\\) contains the following observations, \\[\nX = [1,2,3,4,5,6,7,8,9,10]\n\\]\nthe selection function would then return,\n\\[\n\\mathcal{S}(X) =  [0,0,0,1,1,1,1,0,0,0]\n\\] therefore \\(X\\) given selection on \\(X\\) would be,\n\\[\nX|\\mathcal{S}(X) =  [4,5,6,7]\n\\]\n\n\nDirect selection on \\(X\\) will have a fairly straight forward effect on the distribution of \\(X\\). The probability density function of \\(X\\) (\\(f(X)\\)) under direct selection can be formally defined as,\n\\[\nf\\left(X|\\mathcal{S}(X)\\right) = f(X) \\mathcal{S}(X)\n\\]\nWe can visualize the distribution under direct selection in Figure 9.2. Note that this is a simple case where selection is done on a single variable (i.e., univariate direct selection). In practice, selection is often a function of multiple variables. For instance, bivariate direct selection occurs when the selection is a function of both variables of interest, \\(\\mathcal{S}(X,Y)\\).\n\n\n\n\n\nFigure 9.2: Visualizing direct selection with an example probability density and selection function. Top panel shows a normally distribution of \\(X\\) within the population. The middle panel shows the selection function. The selection function is based on the inequality, \\(-1.0&lt;X&lt;1.5\\). The bottom panel shows the distribution of \\(X\\) after selecting on \\(X\\) (i.e., \\(X|\\mathcal{S}(X)\\))."
  },
  {
    "objectID": "direct_range_restriction.html#quantifying-direct-selection-induced-restrictionenhancement-with-the-u-ratio",
    "href": "direct_range_restriction.html#quantifying-direct-selection-induced-restrictionenhancement-with-the-u-ratio",
    "title": "9  Direct Selection",
    "section": "9.4 Quantifying Direct Selection-Induced Restriction/Enhancement with the u-ratio",
    "text": "9.4 Quantifying Direct Selection-Induced Restriction/Enhancement with the u-ratio\nThe distribution of scores in the population of individuals will exhibit a greater (or lesser) degree of variability compared to the population that meets the selection criterion. Therefore the standard deviation of scores in the target population (\\(\\sigma_{X}\\)) will differ from that of the population under direct selection on \\(X\\) (\\(\\sigma_{X|\\mathcal{S}(X)}\\)). To index the difference between the two standard deviations, we can calculate the \\(u\\)-ratio Wiernik and Dahlke (2020). The \\(u\\)-ratio is defined as the ratio between the standard deviations of the population (\\(\\sigma_{X|\\mathcal{S}(X)}\\)) under selection and the target population (\\(\\sigma_{X}\\)) such that,\n\\[\n\\upsilon_X = \\frac{\\sigma_{X|\\mathcal{S}(X)}}{\\sigma_{X}}\n\\] Where \\(\\upsilon_{X}\\) denotes the population \\(u\\)-ratio. The \\(u\\)-ratio in cases of range restriction will exist in the interval (0–1). Conversely, when the \\(u\\)-ratio is greater than 1 it is indicative of range enhancement. For a sample, the \\(u\\)-ratio is calculated from sample standard deviations,\n\\[\nu_X = \\frac{S_{X|\\mathcal{S}(X)}}{S_X}\n\\]\nWhere \\(u_{X}\\) denotes the sample \\(u\\)-ratio. The target population standard deviation is often quite difficult to estimate since generally we do not have access to an estimate of the population of interest. However, the unrestricted standard deviation can be estimated from some reference or norm study that is representative of the population of interest. This often comes in the form of standardization samples or norm samples (obtained from test manuals) if the population of interest is the general population. For example, the distribution full-scale IQ scores derived from the Wechsler Adult Intelligence Test has a standard deviation of 15 in the US population (Wechsler 2008). We can use this estimate as the standard deviation for the target population. Lets say we select a sample from members of Mensa, a high IQ society, where members are specifically selected on the basis high IQ scores. If the standard deviation of Mensa members is 5, then the \\(u\\)-ratio would be,\n\\[\nu_x = \\frac{S_{X|\\mathcal{S}(X)}}{S_X} = \\frac{5}{15}= .33\\, .\n\\]\nIf an estimate of the population standard deviation is not readily available, then a reliability coefficient from the reference sample and the sample under selection can be used to estimate the \\(u\\)-ratio,\n\\[\nu_X = \\sqrt{\\frac{1-r_{XX'}}{1-r_{XX'|\\mathcal{S}(X)}}}.\n\\]\nWhere \\(r_{XX'|\\mathcal{S}(X)}\\) and \\(r_{XX'}\\) are the reliability estimates within the selected and reference sample respectively."
  },
  {
    "objectID": "direct_range_restriction.html#correcting-correlations-for-direct-range-restriction",
    "href": "direct_range_restriction.html#correcting-correlations-for-direct-range-restriction",
    "title": "9  Direct Selection",
    "section": "9.5 Correcting Correlations for Direct Range Restriction",
    "text": "9.5 Correcting Correlations for Direct Range Restriction\n\n9.5.1 Defining our Target Quantity\nWe want to estimate the correlation in the target population between true scores of the independent (\\(T\\)) and dependent variable (\\(U\\)). Within a study that suffers from direct selection and measurement error, the observed score correlation will be biased relative to our target true score population correlation, \\(\\rho_{TU}\\). We can model observed score correlations under univariate direct selection as,\n\\[\nr_{XY|\\mathcal{S}(X)} = a \\rho_{TU} + e\n\\]\nor under bivariate direct selection,\n\\[\nr_{XY|\\mathcal{S}(X,Y)} = a \\rho_{TU} + e.\n\\]\nWhere \\(a\\) is the artifact attenuation/inflation factor and \\(e\\) is the sampling error term. In either case, an unbiased estimate of the total population true score correlation can be estimated by dividing by an estimate of \\(a\\),\n\\[\nr_{XY|\\mathcal{S}(X)} = a \\rho_{TU} + e\n\\]\nor\n\\[\nr_{XY|\\mathcal{S}(X,Y)} = a \\rho_{TU} + e.\n\\]\n\n\n9.5.2 Artifact Correction for Correlations\n\nThe Univariate Case\nRange restriction (or enhancement) in either the independent or dependent variable will induce bias into the correlation coefficient. Let us consider a case where a study directly selects on \\(X\\) and not \\(Y\\). It is important to note, that if there is direct selection one of the two variables, then there will be indirect selection in the other variable too if the two are correlated. This would suggest that if \\(u_X\\neq 1\\) and \\(\\rho_{XY}\\neq 0\\) then \\(u_Y\\neq 1\\). Lets visualize the correlation between independent (\\(X\\)) and dependent (\\(Y\\)) variables under univariate direct range restriction (see Figure 9.3) by only selecting individuals above some cut off such that,\n\\[\n\\mathcal{S}(X) = \\begin{cases}1 & \\text{if }X\\geq-.50\\\\ 0 & \\text{if }X&lt;-.50  \\end{cases}\n\\]\nThe scores of individuals that have been selected will show less variance than the population pool. Specifically, the scenario below shows a \\(u\\)-ratio of \\(u_X=0.69\\) in the independent variable. We see in Figure 9.3 that the correlation in the restricted scores (\\(r_{XY|\\mathcal{S}(X)}\\)) are attenuated relative to the unrestricted correlation (\\(r_{XY}\\), indicative of \\(a&lt;1\\)).\n\n\n\n\n\nFigure 9.3: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under univariate direct range restriction. Dark blue dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nWe can also visualize what happens to the correlation when the range is enhanced. Enhancement can be accomplished by selecting individuals at the ends of the distribution (Taylor and Griess 1976). Therefore we can define the selection function for Figure 9.4 as,\n\\[\n\\mathcal{S}(X) = \\begin{cases}1 & \\text{if }X\\leq-.50 \\text{ or } X\\geq.50\\\\0 & \\text{if }X&gt;-.50 \\text{ or } X&lt;.50\\end{cases}\n\\]\nIn the visualization below, we see an opposite effect on the correlation, that is, an inflation of the unrestricted correlation rather than an attenuation (indicating \\(a&gt;1\\)) like we see under range restriction. The scenario below has a \\(u\\)-ratio \\(u_X=1.26\\) in the independent variable.\n\n\n\n\n\nFigure 9.4: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under univariate direct range enhancement. Dark blue dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nIt starts to become apparent that if \\(u_X&gt;1\\) (i.e., \\(S_X&gt;S_{X|\\mathcal{S}(X)}\\)) the observed correlation is inflated and when \\(u_X&lt;1\\) it becomes attenuated (Sackett and Yang 2000).\nThe attenuation/inflation of the correlation is dependent on the magnitude of the correlation, this is due to the fact that selection occurs on \\(X\\) and \\(X\\) is correlated with \\(Y\\), there will also be indirect range restriction in \\(Y\\). Therefore unlike other artifacts that have been discussed so far, range restriction/enhancement depends not only on the artifact value (i.e., the \\(u\\)-ratio), but also on the restricted correlation (Hunter, Schmidt, and Le 2006). The attenuation/inflation factor can be estimated as (adapted from equation 4, Hunter, Schmidt, and Le 2006),\n\\[\n\\hat{a} = u_X \\sqrt{1+r_{XY|\\mathcal{S}(X)}^2\\left(\\frac{1}{u_X}-1\\right)}\n\\]\nA bias correction formula for univariate direct selection was first developed by Pearson (1903) and also provided more recently by Hunter, Schmidt, and Le (2006) and Wiernik and Dahlke (2020). To correct for the systematic bias in correlations, we can divide the correlation under selection by the attenuation factor,\n\\[\nr_{XY} = \\frac{r_{XY|\\mathcal{S}(X)}}{\\hat{a}} = \\frac{r_{XY|\\mathcal{S}(X)}}{u_X\\sqrt{1+r_{XY|\\mathcal{S}(X)}^2\\left(\\frac{1}{u^2_X}-1\\right)}}.\n\\tag{9.1}\\]\nWhere the standard error of the corrected correlation is,\n\\[\nse(r_{XY}) = \\frac{se\\left(r_{XY|\\mathcal{S}(X)}\\right)}{\\hat{a}}.\n\\tag{9.2}\\]\nIf we want to also correct for measurement error in both samples, then we can also incorporate the reliability into these equations. Note that the following equations will incorporate the reliability within the selected sample (\\(r_{XX'|\\mathcal{S}(X)}\\)) rather than the reference sample (\\(r_{XX'}\\)). If the reliability coefficient comes from the reference sample, then we can estimate the selected (restricted or enhanced) sample reliability with the corresponding \\(u\\)-ratio,\n\\[\nr_{XX'|\\mathcal{S}(X)} = 1-\\frac{1-r_{XX'}}{u^2_X}.\n\\tag{9.3}\\]\nThen we can use the reliability and \\(u\\)-ratios simultaneously to calculate a new attenuation/inflation factor that accounts for both bias in measurement error and direct selection,\n\\[\n\\hat{a} = u_X\\sqrt{1-u_X^2(1-r_{XX'|\\mathcal{S}(X)})}\\sqrt{r_{YY'|\\mathcal{S}(X)}+r_{XY|\\mathcal{S}(X)}^2\\left(\\frac{1}{u^2_X}-1\\right)}.\n\\]\nIn the following equation to obtain an unbiased estimate of the true score unrestricted population correlation (adapted from table 3, Wiernik and Dahlke 2020),\n\\[\n\\small{r_{TU}=\\frac{r_{XY|\\mathcal{S}(X)}}{\\hat{a}} =\\frac{r_{XY|\\mathcal{S}(X)}}{u_X\\sqrt{1-u_X^2(1-r_{XX'|\\mathcal{S}(X)})}\\sqrt{r_{YY'|\\mathcal{S}(X)}+r_{XY|\\mathcal{S}(X)}^2\\left(\\frac{1}{u^2_X}-1\\right)}}}\n\\]\nWhere the standard error can be corrected similarly,\n\\[\nse(r_{TU}) = \\frac{se(r_{XY|\\mathcal{S}(X)})}{\\hat{a}}\n\\]\nThere are three important things to note about the equations in this section: 1) these corrections assume linearity and homoskedasticity in the target population population, 2) all these corrections show selection on the independent variable, \\(X\\), however it does not matter whether univariate selection is on \\(X\\) or \\(Y\\), the corrections can be applied in the same fashion (just remember to flip \\(X\\) and \\(Y\\) and vice versa in the equations), 3) The corrections assume that any range restriction/enhancement observed in the non-selection variable (in our example this would be \\(Y\\)) is mediated by the range restriction/enhancement in the variable under selection (i.e., \\(X\\)).\n\n\n\n\n\n\nApplied Example in R\n\n\n\nTo continue with our example of Mensa members (a high IQ society), let’s say a researcher wants to know the correlation between IQ and test anxiety. The researcher conduct’s a study on a sample of 100 Mensa members and finds a sample mean IQ of \\(135\\) and a standard deviation of \\(S_{X|\\mathcal{S}(X)}=5\\). Since the mean IQ in the general population is 15, the \\(u\\)-ratio is computed as \\(u_X=.333\\). The researcher then finds a correlation of \\(r_{XY|\\mathcal{S}(X)}=-.15\\) between IQ test scores and test anxiety. The reliability of the IQ test and the anxiety measure is \\(r_{XX'|\\mathcal{S}(X)}=.85\\) and \\(r_{YY'|\\mathcal{S}(X)}=.80\\), respectively.\nIn R, we can correct the correlation by using the correct_r function in the psychmeta package (Dahlke and Wiernik 2019). The correction = 'uvdrr_x'\n\nlibrary(psychmeta)\n\ncorrect_r(correction = 'uvdrr_x',\n          rxyi = -.15,  # restricted correlation\n          ux = .333,   # u ratio of IQ scores\n          rxx = .85,   # reliability of IQ scores\n          ryy = .80,   # reliability of test anxiety measure\n          n = 100)    # sample size\n\nCorrelations Corrected for Measurement Error and Univariate Direct Range Restriction:\n---------------------------------------------------------------------------------------\n   value CI_LL_95 CI_UL_95   n n_effective\n1 -0.459    -0.78     0.16 100        10.6\n\n\nThe true score correlation in the target population is estimated to be \\(r_{TU} = .46\\, [-.16,\\, .78]\\).\n\n\n\n\nThe Bivariate Case\nBivariate direct range restriction/enhancement occurs when selection occurs on both variables of interest, therefore the selection function will be a function of \\(X\\) and \\(Y\\). Let’s visualize the correlation between independent (\\(X\\)) and dependent (\\(Y\\)) variables under bivariate range restriction by only selecting individuals above some cut off point for both \\(X\\) and \\(Y\\) (see Figure 9.5). For this example the selection function is\n\\[\n\\mathcal{S}(X,Y) = \\begin{cases}1 & \\text{if } X\\geq -.5 \\text{ and }Y\\geq-.5\\\\ 0 & \\text{if }X&lt;-.5  \\text{ and }Y&lt;-.5 \\end{cases}\n\\]\nThe scores of individuals that have been selected will show less variance than the entire pool of individuals. Specifically, the scenario below shows a \\(u\\)-ratio of about 0.70 in the independent variable and dependent variables. We see in the figure that the correlation in the restricted sample (\\(r_{XY|\\mathcal{S}(X,Y)}\\)) is attenuated relative to the unrestricted correlation (\\(r_{XY}\\)).\n\n\n\n\n\nFigure 9.5: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under bivariate direct range restriction. Dark blue dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nLikewise let’s visualize what happens to the correlation when the range is enhanced. Enhancement in both variables can be accomplished by selecting individuals at the ends of the distribution of \\(X\\) and \\(Y\\). Therefore we can define the selection function as,\n\\[\n\\small{\\mathcal{S}(X,Y) = \\begin{cases}1 & \\text{if } (X\\leq -.5 \\text{ or } X\\geq .5) \\text{ and } (Y\\leq -.5 \\text{ or } Y\\geq .5) \\\\ 0 & \\text{if }(X&gt;-.5 \\text{ or }X&lt;.5)  \\text{ and }(Y&gt;-.5 \\text{ or }Y&lt;.5) \\end{cases}}\n\\]\nNote that this type of selection would be exceedingly rare to see in practice. In Figure 9.6, we see inflation of the enhanced correlation relative to the target correlation. The scenario below has a \\(u\\)-ratio of about 1.32 in both the independent variable and dependent variable.\n\n\n\n\n\nFigure 9.6: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under bivariate direct range enhancement. Dark blue dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nA bias correction formula for bivariate range restriction is much more complicated than the univariate formulation. To break down the correction formula into simpler parts, let us first define a factor we will denote with the Greek letter \\(\\psi\\),\n\\[\n\\psi = \\frac{u_X u_Y\\left(r_{XY|\\mathcal{S}(X,Y)}^2-1\\right)}{2r_{XY|\\mathcal{S}(X,Y)}}\n\\tag{9.4}\\]\nThis factor contains all the parameters needed to correct the correlation coefficient under direct selection. An unbiased estimate of the target population correlation can obtained by the following correction formula (adapted from table 3 Wiernik and Dahlke 2020),\n\\[\nr_{XY} = \\psi + \\text{sign}\\left[r_{XY|\\mathcal{S}(X,Y)}\\right]\\sqrt{\\psi^2+1}\n\\tag{9.5}\\]\nWhere the standard error can be computed from calculating the artifact factor (\\(a\\)) from the corrected and restricted/enhanced correlation,\n\\[\nse(r_{XY}) = \\frac{se\\left(r_{XY|\\mathcal{S}(X,Y)}\\right)}{\\hat{a}}= \\frac{se\\left(r_{XY|\\mathcal{S}(X,Y)}\\right)}{\\left[\\frac{r_{XY|\\mathcal{S}(X,Y)}}{r_{XY}}\\right]}.\n\\tag{9.6}\\]\nNow we can also incorporate measurement error into the correction formula. Note that the following equations will incorporate the reliability within the selected sample (\\(r_{XX'|\\mathcal{S}(X,Y)}\\)) rather than the unrestricted population (\\(r_{XX'}\\); see Equation 9.3 on converting to the selected sample). Then we can use the restricted/enhanced (selected) sample reliability and the \\(u\\)-ratios in the following equation to obtain an unbiased estimate of the target true score population correlation,\n\\[\nr_{TU} = \\frac{\\psi + \\text{sign}\\left[r_{XY|\\mathcal{S}(X,Y)}\\right]\\sqrt{\\psi^2+1}}{\\sqrt{1-u_X^2\\left(1-r_{XX'|\\mathcal{S}(X,Y)}\\right)}\\sqrt{1-u_Y^2\\left(1-r_{YY'|\\mathcal{S}(X,Y)}\\right)}}.\n\\]\nIf the reliability coefficient comes from the unrestricted population, the formula simplifies to,\n\\[\nr_{TU} = \\frac{\\psi + \\text{sign}\\left[r_{XY|\\mathcal{S}(X,Y)}\\right]\\sqrt{\\psi^2+1}}{\\sqrt{r_{XX'}}\\sqrt{r_{YY'}}}.\n\\]\nWe can use the same equation as Equation 9.6 to calculate the corrected standard error. The standard error can then be calculated as,\n\\[\nse(r_{TU}) = \\frac{se\\left(r_{XY|\\mathcal{S}(X,Y)}\\right)}{\\hat{a}}= \\frac{se\\left(r_{XY|\\mathcal{S}(X,Y)}\\right)}{\\left[\\frac{r_{XY|\\mathcal{S}(X,Y)}}{r_{TU}}\\right]}.\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say we are the admissions department at an top-tier university. This university only admits students that have a high school GPA over 3.5 and a standardized test score over 650. We decide to conduct a study on the predictive validity (i.e., the correlation) between standardized test performance and college grade-point average (GPA). However, the admitted students have a very narrow range of both GPA and test scores wheras the total pool of college applicants have a much wider range. We calculate \\(u\\)-ratios of test scores (\\(u_{X}=0.70\\)) and GPA (\\(u_Y = .80\\)).\nIn R, we can correct the correlation by using the correct_r function in the psychmeta package (Dahlke and Wiernik 2019). The correction = 'bvdrr'\n\nlibrary(psychmeta)\n\ncorrect_r(correction = 'bvdrr',\n          rxyi = .25,  # restricted correlation\n          ux = .70,   # u ratio of IQ scores\n          uy = .80,   # u ratio of GPA\n          n = 100)    # sample size\n\nCorrelations Corrected for Measurement Error and Bivariate Direct Range Restriction:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1   0.4   0.0999    0.597 100          32\n\n\nThe correlation in the target population is estimated to be \\(r_{XY} = .40\\, [.10,\\, .60]\\)."
  },
  {
    "objectID": "direct_range_restriction.html#correcting-standardized-mean-differences-for-direct-range-restriction",
    "href": "direct_range_restriction.html#correcting-standardized-mean-differences-for-direct-range-restriction",
    "title": "9  Direct Selection",
    "section": "9.6 Correcting Standardized Mean Differences for Direct Range Restriction",
    "text": "9.6 Correcting Standardized Mean Differences for Direct Range Restriction\n\n9.6.1 Defining our Target Quantity\nThe quantity of interest is the target population standardized mean difference between actual members of groups \\(A\\) and \\(B\\) on true scores, \\(U\\). We can denote this standardized mean difference as \\(\\delta_{GU}\\). Within a study that suffers from direct selection, the observed standardized mean difference (\\(d_{gY|\\mathcal{S}(Y)}\\)) will be biased relative to the target, \\(\\delta_{GU}\\). We can model the observed standardized mean difference as,\n\\[\nd_{gY|\\mathcal{S}(Y)}= a \\delta_{GU} + e.\n\\]\nWhere \\(a\\) is the attenuation/inflation factor and \\(e\\) is the sampling error term. Therefore an unbiased estimate of the target population true score standardized mean difference would be computed from\n\\[\nd_{GU} = \\frac{ d_{gY|\\mathcal{S}(Y)} }{ \\hat{a}}.\n\\]\n\n\n9.6.2 Artifact Correction for Standardized Mean Difference\n\n9.6.2.1 Selection on the Continuous Variable\nTo correct for direct selection on the continuous variable, we can first convert the observed standardized mean difference (\\(d_{gY|\\mathcal{S}(X)}\\)) to a point-biserial correlation (\\(r_{gY|\\mathcal{S}(X)}\\)). Converting \\(d_{gY|\\mathcal{S}(X)}\\) to \\(r_{gY|\\mathcal{S}(X)}\\) can be done by using the observed proportion of individuals in group \\(A\\) (or \\(B\\)), \\(p_g\\),\n\\[\nr_{gY|\\mathcal{S}(X)} = \\frac{d_{gY|\\mathcal{S}(X)}}{\\sqrt{\\frac{1}{p_g(1-p_g)}-d_{gY|\\mathcal{S}(X)}^2}}.\n\\]\nWe can then correct the point-biserial correlation for univariate direct selection using the formulas in Section 9.5.2. Note that if you want to correct for measurement error as well, replace \\(r_{XX'}\\) with \\(r_{gg'}\\) (i.e., group classification reliability; see chapter on group misclassification) whenever you are working with standardized mean differences. Once we obtained the corrected correlation, \\(r_{GU}\\), we can convert back to a standardized mean difference, we need to use an adjusted group proportions, \\(p_g^*\\):\n\\[\nd_{GU} = \\frac{r_{GU}}{\\sqrt{p_g^*\\left(1-p_g^*\\right)\\left(1-r_{GU}^2\\right)}}.\n\\]\nWhere \\(p_g^*\\) is\n\\[\np_g^* = \\frac{1}{2}-\\frac{1}{2}\\sqrt{1-4p_g(1-p_g)\\left[1+r_{gY|\\mathcal{S}(Y)}^2\\left(\\frac{1}{u^2_X}-1\\right)\\right]}\n\\]\nThe adjusted proportion, \\(p_g^*\\), can also be estimated from the proportion of individuals in the target population (e.g., the proportion of men vs women in the general population). This adjustment is necessary in order to account for indirect selection in the grouping variable when \\(d\\neq 0\\). This is similar to the situation described in Section 9.5.2, where one variable suffers from direct range restriction and any variable that is correlated with it, will suffer from indirect selection. The corresponding corrected sampling error can also be computed with the observed and adjusted proportions such that,\n\\[\n\\small{se(d_{GU}) = \\frac {se\\left(d_{gY|\\mathcal{S}(Y)}\\right)\\left(\\frac{r_{GU}}{r_{gY|\\mathcal{S}(Y)}}\\right)^2} {\\left(1+d_{gY|\\mathcal{S}(Y)}^2p_g[1-p_g]\\right)^2\\left(d_o^2+\\frac{1}{p_g(1-p_g)}\\right)p^*(1-p_g^*)(1-r_{GU}^2)^3}.}\n\\]\n\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of meta-analysis: correcting error and bias in research findings (third). Third. Thousand Oaks, California: Sage Publications.\n\n\nHunter, John E., Frank L. Schmidt, and Huy Le. 2006. “Implications of Direct and Indirect Range Restriction for Meta-Analysis Methods and Findings.” Journal of Applied Psychology 91 (3): 594–612. https://doi.org/10.1037/0021-9010.91.3.594.\n\n\nPearson, Karl. 1903. “I. Mathematical Contributions to the Theory of Evolution. XI. On the Influence of Natural Selection on the Variability and Correlation of Organs.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 200 (321-330): 1–66. https://doi.org/10.1098/rsta.1903.0001.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nTaylor, Erwin K., and Thomas Griess. 1976. “The Missing Middle in Validation Research.” Personnel Psychology 29 (1): 5–11. https://doi.org/10.1111/j.1744-6570.1976.tb00397.x.\n\n\nWechsler, David. 2008. Wechsler Adult Intelligence Scale–Fourth Edition. 4th ed. https://doi.org/10.1037/t15169-000.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "indirect_range_restriction.html#introduction",
    "href": "indirect_range_restriction.html#introduction",
    "title": "10  Indirect Selection",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nIndirect selection occurs when the selection process is not directly on the variable of interest, but rather on another related variable. Similar to direct range restriction, this will cause restriction (or enhancement) in the variable of interest."
  },
  {
    "objectID": "indirect_range_restriction.html#an-example-of-indirect-range-restriction",
    "href": "indirect_range_restriction.html#an-example-of-indirect-range-restriction",
    "title": "10  Indirect Selection",
    "section": "10.2 An Example of Indirect Range Restriction",
    "text": "10.2 An Example of Indirect Range Restriction\nImagine a research team is conducting a study on academic motivation among college students using a survey that includes various questions related to academic engagement, goal orientation, and effort investment. The researchers administer the survey to a large sample of students across different universities. However, during the data cleaning process, the researchers identify a subset of respondents who exhibited signs of inattentiveness and carelessness in their responses. These signs include straight-lining questions (e.g., consistently selecting the same response option without reading the questions) or responding randomly without considering the content of the questions. Recognizing that inattentive or careless responding can distort the measurement of academic motivation, the researchers decide to exclude these individuals from the analysis. The rationale is to ensure that the data collected represents genuine responses and validly measures academic motivation. The unintended consequence of this decision is indirect range restriction. By removing inattentive and careless responders, who likely also have lower academic motivation and engagement, from the dataset, the observed range of academic motivation scores is reduced. The excluded individuals, who may have had lower academic motivation scores, are not accounted for in the analysis, resulting in an underestimation of the variability of academic motivation relative to the population."
  },
  {
    "objectID": "indirect_range_restriction.html#selection-functions",
    "href": "indirect_range_restriction.html#selection-functions",
    "title": "10  Indirect Selection",
    "section": "10.3 Selection Functions",
    "text": "10.3 Selection Functions\nIn the previous chapter we introduced the concept of a selection function for direct range restriction. Here we will expand on that section for the case of indirect range restriction, where we must introduce a new variable, \\(Z\\). Indirect selection by definition means that the variable of interest is not used in the selection process, rather, the selection directly on another variable such that, \\(\\mathcal{S}(Z)\\) (Figure 10.1 illustrates the selection process).\n\n\n\nFigure 10.1: Diagram illustrating the selection process. The variable, \\(X\\), is selected based on a function of itself. Therefore the variable under selection is conditional on the selection procedure \\(X|\\mathcal{S}(X)\\).\n\n\nHowever, selecting on \\(Z\\) may affect the distribution of \\(X\\) depending on the relationship between \\(X\\) and \\(Z\\). Particularly, if we assume normally distributed variables the correlation between \\(X\\) and \\(Z\\) can entirely describe the affect that \\(\\mathcal{S}(Z)\\) has on the distribution of \\(X\\). If \\(X\\) and \\(Z\\) are independent (\\(\\rho_{XZ}=0\\)), then selection on \\(Z\\) would not affect the distribution of \\(X\\) such that,\n\\[\nf(X|\\mathcal{S}(Z)) = f(X),\\;\\;\\;\\; \\rho_{XZ}= 0\n\\]\nFor example, consider that we select individual’s in the top half of \\(Z\\) (i.e., above the mean, \\(\\mu_{Z}\\)), such that our selection function can be defined as,\n\\[\\mathcal{S}(Z) = \\begin{cases}1 & \\text{if }Z\\geq\\mu_{Z}\\\\ 0 & \\text{if }Z&lt;\\mu_{Z} \\end{cases}\\] If the correlation, \\(\\rho_{XZ}\\) is positive, then the distribution of \\(X|\\mathcal{S}(Z)\\) (i.e., \\(X\\) given selection on \\(Z\\)) will have a higher mean and less variability (see Figure 10.1)\n\n\n\nDiagram illustrating the impact of selecting on \\(Z\\) has on \\(X\\) given a positive correlation. The selection function on the top of the plot denotes what values of \\(Z\\) are selected (i.e., \\(\\mathcal{S}(Z)\\)). The distributions on the right indicate the distribution of \\(X\\) (in black) and the distribution \\(X\\) after\n\n\nIf there is no correlation between \\(X\\) and \\(Z\\), then the distribution of \\(X|\\mathcal{Z}\\) would be left unchanged (see Figure 10.2).\n\n\n\nFigure 10.2: Diagram illustrating the direct selection function. The variable, \\(X\\), in the population is selected based on a function of itself. Therefore the variable under selection is conditional on the selection procedure \\(X|\\mathcal{S}(X)\\)."
  },
  {
    "objectID": "indirect_range_restriction.html#quantifying-selection-induced-restrictionenhancement",
    "href": "indirect_range_restriction.html#quantifying-selection-induced-restrictionenhancement",
    "title": "10  Indirect Selection",
    "section": "10.4 Quantifying Selection-Induced Restriction/enhancement",
    "text": "10.4 Quantifying Selection-Induced Restriction/enhancement\nThe distribution of scores in the target population may exhibit a greater (or lesser) degree of variability compared to the sample that has been selected into the study. Therefore the standard deviation of scores in the target population (\\(\\sigma_{X}\\)) may differ from the population under selection. (\\(\\sigma_{X|mathcal{S}(Z)}\\)). To index the difference between the two standard deviations, we can calculate the \\(u\\)-ratio Wiernik and Dahlke (2020). The \\(u\\)-ratio is the ratio between the standard deviations of the population under selection and the target population such that (\\(\\upsilon\\) denotes the population \\(u\\)-ratio),\n\\[\n\\upsilon_X = \\frac{\\sigma_{X|\\mathcal{S}(Z)}}{\\sigma_{X}}\n\\]\nThe \\(u\\)-ratio in cases of range restriction will exist in the interval (0–1). Conversely, when the \\(u\\)-ratio is greater than 1 it is indicative of range enhancement. The target population standard deviation is often quite difficult to acquire since we do not usually have access to a random sample from that population. However, the target population standard deviation can be estimated from a reference sample that is representative of the target population. This often comes in the form of standardization samples or norm samples (obtained from test manuals) if the unrestricted group is the general population. For example, the distribution full-scale IQ scores derived from the Wechsler Adult Intelligence Test has a standard deviation of 15 in the US population (Wechsler 2008). We can use this estimate as the standard deviation for the unrestricted population. Lets say we select a sample from members of Harvard students, who tend to have higher IQs than the general population (this is due to the fact that selection criterion, such as GPA and SAT scores are positively correlated with IQ). If the standard deviation of IQ in Harvard students is 10, then the \\(u\\)-ratio would be,\n\\[\nu_X =  \\frac{S_{X|\\mathcal{S}(Z)}}{S_X} = \\frac{10}{15}= .67\n\\]\nHowever it is not always the case that an estimate of the unrestricted standard deviation is readily available. Therefore if the reliability coefficient from the reference sample and the sample under selection can be used to estimate the \\(u\\)-ratio,\n\\[\nu_X = \\sqrt{\\frac{1-r_{XX'}}{1-r_{XX'|\\mathcal{S}(Z)}}}\n\\]\nWhere \\(r_{XX'|\\mathcal{S}(Z)}\\) and \\(r_{XX'}\\) are the reliability estimates from the sample under selection and the reference (target population) sample, respectively. In the context of indirect range restriction, the selection does not occur directly on \\(X\\) (or \\(Y\\)), instead it occurs on a third variable, \\(Z\\). The affect that selection on \\(Z\\) has on \\(X\\) is dependent on the correlation between them, \\(\\rho_{XZ}\\). Therefore we can see how the \\(u\\)-ratio of \\(Z\\) (\\(u_Z\\)) related to the \\(u\\)-ratio of \\(X\\),\n\\[\nu_X = \\sqrt{\\rho_{XZ}^2u_Z^2 -\\rho_{XZ}^2 + 1 }\n\\]\nIf \\(\\rho_{XZ}=0\\), then you will notice that \\(u_X=1\\), effectively having no selection effect on \\(X\\). Also, notice that a correlation of \\(\\rho_{XZ}=1\\) will return \\(u_X=u_Z\\), indicating that selecting on \\(Z\\) would affect the variation of \\(Z\\) similarly to the variation in \\(X\\). This relationship between \\(u_X\\), \\(u_Z\\), and \\(\\rho_{XZ}\\) can be visualized in Figure 10.3\n\n\n\n\n\nFigure 10.3: The impact of the association between \\(X\\) and \\(Z\\). The data consists of \\(N=1,000\\) simulated observations, where the red plots show a low correlation case (\\(\\rho_{XZ}=.20\\)) and the blue plots show a high correlation case (\\(\\rho_{XZ}=.80\\)). The top plots show the relationship between \\(X\\) and \\(Z\\) with the darker points indicating individuals that have been selected into the sample. The bottom plots shows the probability of selection into the sample as a function of X. The dark distribution on the top of the plot show the distribution of individuals in the selected sample. Notice that the distribution of the selected individual’s in the high correlation case is a narrower distribution than the low correlation case."
  },
  {
    "objectID": "indirect_range_restriction.html#correcting-correlations-for-indirect-range-restriction",
    "href": "indirect_range_restriction.html#correcting-correlations-for-indirect-range-restriction",
    "title": "10  Indirect Selection",
    "section": "10.5 Correcting Correlations for Indirect Range Restriction",
    "text": "10.5 Correcting Correlations for Indirect Range Restriction\n\n10.5.1 Defining our Target Quantity\nWe want to estimate the population correlation of the true scores of the independent variable (\\(T\\)) and dependent variable (\\(U\\)). We can denote this correlation as \\(\\rho_{TU}\\). Within a study sample that suffers from indirect selection (and sampling error), the study correlation is under selection (\\(r_{xy_S}\\)) will be biased relative to our target quantity, \\(\\rho_{xy}\\). This bias is captured by an artifact attenuation/inflation factor, \\(a\\), such that,\n\\[\nr_{XY} = a \\rho_{XY|\\mathcal{S}(Z)} + e\n\\]\nTherefore an unbiased estimate of the true score target population correlation can be estimated by dividing the observed score correlation under selection by an estimate of \\(a\\),\n\\[\nr_{TU} = \\frac{ r_{XY|\\mathcal{S}(Z)} }{ \\hat{a}}.\n\\]\n\n\n10.5.2 Artifact Correction for Correlations\n\nThe Univariate Case\nRange restriction (or enhancement) in either the independent or dependent variable will induce bias into the correlation coefficient. Let us consider a case where we select individuals based on meeting some criterion of some third variable, \\(Z\\), such that \\(\\mathcal{S}(Z)\\). In the univariate case, we assume that selection on \\(Z\\) only directly affects restriction/enhancement in \\(X\\) while any restriction/enhancement in \\(Y\\) is mediated by the effect on \\(X\\) (see Figure 10.4).\nNow consider a study where we want to calculate correlation in the target population between an independent variable, \\(X\\), and a dependent variable, \\(Y\\). However, the individual’s are selected whether they are above the mean of \\(Z\\) (Mean = 0). We can thus define the selection function such that,\n\\[\n\\mathcal{S}(Z) = \\begin{cases} 1 & \\text{if } Z \\geq 0 \\\\ 0 & \\text{if } Z &lt; 0 \\end{cases}\n\\]\nIn the following examples, we will simulate a correlation of \\(\\rho_{XZ}=.80\\).\n\n\n\nFigure 10.4: The assumed correlation structure of univariate indirect selection. The correlation between \\(Z\\) and \\(Y\\) is completely mediated by \\(X\\) (i.e., the partial correlation between \\(Y\\) and \\(Z\\), controlling for \\(X\\) is zero such that, \\(\\rho_{YZ.X}=0\\)).\n\n\nFigure 10.5 shows a \\(u\\)-ratio of about \\(u_X=0.75\\) in the independent variable. We see that the sample correlation in the restricted scores (\\(r_{XY|\\mathcal{S}(Z)}=.42\\)) is attenuated relative to the unrestricted correlation (\\(r_{XY}=.50\\)).\n\n\n\n\n\nFigure 10.5: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under univariate indirect range restriction. Dark red dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nWe can also visualize what happens to the correlation when the range is enhanced. Enhancement can be accomplished by selecting individuals at the ends of the distribution (Taylor and Griess 1976). For indirect selection, individuals are selected at the ends of the distribution of \\(Z\\) such that the selection function can be defined as,\n\\[\n\\mathcal{S}(Z) = \\begin{cases} 1 & \\text{if } Z \\leq -1 \\text{ or } Z \\geq 1 \\\\ 0 & \\text{if } Z &lt; -1 \\text{ or } Z &gt;1 \\end{cases}\n\\]\nIn Figure 10.6, we see an opposite effect on the correlation, that is, an inflation of the correlation rather than an attenuation like we see under range restriction. The scenario below has a \\(u\\)-ratio of about \\(u_X=1.32\\) in the independent variable.\n\n\n\n\n\nFigure 10.6: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under univariate indirect range enhancement. Dark red dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nIn summary, if \\(u_X&gt;1\\) (i.e., \\(S_{X|\\mathcal{S}(Z)}&gt;S_{X}\\)) the observed correlation is inflated relative to the correlation in the target population. Whereas the correlation is attenuated when \\(u_X&lt;1\\) (i.e., \\(S_{X|\\mathcal{S}(Z)}&lt;S_{X}\\), Sackett and Yang 2000). An estimate of the attenuation/inflation factor, \\(\\hat{a}\\), can account for the bias in the observed correlation induced by range restriction/enhancement (equation 5, Le and Schmidt 2006).\n\\[\n\\hat{a} = \\sqrt{r_{XY|\\mathcal{S}(Z)}^2 + u_{X}^2 (1- r_{XY|\\mathcal{S}(Z)}^2)  }\n\\] Using the estimated attenuation/inflation factor, we can correct the observed correlation for bias induced by indirect selection\n\\[\nr_{XY} = \\frac{r_{XY|\\mathcal{S}(Z)}}{\\hat{a}} = \\frac{r_{XY|\\mathcal{S}(Z)}}{\\sqrt{r_{XY|\\mathcal{S}(Z)}^2 + u_{X}^2 (1- r_{XY|\\mathcal{S}(Z)}^2)  }}\n\\tag{10.1}\\]\nIf we wants to correct for range restriction/enhancement and measurement error, we can incorporate the reliability coefficients (under selection) of \\(X\\) (\\(r_{XX'|\\mathcal{S}(Z)}\\)) and \\(Y\\) (\\(r_{YY'|\\mathcal{S}(Z)}\\)) into the formula for \\(\\hat{a}\\),\n\\[\n\\hat{a} = \\sqrt{r_{XY|\\mathcal{S}(Z)}^2 + \\frac{u_{X}^2 r_{XX'|\\mathcal{S}(Z)}\\left(r_{XX'|\\mathcal{S}(Z)}r_{YY'|\\mathcal{S}(Z)} - r_{XY|\\mathcal{S}(Z)}^2\\right) }{1 - u_{X}^2 \\left(1-r_{XX'|\\mathcal{S}(Z)}\\right)} }\n\\]\nNow correcting the observed correlation with this modified estimate of \\(a\\) to correct the observed correlation will yield the true score correlation in the target population,\n\\[\nr_{TU}=\\frac{r_{XY|\\mathcal{S}(Z)}}{\\hat{a}} = \\frac{r_{XY|\\mathcal{S}(Z)}}{\\sqrt{r_{XY|\\mathcal{S}(Z)}^2 + \\frac{u_{X}^2 r_{XX'|\\mathcal{S}(Z)}\\left(r_{XX'|\\mathcal{S}(Z)}r_{YY'|\\mathcal{S}(Z)} - r_{XY|\\mathcal{S}(Z)}^2\\right) }{1 - u_{X}^2 \\left(1-r_{XX'|\\mathcal{S}(Z)}\\right)} }}\n\\]\nIf the reliability coefficients come from the target population and do not suffer selection effects, we can estimate the reliability under selection using the following formulas (equation 11 and 12 Le and Schmidt 2006):\n\\[\nr_{XX'|\\mathcal{S}(Z)} = 1-\\frac{1-r_{XX'}}{u_X^2}\n\\tag{10.2}\\]\n\\[\nr_{YY'|\\mathcal{S}(Z)} = 1-\\frac{1-r_{YY'}}{u_Y^2}\n\\tag{10.3}\\]\nWe now need to adjust the standard error for the corrected correlation coefficient. To do this, we can either divide the observed standard error by the attenuation/inflation factor (or equivalently, the observed correlation divided by the corrected correlation),\n\\[\nse(r_{TU}) = \\frac{se\\left(r_{XY|\\mathcal{S}(Z)}\\right)}{\\hat{a}}= \\frac{se\\left(r_{XY|\\mathcal{S}(Z)}\\right)}{\\left[\\frac{r_{XY|\\mathcal{S}(Z)}}{r_{TU}}\\right]}.\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s say a university admits students based on some criterion upon a composite of multiple measures of performance (\\(Z\\)). One of those measures is a standardized test (\\(X\\)). A researcher then wants to see how test performance predicts stress (\\(Y\\)) in college. Since test performance is correlated with the composite used in the college admissions process, it is likely that we will observe substantial range restriction in the college sample. To index this restriction in range, we calculate the \\(u\\)-ratio of test scores as \\(u_{X}=0.70\\) (relative to the total pool of applicants). We also want to correct for measurement error in test performance and the self-report questionnaire we use to measure stress, therefore we obtain reliability estimates within our sample of \\(r_{XX'|\\mathcal{S}(Z)}=.90\\) and \\(r_{XX'|\\mathcal{S}(Z)}=.80\\), respectively. The researcher then conducts the study and finds a sample correlation of \\(r_{XY|\\mathcal{S}(Z)}=.25\\), but he wants to know that true score correlation.\nIn R, we can correct the correlation by using the correct_r function in the psychmeta package (Dahlke and Wiernik 2019). The correction = 'uvirr_x'\n\nlibrary(psychmeta)\n\ncorrect_r(correction = 'uvirr_x',\n          rxyi = .25,  # restricted correlation\n          rxx = .90,  # reliability of test scores\n          ryy = .80,  # reliability of stress\n          ux = .70,   # u ratio of SAT scores\n          n = 100)    # sample size\n\nCorrelations Corrected for Measurement Error and Univariate Indirect Range Restriction:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.412   0.0973    0.648 100        33.8\n\n\nThe true score correlation in the target population is estimated to be \\(r_{TU} = .41\\, [.10,\\, .65]\\).\n\n\n\n\nThe Bivariate Case\nBivariate indirect range restriction/enhancement occurs when the selection variable has independent relationships with both the independent and dependent variable. Like we did for the univariate case, let’s visualize the correlation between independent (\\(X\\)) and dependent (\\(Y\\)) variables under range restriction by only selecting individuals above a score of -0.50 in our selector variable, \\(Z\\). Therefore the selection function can be defined as,\n\\[\n\\mathcal{S}(Z)=\\begin{cases} 1 &\\text{ if } Z \\geq-.50 \\\\  0 &\\text{ if } Z &lt;-.50  \\end{cases}\n\\]\nWe will also fix the correlations between the selector and independent variable (\\(\\rho_{XZ}\\)), as well as the selector and dependent variable (\\(\\rho_{YZ}\\)) to be .80. Unlike the univariate case, in the bivariate case \\(X\\) and \\(Y\\) have direct relationships with \\(Z\\) (see Figure 10.7).\n\n\n\nFigure 10.7: The assumed correlation structure of univariate indirect selection. The correlation between \\(Z\\) and \\(Y\\) is completely mediated by \\(X\\) (i.e., the partial correlation between \\(Y\\) and \\(Z\\), controlling for \\(X\\) is zero such that, \\(\\rho_{YZ.X}=0\\)).\n\n\nThe scenario displayed in Figure 10.8, shows a \\(u\\)-ratio of about \\(u_X=u_Y=0.81\\) in the independent variable and dependent variables. We see in the figure below that the correlation in the restricted sample (\\(\\rho_{XY|\\mathcal{S}(Z)}=.25\\)) is attenuated relative to the target population correlation (\\(\\rho_{XY}=.50\\)).\n\n\n\n\n\nFigure 10.8: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under bivariate indirect range restriction. Dark red dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nLikewise let’s visualize what happens to the correlation when the range is enhanced. Enhancement in both variables can be accomplished by selecting individuals at the ends of the distribution of \\(Z\\) (for this case we will select individuals below a score of -1 and above a score of 1). We can thus define the selection function as,\n\\[\n\\mathcal{S}(Z)=\\begin{cases} 1 &\\text{ if } Z \\leq-1 \\text{ or } Z \\geq 1  \\\\  0 &\\text{ if } Z &gt; -1  \\text{ or } Z &lt; 1   \\end{cases}\n\\]\nIn Figure 10.9, we observe an inflation of observed correlation (\\(\\rho_{XY|\\mathcal{S}(Z)}=.74\\)) relative to the target correlation (\\(\\rho_{XY}=.50\\)). Figure 10.9 has a \\(u\\)-ratio of about \\(u_X=u_Y=1.38\\) in both the independent variable and dependent variable.\n\n\n\n\n\nFigure 10.9: Scatterplot showing a correlation between \\(X\\) and \\(Y\\) under bivariate indirect range enhancement. Dark red dots indicate the selected sample and the transparent dots indicate the rejected sample.\n\n\n\n\nA bias correction formula for bivariate range restriction is much more complicated than the univariate formulation. In the univariate case, we did not need any more information about the selection process beyond what we could infer from \\(u_X\\). However in the bivariate case, we need to have a basic idea of the selection mechanism at play (Dahlke and Wiernik 2020). Particularly we at least know the direction of the correlation between the selector variable, \\(Z\\), and the independent (\\(\\rho_{XZ}\\)) and dependent variable (\\(\\rho_{YZ}\\)). This will require a little bit of knowledge about the selection process within a given study. Let us first define a factor we will denote with \\(\\lambda\\) (Dahlke and Wiernik 2020). This factor takes into account the direction of the correlation of \\(\\rho_{XZ}\\) (if positive, we can set \\(\\rho_{XZ}=1\\), if negative, \\(\\rho_{XZ}=-1\\), if zero, \\(\\rho_{XZ}=0\\)) and \\(\\rho_{YZ}\\) (repeat the same procedure as \\(\\rho_{XZ}\\)). Therefore \\(\\lambda\\) can be defined as,\n\\[\\begin{aligned}\n\\lambda =& \\text{ sign}\\left(\\rho_{XZ}\\rho_{YZ} [1-u_X][1-u_Y]\\right)\\times \\\\[.3em] &\\frac{\\text{sign}\n\\left(1-u_X\\right)\\min\\left(u_X,\\frac{1}{u_X}\\right) +\n\\text{ sign}\\left(1-u_Y\\right)\\min\\left(u_Y,\\frac{1}{u_Y}\\right)\n}{\\min\\left(u_X,\\frac{1}{u_X}\\right)+\\min\\left(u_Y,\\frac{1}{u_Y}\\right)}.\n\\end{aligned}\\]\nAlthough complex, the output of \\(\\lambda\\) will be either -1, 0, or 1. We can then plug this factor into the full correction equation that provides us with an unbiased estimate of the correlation in the unrestricted population,\n\\[\nr_{XY} = r_{XY|\\mathcal{S}(Z)}u_Xu_Y+\\lambda\\sqrt{|1-u_X^2||1-u_Y^2|}\n\\]\nSimilar to the univariate formula, we can also incorporate measurement error into the correction. Measurement error will bias the correlation on top of the bias induced by range restriction/enhancement. Therefore we can incorporate the reliabilities estimated within the restricted sample (\\(r_{XX'|\\mathcal{S}(Z)}\\) and \\(r_{YY'|\\mathcal{S}(Z)}\\)), into our correction formula:\n\\[\nr_{TU} = \\frac{r_{XY|\\mathcal{S}(Z)}u_Xu_Y+\\lambda\\sqrt{|1-u_X^2||1-u_Y^2|}}{\\sqrt{1-u_X^2\\left(1-r_{XX'|\\mathcal{S}(Z)}\\right)}\\sqrt{1-u_Y^2\\left(1-r_{YY'|\\mathcal{S}(Z)}\\right)}}\n\\]\nIf the reliability estimates come from an target population reference sample, we can get estimates of the reliability coefficients in the selected sample using Equation 10.2 and Equation 10.3. We then can correct the observed sampling variance (\\(\\sigma^2_{\\varepsilon_o}\\)),\n\\[\nse(r_{TU}) = \\frac{se\\left(r_{XY|\\mathcal{S}(Z)}\\right)}{\\hat{a}}= \\frac{se\\left(r_{XY|\\mathcal{S}(Z)}\\right)}{\\left[\\frac{r_{XY|\\mathcal{S}(Z)}}{r_{TU}}\\right]}.\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nContinuing with the example from the univariate case, a university admits applicants based on some criterion upon a composite of multiple measures of performance (\\(Z\\)). Two of those measures is a standardized test (\\(X\\)) and high school grade-point average (GPA). A researcher then wants to see how test performance predicts college GPA (\\(Y\\)). Since both test performance is correlated with the composite used in the college admissions process, it is likely that we will observe substantial range restriction in the college sample. The \\(u\\)-ratio in test performance and GPA was \\(u_{X}=u_{Y}=0.80\\) (applicants are the target population). We also want to correct for measurement error in the dependent variable (grade-point average) only we use to measure stress, therefore we obtain a reliability estimate within our sample of \\(r_{YY'|\\mathcal{S}(Z)}=.90\\). The researcher then conducts the study and finds a sample correlation of \\(r_{XY|\\mathcal{S}(Z)}=.25\\).\nIn R, we can correct the correlation by using the correct_r function in the psychmeta package (Dahlke and Wiernik 2019). The correction = 'uvirr_x'\n\nlibrary(psychmeta)\n\ncorrect_r(correction = 'uvirr_x',\n          rxyi = .25,  # restricted correlation\n          ryy = .90,  # reliability of GPA\n          ux = .80,   # u ratio of SAT scores\n          n = 100)    # sample size\n\nCorrelations Corrected for Measurement Error and Univariate Indirect Range Restriction:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.323   0.0742    0.531 100        58.9\n\n\nThe true score correlation in the target population is estimated to be \\(r_{TU} = .32\\, [.07,\\, .53]\\)."
  },
  {
    "objectID": "indirect_range_restriction.html#beware-of-assumptions",
    "href": "indirect_range_restriction.html#beware-of-assumptions",
    "title": "10  Indirect Selection",
    "section": "Beware of assumptions",
    "text": "Beware of assumptions\nNote that these corrections require the following assumptions: 1) linearity between \\(X\\) and \\(Y\\), 2) homoskedasticity, that is, equal variance in \\(Y\\) at every level of \\(X\\).\n\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\n———. 2020. “Not Restricted to Selection Research: Accounting for Indirect Range Restriction in Organizational Research.” Organizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of meta-analysis: correcting error and bias in research findings (third). Third. Thousand Oaks, California: Sage Publications.\n\n\nLe, Huy, and Frank L. Schmidt. 2006. “Correcting for Indirect Range Restriction in Meta-Analysis: Testing a New Meta-Analytic Procedure.” Psychological Methods 11 (4): 416–38. https://doi.org/10.1037/1082-989X.11.4.416.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nTaylor, Erwin K., and Thomas Griess. 1976. “The Missing Middle in Validation Research.” Personnel Psychology 29 (1): 5–11. https://doi.org/10.1111/j.1744-6570.1976.tb00397.x.\n\n\nWechsler, David. 2008. Wechsler Adult Intelligence Scale–Fourth Edition. 4th ed. https://doi.org/10.1037/t15169-000.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "metaanalysis_intro.html#introduction",
    "href": "metaanalysis_intro.html#introduction",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nMeta-analysis is an analytic tool to synthesize quantitative evidence from multiple studies. By systematically combining and analyzing the results of multiple studies, meta-analysis provides a comprehensive overview, unveiling patterns, trends, and insights that individual studies might not be able to capture. Combining research findings also has the added benefit of increasing the precision of our results (i.e., greater statistical power). In this section we will cover the method described by (Hunter and Schmidt 2015) since it is readily compatible with artifact corrections (see next chapter). For the random-effects model however, we use an integrated approach that incorporates methods from Hunter and Schmidt (2015) and Hedges and Vevea (1998) that was first introduced by Morris et al. (2014). However it is important to note that there are other common methods to conduct meta-analyses that have their strengths and weaknesses (hedges2014?; Callender and Osburn 1980; Johnson, Mullen, and Salas 1995)."
  },
  {
    "objectID": "metaanalysis_intro.html#common-effect-model",
    "href": "metaanalysis_intro.html#common-effect-model",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.2 Common-Effect Model",
    "text": "11.2 Common-Effect Model\nA common effect model is the simplest form of meta-analysis. It assumes that all the variation in observed effect sizes is attributable to sampling error (see Figure 11.1). In other words, all the observed effect sizes are estimates of the same population effect size. Note that there is a distinction between fixed-effects models and a common effect model (Viechtbauer, n.d.; Laird and Mosteller 1990). The common effect model assumes that the true effect size is identical for each study while the fixed effects model does not assume this. Instead, the fixed effects model can be interpreted as the weighted average of true effects. Computationally, they are the same and provide the same parameter estimates, yet the interpretation differs (Viechtbauer, n.d.).\n\n\n\nFigure 11.1: The diagram above depicts a common effect meta-analysis of five studies. The study effect sizes are homogenous and all estimate a single true population effect size.\n\n\nThe common effect model can be modeled such that population effect size \\(\\theta\\) is held constant each study effect size estimate (\\(h_i\\)), such that,\n\\[\nh_i  = \\theta + e_i\n\\tag{11.1}\\]\nWhere \\(e_i\\) indicates sampling error and the subscript \\(i\\) denotes each study. Similar to the true score theory model that we discussed in chapter 4, the variance components of each term can similarly be written out as,\n\\[\n\\sigma^2_h = \\sigma^2_\\theta + \\sigma^2_e\n\\]\nHowever in our common effect model, the population effect size is fixed across studies and will not vary, simplifying the formula to,\n\\[\n\\sigma^2_h = \\sigma^2_e\n\\tag{11.2}\\]\nTherefore the only source of variation in the observed effect sizes, is sampling error.\nUltimately, our goal is to obtain a precise estimate of the population effect size. To obtain an estimate of the population effect size, \\(\\theta\\), we can calculate the average observed effect size, \\(\\bar{h}_i\\) from \\(k\\) studies. However, in practice, effect sizes from different studies have varying levels of precision (i.e., different standard errors). A simple arithmetic average will not account for the differences between studies in their precision. Instead, we can calculate a weighted average where the weights each study can be calculated by the inverse variance (i.e., squared standard error) of each study such that,\n\\[\nw_i = \\frac{1}{se(h_i)^2}.\n\\]\nThen we can calculate a weighted average,\n\\[\n\\hat{\\theta} =\\frac{\\sum^k_{i=1}w_ih_i}{\\sum^k_{i=1}w_i}.\n\\]\nWhere \\(\\sum^k_{i=1}\\) is the sum across all \\(k\\) studies. This weighted average will be an unbiased estimate of the population effect size. However, even though this mean effect size is more precise compared to single-study estimates, it is not exempt from error itself. We we can compute the standard error for \\(\\hat{\\theta}\\) as,\n\\[\nse(\\hat{\\theta}) = \\frac{1}{\\sqrt{\\sum^k_{i=1} w_i}}\n\\]\nThe standard error can be used to compute the 95% confidence intervals (if the sampling distribution is approximately normal) of the meta-analytic point estimate:\n\\[\nCI_{95} = \\hat{\\theta}\\pm 1.96 \\cdot se(\\hat{\\theta})\n\\]\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLets use a meta-analytic data set investigating the the effectiveness of a writing-to-learn intervention on academic achievement from Bangert-Drowns, Hurley, and Wilkinson (2004). This data set has standardized mean differences between the treatment group and a control group from \\(k=48\\) studies (total sample size: \\(n=5,576\\)) and is available within the developmental version of the metadat package (White et al. 2022). Lets conduct a common effect meta-analysis using the equations from the previous section. We can use the rma function in the metafor package (Viechtbauer 2010) to conduct a common effect (method = 'EE') meta-analysis without having to write each equation by hand.\n\nlibrary(metadat)\nlibrary(metafor)\n\n# display first 6 studies\nhead(dat.bangertdrowns2004[,c('author','year','ni','yi','vi')])\n\n\n    author year  ni     yi    vi \n1 Ashworth 1992  60  0.650 0.070 \n2    Ayers 1993  34 -0.750 0.126 \n3   Baisch 1990  95 -0.210 0.042 \n4    Baker 1994 209 -0.040 0.019 \n5   Bauman 1992 182  0.230 0.022 \n6   Becker 1996 462  0.030 0.009 \n\n# fixed effects model\nmdl &lt;- rma(data = dat.bangertdrowns2004,\n           yi = yi,\n           vi = vi,\n           method = 'EE') # Equal-effects = Common Effect\n\n# print results\ndata.frame(theta_hat = mdl$b[1],\n           se = mdl$se[1],\n           CI_ll = mdl$ci.lb[1],\n           CI_ul = mdl$ci.ub[1])\n\n  theta_hat         se     CI_ll     CI_ul\n1 0.1656264 0.02693136 0.1128419 0.2184109\n\n\nThe results show an estimated population effect of \\(\\hat{\\theta}=0.17\\, [0.11,\\, 0.22]\\)."
  },
  {
    "objectID": "metaanalysis_intro.html#random-effects-model",
    "href": "metaanalysis_intro.html#random-effects-model",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.3 Random Effects Model",
    "text": "11.3 Random Effects Model\nThe random-effects model refers to a model that allows for the population effect size to vary from study to study (see Figure 11.2). Random-effects differs from the fixed effects model in an important way: it does not assume that all observed effect sizes come from a single (fixed) population effect size (Borenstein et al. 2010). This variation in population effect sizes is called heterogeneity. In the traditional Hunter and Schmidt (2015) the weights utilized in the random effects meta-analysis are identical to the common effect model. In other conventional meta-analysis methods (Hedges and Vevea 1998), random-effect weights usually include a random effect component containing the variation in population effect sizes (this has the effect of making study weights more similar to each other with more variation in population effects). A modern approach introduced by Morris et al. (2014) and later tested by Brannick et al. (2019), added this random effect component to the Hunter-Schmidt method. The simulation study by Brannick et al. (2019), concluded that weights incorporating random effect components improved the Hunter-Schmidt estimates. Here we will discuss Hedges-Vevea’s method with some elements taken from Hunter-Schmidt.\n\n\n\nFigure 11.2: The diagram above depicts a random-effects meta-analysis of five studies. Effect sizes are more variable than the common effect meta-analysis since effect sizes vary due to sampling error and population effect sizes.\n\n\nThe model from Equation 11.1 can be changed slightly to encompass variation of the population effect size from study to study:\n\\[\nh_i = \\theta_i + e_i.\n\\]\nIn the common effect model, we assumed that all the variation in study effect sizes is accounted for by variation in sampling error (see Equation 11.2). However in the random-effects model the variance in population effect sizes (\\(\\sigma^2_\\theta\\)) is allowed to be greater than zero. The variance components can be written out as,\n\\[\n\\sigma^2_h=\\sigma^2_\\theta + \\sigma^2_e.\n\\tag{11.3}\\]\nEstimating variance components can be done computationally through an iterative estimation procedure called REstricted Maximum Likelihood (REML) estimation. The estimated variance of population effect sizes, \\(\\hat{\\sigma}_\\theta\\), can now be incorporated into the inverse variance weights alongside\n\\[\nw_i = \\frac{1}{se(h_i)^2 + \\hat{\\sigma}^2_\\theta}\n\\]\n\n\n\n\n\n\nCalculating Standard Errors\n\n\n\nThe standard error for a given study often uses the sample estimate in the sampling variance equation. For example, a study correlation, \\(r_i\\), will have a sampling variance of\n\\[\nse(r_i)^2 = \\frac{(1-\\rho_i^2)^2}{n}\n\\]\nWhere \\(\\rho_i\\) is the population effect size for that study. Since the population effect size is not known, \\(\\rho_i\\) is often replaced with the study estimate, \\(r_i\\) (also the denominator will be adjusted to be \\(n-1\\) for the decreased degrees of freedom). However this makes the sampling variances and estimates dependent on one another which can cause problems for other types analyses (e.g., funnel plot asymmetry). Instead, we will use the sample size weighted mean correlation for each study to replace \\(\\rho_i\\) (Hunter and Schmidt 2015),\n\\[\nse(r_i)^2 = \\frac{\\left(1-\\bar{r}^2\\right)^2}{n-1}\n\\]\nWhere,\n\\[\n\\bar{r} = \\frac{\\sum^k_{i=1}n_ir_i}{\\sum^k_{i=1}n_i}\n\\]\nIn studies with no heterogeneity (\\(\\sigma_\\theta=0\\)) this is nearly optimal, but it still works well in cases of substantial heterogeneity.\n\n\nNow we can estimate the mean of population effects by taking the weighted average effect size (equation 16.5, Cooper, Hedges, and Valentine 2009),\n\\[\n\\hat{\\bar{\\theta}} = \\frac{\\sum^k_{i=1}w_i h_i}{\\sum^k_{i=1}w_i}.\n\\]\nWhere \\(i=1...k\\) studies. The standard error of the mean of population effects can calculated from the summation of inverse weights (equation 16.6, Cooper, Hedges, and Valentine 2009),\n\\[\nse(\\hat{\\bar{\\theta}}) = \\frac{1}{\\sqrt{\\sum^k_{i=1}w_i}}.\n\\] The 95% confidence interval can then be calculated as,\n\\[\nCI = \\hat{\\bar{\\theta}} \\pm 1.96\\cdot se(\\hat{\\bar{\\theta}})\n\\]\nIn other conventions, the variance in population effects (\\(\\sigma^2_\\theta\\)) is denoted as \\(\\tau^2\\) (Borenstein et al. 2010; DerSimonian and Kacker 2007; Hedges and Vevea 1998), but conceptually \\(\\sigma^2_\\theta\\) and \\(\\tau^2\\) these are identical. Taking the root of \\(\\sigma^2_\\theta\\) is the standard deviation of population effect sizes which can be a useful measure of heterogeneity. Furthermore, we can use \\(\\hat{\\sigma}_\\theta\\) to calculate credibility (prediction) intervals which allows us to draw inferences about the range of plausible population effect sizes. For example, the 90% credibility interval can be calculated with the following equations:\n\\[\nCR = \\hat{\\bar{\\theta}} \\pm 1.645\\hat{\\sigma}_\\theta\n\\]\nThe confidence interval and credibility interval have fundamentally different interpretations that are often misinterpreted in published work (Whitener 1990). When we are interpreting a single realized interval (i.e., our estimate-in-hand), the 90% credibility interval can be interpreted as the region in which 90% of population effect sizes exist, however, a 95% confidence interval describes the interval in which there is a 95% probability of containing the true mean of population effect sizes. It is important to note that the confidence interval interpretation here is only valid in the case of a single realized interval (Vos and Holbert 2022), if there is more than one interval obtained from the same population of studies, then the interpretation does not hold (this would be a rare in a meta-analysis).\n\n\n\n\n\n\nApplied Example in R\n\n\n\nLet’s continue looking at the meta-analysis from Bangert-Drowns, Hurley, and Wilkinson (2004). This data set has standardized mean differences between the treatment group and a control group from \\(k=48\\) studies (total sample size: \\(n=5,576\\)) and is available within the developmental version of the metadat package (White et al. 2022). Lets conduct a common effect meta-analysis using the equations from the previous section. We can use the rma function in the metafor package (Viechtbauer 2010) to conduct a random effect meta-analysis with REML estimation we can use the method = 'REML' argument.\n\nlibrary(metadat)\nlibrary(metafor)\n\n# display first 6 studies\nhead(dat.bangertdrowns2004[,c('author','year','ni','yi','vi')])\n\n\n    author year  ni     yi    vi \n1 Ashworth 1992  60  0.650 0.070 \n2    Ayers 1993  34 -0.750 0.126 \n3   Baisch 1990  95 -0.210 0.042 \n4    Baker 1994 209 -0.040 0.019 \n5   Bauman 1992 182  0.230 0.022 \n6   Becker 1996 462  0.030 0.009 \n\n# fixed effects model\nmdl &lt;- rma(data = dat.bangertdrowns2004,\n           yi = yi,\n           vi = vi,\n           method = 'REML') # Equal-effects = Common Effect\n\n# print results\ndata.frame(theta_hat = mdl$b[1],\n           se = mdl$se[1],\n           CI_ll = mdl$ci.lb[1],\n           CI_ul = mdl$ci.ub[1])\n\n  theta_hat         se     CI_ll     CI_ul\n1 0.2219296 0.04603453 0.1317036 0.3121556\n\n\nThe results show an estimated population effect of \\(\\hat{\\theta}=0.22\\, [0.13,\\, 0.31]\\).\n\n\n\n\n\n\n\nBangert-Drowns, Robert L, Marlene M Hurley, and Barbara Wilkinson. 2004. “The Effects of School-Based Writing-to-Learn Interventions on Academic Achievement: A Meta-Analysis.” Review of Educational Research 74 (1): 29–58. https://doi.org/10.3102/00346543074001029.\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and Random-Effects Models for Meta-Analysis.” Research Synthesis Methods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nBrannick, Michael T., Sean M. Potter, Bryan Benitez, and Scott B. Morris. 2019. “Bias and Precision of Alternate Estimators in Meta-Analysis: Benefits of Blending Schmidt-Hunter and Hedges Approaches.” Organizational Research Methods 22 (2): 490–514. https://doi.org/10.1177/1094428117741966.\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test of a New Model for Validity Generalization.” Journal of Applied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nCooper, Harris M., Larry V. Hedges, and Jeff C. Valentine, eds. 2009. The Handbook of Research Synthesis and Meta-Analysis. 2nd ed. New York: Russell Sage Foundation.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects Model for Meta-Analysis of Clinical Trials: An Update.” NIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nHedges, Larry V., and Jack L. Vevea. 1998. “Fixed- and Random-Effects Models in Meta-Analysis.” Psychological Methods 3 (4): 486–504. https://doi.org/10.1037/1082-989X.3.4.486.\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of meta-analysis: correcting error and bias in research findings (third). Third. Thousand Oaks, California: Sage Publications.\n\n\nJohnson, Blair T., Brian Mullen, and Eduardo Salas. 1995. “Comparison of Three Major Meta-Analytic Approaches.” Journal of Applied Psychology 80 (1): 94–106. https://doi.org/10.1037/0021-9010.80.1.94.\n\n\nLaird, Nan M., and Frederick Mosteller. 1990. “Some Statistical Methods for Combining Experimental Results.” International Journal of Technology Assessment in Health Care 6 (1): 5–30. https://doi.org/10.1017/S0266462300008916.\n\n\nMorris, Scott, Rebecca Daisley, Megan Wheeler, and Peggy Boyer. 2014. “A Meta-Analysis of the Relationship Between Individual Assessments and Job Performance.” The Journal of Applied Psychology 100 (May). https://doi.org/10.1037/a0036938.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting meta-analyses in R with the metafor package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\n———. n.d. “Fixed-Effects and Random-Effects Models in Meta-Analysis.” https://wviechtb.github.io/metafor/index.html.\n\n\nVos, Paul, and Don Holbert. 2022. “Frequentist Statistical Inference Without Repeated Sampling.” Synthese 200 (2): 89. https://doi.org/10.1007/s11229-022-03560-x.\n\n\nWhite, Thomas, Daniel Noble, Alistair Senior, W. Kyle Hamilton, and Wolfgang Viechtbauer. 2022. Metadat: Meta-Analysis Datasets. https://CRAN.R-project.org/package=metadat.\n\n\nWhitener, Ellen M. 1990. “Confusion of Confidence Intervals and Credibility Intervals in Meta-Analysis.” Journal of Applied Psychology 75 (3): 315–21. https://doi.org/10.1037/0021-9010.75.3.315."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#introduction",
    "href": "artifact_correction_metaanalysis.html#introduction",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nArtifact correction meta-analysis, also referred to as psychometric meta-analysis, is a form of meta-analysis where effect sizes are systematically corrected for sources of bias. These sources of bias have been discussed in previous chapters 4-10. Methodology for conducting artifact correction style meta-analyses were originally pioneered by Frank Schmidt and John Hunter (2015; 1977) and then reviewed more recently by Brenton Wiernik and Jeffrey Dahlke (2020). There has also been powerful R packages developed to aide in the application of artifact correction meta-analyses that we have used in previous chapters (Dahlke and Wiernik 2019). You will notice that in this section, we do not discuss standardized mean differences. This is due to the fact that the artifact correction model is designed for pearson correlations, in order to use this method for standardized mean differences, convert to pearson correlations using the methods described in chapter 11, and then use the correction methods used below. Once you apply the corrections to the converted correlations they can then be converted back to a standardized mean difference."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#bare-bones-vs-artifact-correction-meta-analysis",
    "href": "artifact_correction_metaanalysis.html#bare-bones-vs-artifact-correction-meta-analysis",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.2 Bare Bones vs Artifact Correction Meta-Analysis",
    "text": "12.2 Bare Bones vs Artifact Correction Meta-Analysis\nChapter 11 focused on bare-bones meta-analysis, that is, meta-analyses that do not correct for biases in effect size estimates. This section will be dedicated to the artifact correction approach to meta-analysis that aims to correct for statistical artifacts. The choice between these two types of meta-analyses depends on the research question, the available data, and the assumptions researchers are willing to make. If the goal is to investigate effect sizes as they are reported while avoiding additional assumptions about the data, then a bare-bones meta-analysis might be the way to go. On the other hand, if the goal is to obtain a more accurate estimate of the target effect size by accounting for biases induced by statistical artifacts, an artifact correction meta-analysis is preferable.\n\nBare-Bones Meta-Analysis: In a bare-bones meta-analysis, the focus is on aggregating effect sizes from various studies without explicitly correcting for potential biases in these effect size estimates (see first panel of ?fig-art-corr-mdl).\nArtifact Correction Meta-Analysis: In contrast, an artifact correction meta-analysis takes into account and attempts to correct for biases that may be present in the effect size estimates from individual studies. This involves addressing potential sources of bias, such as measurement error or selection effects, through statistical techniques or adjustments. By doing so, the meta-analysis aims to provide a more accurate and unbiased estimate of the true effect size. Although it is important to note that this method will require additional assumptions about the nature of the data (see second panel of ?fig-art-corr-mdl).\n\nNote that the bare-bones model does not assume that there is no bias, rather, the bare-bones model is estimating something else entirely, that is, the population effect size."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#individual-artifact-correction-model",
    "href": "artifact_correction_metaanalysis.html#individual-artifact-correction-model",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.3 Individual Artifact Correction Model",
    "text": "12.3 Individual Artifact Correction Model\nThe individual artifact correction model corrects each effect size individually prior to conducting the meta-analysis. Let us recall the random effects model in chapter 11, that an effect size \\(h_i\\) from study \\(i\\) can be modeled such that, \\(h_i=\\theta_i+e_i\\). This model would be considered a bare-bones meta-analytic model. In practice, observed (study) effect sizes (\\(h_i\\)) tend to be biased relative to our target quantity due to many artifacts, some that we can account for and some we can not. If we decide that corrections to observed effect sizes are necessary to answer our research question, then we can construct an artifact correction model. In the artifact correction model, we can incorporate an artifact attenuation/inflation factor, \\(a\\), to the bare-bones formula such that,\n\\[\nh_{i} = a_i\\theta_i + e_i\n\\tag{12.1}\\]\nThe attenuation/inflation factor, \\(a_i\\), must be estimated for each study, \\(i\\). Using estimates of \\(a\\), Equation 12.1 can be re-arranged to obtain unbiased estimates of the target population effect size:\n\\[\n\\frac{h_i}{\\hat{a}_i} = \\theta_i + \\frac{e_i}{\\hat{a}_i}\n\\tag{12.2}\\]\nThis division of \\(\\hat{a}_i\\) will provide us with our corrected effect size estimates that we can denote with the subscript, \\(c\\),\n\\[\nh_{c_i} = \\frac{h_{i}}{\\hat{a}_i}\n\\]\nand of course we must also correct the sampling error term,\n\\[\ne_{c_i} = \\frac{e_{i}}{\\hat{a}_i}.\n\\tag{12.3}\\]\nTherefore Equation 12.2 can be expressed as,\n\\[\nh_{c_i} = \\theta_i + e_{c_i}\n\\tag{12.4}\\]\nLike we saw in chapter 11, we can breakdown the variance components of the model,\n\\[\n\\sigma^2_{h_c} = \\sigma^2_{\\theta} + \\sigma^2_{e_c}\n\\tag{12.5}\\]\nTo obtain these variance components, we can start by correcting the standard errors from each study. We can calculate the corrected standard error of the corrected correlations,\n\\[\nse(h_{c_i}) = \\frac{se(h_{i})}{\\hat{a}_i}\n\\]\nNote that correcting the standard error by dividing by \\(\\hat{a}\\) ignores any error in the estimation of \\(a\\) This may also be done by using the corrected effect size and the observed effect size to correct the sampling variance:\n\\[\n\\sigma^2_{\\varepsilon_ci} = \\sigma^2_{\\varepsilon_oi} \\left(\\frac{\\theta_{c_i}}{\\theta_{o_i}}\\right)^2\n\\]\nThe next step is to obtain the random effects weights of the study, we can do this with the inverse corrected variance for each study, \\(w_i=1/(\\sigma^2_{\\varepsilon_ci}+\\sigma^2_\\rho)\\). From here we can calculate our estimate of the mean of true population correlations,\n\\[\n\\hat{\\bar{\\vartheta}}=\\frac{\\sum^k_{i=1}n_i\\theta_{c_i}}{\\sum^k_{i=1}n_i}\n\\]\nRemember that because this is a random effects model, \\(\\hat{\\bar{\\vartheta}}\\) is not an estimate of the true population effect size, instead it is an estimate of the mean of a distribution of true population effect sizes. Now that we have an estimate of the mean and the corrected sampling variances, the variance components from Equation 12.5 can be easily calculated as follows:\n\\[\n\\sigma^2_{\\theta_c} = \\frac{\\sum^k_{i=1}n_i(\\theta_{c_i} - \\hat{\\bar{\\vartheta}})^2}{\\sum^k_{i=1}n_i}\n\\]\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_c i}}{\\sum^k_{i=1}n_i}\n\\]\n\\[\n\\sigma^2_{\\vartheta}  = \\sigma^2_{\\theta_c} - \\sigma^2_{\\varepsilon_c}\n\\]\nThe standard deviation of true effects is a useful measure of heterogeneity and is simply the square root of the variance of true population effect sizes (\\(\\sigma_{\\vartheta}\\)). From the standard deviation in true effects, we can also calculate a credibility (prediction) interval that shows the range of plausible values for which a true effect size is likely to fall,\n\\[\n\\vartheta_{\\text{Upper}} = \\hat{\\bar{\\vartheta}} + 1.645\\sigma_\\vartheta\n\\]\n\\[\n\\vartheta_{\\text{Lower}} = \\hat{\\bar{\\vartheta}} - 1.645\\sigma_\\vartheta\\, .\n\\]\nNote that this is not to be confused with confidence intervals which denotes the range of plausible values that the mean of true effects can take on. This differentiation is akin to understanding the disparity between the standard error of the mean and the standard deviation in the context of a normal distribution. We can also see how the corrections reduced the heterogeneity in the effect size estimates by comparing variance in true effect sizes (\\(\\sigma^2_{\\vartheta_o}\\)) to the variance in observed effect sizes (\\(\\sigma^2_{\\vartheta_o}\\); this can be calculated by conducting a bare-bones random effects meta-analysis described in chapter 11). The percent reduction in heterogeneity can be computed by taking the ratio of the two, \\(\\sigma^2_{\\vartheta}/\\sigma^2_{\\vartheta_o}\\). Hunter and Schmidt (2015) suggest that if 75% of the heterogeneity is accounted for by artifact corrections, then we can assume that the remaining heterogeneity is attributable to remaining artifacts that have not been addressed in the current meta-analysis. Although it is important to point out that this is simply a rule of thumb rather than a mathematical property.\n\n12.3.1 Individual Corrections in Correlations\nFor correlation coefficients we can define the model similarly to Equation 12.1, with the only difference being that we will use the notation for pearson correlations,\n\\[\nr_{o_i} = A_i\\rho_i + \\varepsilon_{o_i}\n\\]\nThe artifact correction formulation of this, corresponding to Equation 12.4, would be\n\\[\nr_{c_i} = \\rho_i + \\varepsilon_{c_i}\n\\]\nThe corresponding variance components would then be,\n\\[\n\\sigma^2_{r_c} = \\sigma^2_\\rho + \\sigma^2_{\\varepsilon_c}\n\\]\nIn order to compute the variance components as well as the mean true population correlation, we first need to calculate the study weights. We will follow a similar procedure for calculating random effects weights in chapter 11. Lets define the corrected random effects weights as,\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_ci}+\\sigma^2_\\rho}.\n\\] However the variance components, \\(\\sigma^2_{\\varepsilon_ci}\\) and \\(\\sigma^2_\\rho\\), require the weights themselves to actually estimate them, so instead we can approximate the variance components using the sample size as the weights such that,\n\\[\nw_i =\\frac{1}{\\sigma^2_{\\varepsilon_ci}+\\sigma_\\rho^2}= \\frac{1}{\\sigma^2_{\\varepsilon_ci}+(\\sigma^2_{r_c}-\\sigma^2_{\\varepsilon})} \\approx \\frac{1}{\\sigma^2_{\\varepsilon_ci}+\\left(\\frac{\\sum^k_{i=1}n_i(r_{c_i} - \\bar{r}_c)^2}{\\sum^k_{i=1}n_i}-\\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_ci}}{\\sum^k_{i=1}n_i}\\right)}\n\\]\nWhere \\(\\bar{r}_c\\) is the sample size weighted average corrected correlation. These weights can then be used to obtain a more precise estimate of the true population correlation,\n\\[\n\\hat{\\bar{\\rho}}=\\frac{\\sum_{i=1}^k w_i r_{c_i}}{\\sum_{i=1}^k w_i}\n\\] Now we can compute each of the three variance components:\n\nVariance in corrected correlations:\n\n\\[\n\\sigma^2_{r_c}=\\frac{\\sum^k_{i=1}w_i(r_{c_i} - \\hat{\\bar{\\rho}})}{\\sum^k_{i=1}w_i}.\n\\] 2) Sampling error variance: \\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sum^k_{i=1}w_i\\sigma^2_{\\varepsilon_ci}}{\\sum^k_{i=1}w_i}.\n\\] 3) Variance in population correlations: \\[\n\\sigma^2_\\rho = \\sigma^2_r - \\sigma^2_{\\varepsilon}.\n\\] Now lets use these variance components to calculate the 90% credibility (prediction) interval and the 95% confidence interval. The 90% credibility interval can be calculated with the following equations:\n\\[\n\\rho_\\text{Upper} = \\hat{\\bar{\\rho}} + 1.645\\sigma_\\rho\n\\]\n\\[\n\\rho_\\text{Lower} = \\hat{\\bar{\\rho}} - 1.645\\sigma_\\rho\n\\]\nWe can also calculate the standard error of the mean of true population effect sizes (\\(SE_{\\hat{\\bar{\\rho}}}\\)) by dividing the sampling error variance component by the number of studies, \\(k\\),\n\\[\nSE_{\\hat{\\bar{\\rho}}} = \\sqrt{\\frac{\\sigma^2_{r_c}}{k}}\n\\]\nWhich can then be used to calculate 95% confidence intervals:\n\\[\n\\bar{\\rho}_\\text{Upper} = \\hat{\\bar{\\rho}} + 1.96\\cdot SE_{\\hat{\\bar{\\rho}}}\n\\]\n\\[\n\\bar{\\rho}_\\text{Lower} = \\hat{\\bar{\\rho}} - 1.96\\cdot SE_{\\hat{\\bar{\\rho}}}\n\\]\n\n\n12.3.2 Applied Example in R\nLets conduct an individual correction meta-analysis in r using the data set by Roth (2015). This data set consists of correlations between school grades and intelligence test scores. It also contains information on the reliability of the intelligence test scores and the extent of range restriction in test scores. We can conduct a meta-analysis correcting for univariate indirect range restriction and measurement error in test scores. The compound artifact biasing factor for the correlation would be:\n\\[\nA_i=\\sqrt{r_{o_i}^2 + \\frac{u_{x_i}^2 r_{xx'_i}(r_{xx'_i} - r_{o_i}^2) }{1 - u_{x_i}^2 (1-r_{xx'_i})} }\n\\]\nSticking with our theme of doing everything in base R first, lets use the equations from the previous section to conduct the meta-analysis.\n\n# Load in packages (we need the development version of psychmeta)\n# install.packages(\"devtools\")\n# devtools::install_github(\"psychmeta/psychmeta\")\nlibrary(psychmeta)\n\n# obtain artifact values\nrxx &lt;- data_r_roth_2015$rxxi\nux &lt;- data_r_roth_2015$ux\nro &lt;- data_r_roth_2015$rxyi\nn &lt;- data_r_roth_2015$n\nk &lt;- length(ro)\n\n# fill in missing artifact values with mean\nrxx[is.na(rxx)] &lt;- mean(rxx,na.rm=TRUE)\nux[is.na(ux)] &lt;- mean(ux,na.rm=TRUE)\n\n# calculate compound artifact biasing factor for univariate direct range restriction with measurement error\nA &lt;- sqrt(ro^2 + (ux^2*rxx*(rxx - ro^2)) / (1 - ux^2*(1-rxx)))\n\n# calculate the sample size weighted average of r\nro_bar &lt;- sum(ro*n) / sum(n)\n\n# calculate the observed sampling variance for each study\nvar_eoi &lt;- (1-ro_bar^2)^2 / (n-1)\n\n# correct sampling variance\nvar_eci &lt;- var_eoi / A^2\n\n# calculate corrected correlations\nrc &lt;- ro / A\n\n# calculate weights\nw &lt;- 1/var_eci\n\n# calculate population effect size estimate\nmean_rho_hat &lt;- sum(rc*w) / sum(w)\n\n# calculate the variance in corrected correlations (rc)\nvar_rc &lt;- sum(w*(rc - mean_rho_hat)^2) / sum(w)\n\n# calculate average corrected sampling variance\nvar_ec &lt;- sum(var_eci*w) / sum(w)\n\n# calculate the variance in true population correlations\nvar_rho &lt;- var_rc - var_ec\n\n# calculate standard error of rho estimate\nSE_rho = sqrt(var_rc/k)\n\n# print results\ndata.frame(k = k,\n           n = sum(n),\n           mean_rho_hat,\n           SE = SE_rho,\n           SD_rho = sqrt(var_rho))\n\n    k      n mean_rho_hat         SE    SD_rho\n1 240 105151    0.5398838 0.01339916 0.2022865\n\n\nThe estimated mean correlation of .540 is precisely what is precisely what the original paper reported (Roth 2015). Lets conduct the meta-analysis using the the psychmeta package (Dahlke and Wiernik 2019). The function ma_r_ic is designed to conduct an individual correction meta-analysis on correlation coefficients.\n\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\n# conduct individual correction meta-analysis\nmdl_ic &lt;- ma_r_ic(rxyi = ro, n = n,\n            correction_method = \"uvirr\",\n            rxx = rxx,\n            ux = ux,\n            ux_observed = TRUE,\n            rxx_restricted = TRUE)\n\nsummary_stats &lt;- data.frame(k = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$k,\n                            n = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$N,\n                            mean_rho = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$mean_rho,\n                            SE = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$se_r_c,\n                            SD_rho = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$sd_rho)\nsummary_stats \n\n    k      n  mean_rho        SE    SD_rho\n1 240 105151 0.5404134 0.0134356 0.2036946\n\n\nWe can also obtain credibility intervals by using the credibility function in the psychmeta package. The interval defaults to 80% intervals, however we can change that to 90% by inputting .90 into the cred_level argument.\n\ncredibility(mean = summary_stats$mean_rho_hat,\n            sd = summary_stats$SD_rho,\n            cred_method = \"norm\",\n            cred_level = .90)\n\n     CR_LL_90 CR_UL_90\n\n\nLets compare these results to the bare-bones model. In psychmeta the bare-bones model can be conduced using ma_r_bb. However, the ma_r_ic function also reports the bare-bones results as well. Therefore we can just extract the necessary statistics from the model.\n\ndata.frame(\n  k = mdl_ic$meta_tables$`analysis_id: 1`$barebones$k,\n  n = mdl_ic$meta_tables$`analysis_id: 1`$barebones$N,\n  mean_rho_obs = mdl_ic$meta_tables$`analysis_id: 1`$barebones$mean_r,\n  SE = mdl_ic$meta_tables$`analysis_id: 1`$barebones$se_r,\n  SD_rho_obs = mdl_ic$meta_tables$`analysis_id: 1`$barebones$sd_r)\n\n    k      n mean_rho_obs         SE SD_rho_obs\n1 240 105151    0.4418789 0.01191933  0.1846534\n\n\nWe can see that the estimate of the population correlation is largely attenuated in the observed values. This is due to the fact tests of intelligence are not perfectly reliable and the scores were restricted in their range."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#artifact-distribution-model",
    "href": "artifact_correction_metaanalysis.html#artifact-distribution-model",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.4 Artifact Distribution Model",
    "text": "12.4 Artifact Distribution Model\nWhen we observe a lot of missingness in artifact values (e.g., studies not reporting reliability), we may choose to use an artifact distribution model. The artifact distribution model conducts a meta-analysis on the observed effect sizes and artifact values separately, and then uses the aggregate artifact values to correct for the observed mean effect size. Since the artifact distribution method uses Taylor series approximations (Dahlke and Wiernik 2020) that are custom-tailored to estimate the sampling variance of corrected correlations, we will skip the general case to focus on its application to correlations.\n\n12.4.1 The Correlational Case\nThe model here can be broken down into two parts, the first part aggregates the observed effect sizes and the second part aggregates the artifact values. The artifact values we will focus on here are the reliability coefficients (see chapter 5 and 6), however other artifact values like \\(u\\)-ratios will follow similar procedures. We can start with the bare-bones meta-analysis model: \\(r_{o_i} = \\rho_{o_i} + \\varepsilon_{o_i}\\). We can estimate the observed population correlation (\\(\\vartheta_{o_i}\\)) by first calculating the weights (using the \\(n\\)-weighted mean correlation in the formula for sampling variance, \\(\\bar{r}\\)):\n\\[\n\\sigma^2_{\\varepsilon_oi} \\approx \\frac{(1-\\bar{r}^2)^2}{n_i-1}\n\\]\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\sigma_\\vartheta^2} = \\frac{1}{\\sigma^2_{\\varepsilon_i}+(\\sigma^2_{\\theta}-\\sigma^2_{\\varepsilon})} \\approx \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\left(\\frac{\\sum^k_{i=1}n_i(\\theta_i - \\bar{\\theta})^2}{\\sum^k_{i=1}n_i}-\\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_oi}}{\\sum^k_{i=1}n_i}\\right)}\n\\]\nTaking the mean of the observed study correlations weighted by the inverse sampling variance,\n\\[\n\\hat{\\bar{\\rho}}_o=\\frac{\\sum^k_{i=1}w_i r_{o_i}}{\\sum^k_{i=1}w_i}\n\\]\nThen lets get the variance in observed population correlations, in order to do this we need the v\n\\[\n\\sigma^2_{\\rho_o}=\\sigma^2_{r_o} - \\sigma^2_{\\varepsilon_o} = \\frac{\\sum^k_{i=1}w_i (r_{o_i}-\\hat{\\bar{\\rho}}_o)^2}{\\sum^k_{i=1}w_i} - \\frac{\\sum^k_{i=1}w_i \\sigma^2_{\\varepsilon_oi}}{\\sum^k_{i=1}w_i}\n\\]\nWith the weights we can also take the weighted average of the artifact values (such as \\(u\\)-ratios or reliabilities) that are available. For our example here, we will correct only for measurement error, therefore the weighted means for reliability in \\(x\\) and \\(y\\) will be:\n\\[\n\\bar{r}_{xx'}=\\frac{\\sum^k_{i=1}w_i r_{xx'_i}}{\\sum^k_{i=1}w_i}\n\\]\n\\[\n\\bar{r}_{yy'}=\\frac{\\sum^k_{i=1}w_i r_{yy'_i}}{\\sum^k_{i=1}w_i}\n\\]\nNow recall from chapter 5 that the square root of the reliability is equal to the correlation between observed scores and true scores. We can denote the mean correlation as follows: \\(\\bar{r}_{xT}=\\sqrt{\\bar{r}_{xx'}}\\) and \\(\\bar{r}_{yU}=\\sqrt{\\bar{r}_{xx'}}\\). We then must also compute the average sampling variances of \\(r_{xT_i}\\) and \\(r_{yU_i}\\) between studies. These sampling variance of these correlations can be computed the same way as a pearson correlation:\n\\[\n\\sigma^2_{r_{xT}i} \\approx \\frac{(1-\\bar{r}_{xT}^2)^2}{n_i-1}\n\\]\n\\[\n\\sigma^2_{r_{yU}i} \\approx \\frac{(1-\\bar{r}_{yU}^2)^2}{n_i-1}\n\\]\nThen weighted average of these sampling variances is\n\\[\n\\sigma^2_{r_{xT}} = \\frac{\\sum^k_{i=1}w_i r_{xT_i}}{\\sum^k_{i=1}w_i}\n\\]\n\\[\n\\sigma^2_{r_{yU}} = \\frac{\\sum^k_{i=1}w_i r_{yU_i}}{\\sum^k_{i=1}w_i}\n\\]\nNow that we have the point-estimate of the population observed correlation, the variance of observed population correlations, the sampling variance of observed correlations, and the sampling variance of the square root of the reliability for \\(x\\) and \\(y\\), we can now attempt to correct the point-estimate and the variance of population correlations.\n\nCorrecting Using Summary Values\nFirst, we can start by correcting the overall point-estimate for the observed population correlation in order to remove bias due to measurement error. Recall from chapter 5 the correction formula:\n\\[\n\\hat{\\bar{\\rho}} = \\frac{\\hat{\\bar{\\rho}}_o}{\\bar{A}} = \\frac{\\hat{\\bar{\\rho}}_o}{\\bar{r}_{xT} \\bar{r}_{yU}} = \\frac{\\hat{\\bar{\\rho}}_o}{\\sqrt{\\bar{r}_{xx'}} \\sqrt{\\bar{r}_{yy'}}}\n\\]\nNote that the artifact biasing factor, \\(A\\), is the product of the two sources of attenuation. Correcting the variance in observed population correlations (\\(\\sigma^2_{\\rho_o}\\)), so that it is accurately estimating the variance of true population effect sizes (\\(\\sigma^2_{\\rho}\\)), we must use a Taylor series approximation. This formula can become fairly complex with more types of artifacts involved. The taylor series approximation is for estimating specifically the amount of sampling variance within the correction factor we apply to the observed correlation. The first step is lay out our attenuation formula (the equation where observed effect size is on the left side of the equation and the artifact values and true effect size is on the right hand side of the equation). In the case of correcting only for measurement error, the attenuation formula is relatively simple\n\\[\n\\hat{\\bar{\\rho}}_o = \\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\\cdot \\bar{r}_{yU}\n\\]\nFor the taylor series approximation, we want to first find the partial derivitive with respect to each artifact component:\n\\[\nB_{r_{xT}}=\\frac{\\partial}{\\partial r_{xT}} (\\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\\cdot \\bar{r}_{yU}) = \\hat{\\bar{\\rho}}\\cdot \\bar{r}_{yU}\n\\] \\[\nB_{r_{yU}}=\\frac{\\partial}{\\partial r_{yU}} (\\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\\cdot \\bar{r}_{yU}) = \\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\n\\]\nThe variance due to artifacts is then approximately,\n\\[\n\\sigma^2_A\\approx B^2_{r_{xT}} \\sigma^2_{r_{xT}} + B^2_{r_{yU}} \\sigma^2_{r_{yU}}\n\\]\nNow we can approximate the variance in true population correlations,\n\\[\n\\sigma_\\rho^2= \\frac{\\sigma^2_{\\rho_o} - \\sigma^2_A}{\\bar{A}^2}\n\\]\nWhere the artifact biasing factor is: \\(\\bar{A}=\\bar{r}_{xT}\\cdot \\bar{r}_{yU}\\). See the supplementary materials of Dahlke and Wiernik (2020) for detailed Taylor series approximation derivations for the immensely more complicated bivariate indirect range restriction plus measurement error correction.\n\n\n\n12.4.2 Applied Example in R\nLets conduct an artifact distribution correction meta-analysis in R, instead using data from the meta-analysis by McDaniel et al. (1994). This dataset contains correlations between employment interviews and job performance. This data set has a lot of missing values for reliability coefficients and \\(u\\)-ratios which might suggest that the artifact distribution approach is a better choice compared to the individual correction approach. We can conduct a meta-analysis correcting for univariate indirect range restriction and measurement error in both job performance and employment interviews. The attenuation formula will be important for calculating the Taylro series approximation can be defined as\n\\[\n\\bar{\\rho}_o=\\bar{\\rho}\\sqrt{\\bar{r}_{o_i}^2 + \\frac{\\bar{u}_{x_i}^2 \\bar{r}_{xx'_i}(\\bar{r}_{xx'_i}\\bar{r}_{yy'_i} - \\bar{r}_{o_i}^2) }{1 - \\bar{u}_{x_i}^2 (1-\\bar{r}_{xx'_i})} }\n\\] Instead of conducting a taylor series approximation by hand, we will simply use the psychmeta package to perform the artifact distribution meta-analysis. The function ma_r_ad is designed to conduct an artifact distribution meta-analysis on correlation coefficients. The function also reports the bare-bones model allowing us to compare the corrected estimates to the uncorrected.\n\n# Load in packages (we need the development version of psychmeta)\n# install.packages(\"devtools\")\n# devtools::install_github(\"psychmeta/psychmeta\")\nlibrary(psychmeta)\n\n# obtain artifact values\nrxx &lt;- data_r_roth_2015$rxxi\nux &lt;- data_r_roth_2015$ux\nro &lt;- data_r_roth_2015$rxyi\nn &lt;- data_r_roth_2015$n\nk &lt;- length(ro)\n\n# compute barebones meta-analysis\nma_obj &lt;- ma_r_bb(r = rxyi, \n                  n = n, \n                  correct_bias = FALSE, \n                  wt_type = \"REML\",\n                  data = data_r_mcdaniel_1994)\n\n# construct artifact distribution for x\nad_obj_x &lt;- create_ad(ad_type = \"tsa\", \n                      mean_rxxi = data_r_mcdaniel_1994$Mrxxi[1],\n                      var_rxxi = data_r_mcdaniel_1994$SDrxxi[1]^2,\n                      ux = data_r_mcdaniel_1994$ux,\n                      wt_ux = data_r_mcdaniel_1994$`ux frequency`)\n\n# construct artifact distribution for y\nad_obj_y &lt;- create_ad(ad_type = \"tsa\", \n                      rxxi = data_r_mcdaniel_1994$ryyi,\n                      wt_rxxi = data_r_mcdaniel_1994$`ryyi frequency`)\n\n# compute artifact-distribution meta-analysis, correcting for measurement error only\nmdl_ad &lt;- ma_r_ad(ma_obj = ma_obj, \n                  ad_obj_x = ad_obj_x, \n                  ad_obj_y = ad_obj_y, \n                  correction_method = \"meas\")\n\n\n# summary table of meta-analysis\nsummary_stats &lt;- data.frame(\n  type = c('Artifact Distribution', 'Bare-Bones'),\n  k = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$k,mdl_ad$meta_tables$`analysis_id: 1`$barebones$k),\n  n = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$N,mdl_ad$meta_tables$`analysis_id: 1`$barebones$N),\n  mean_rho = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$mean_rho,mdl_ad$meta_tables$`analysis_id: 1`$barebones$mean_r),\n  SE = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$se_r_c,mdl_ad$meta_tables$`analysis_id: 1`$barebones$se_r),\n  SD_rho = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$sd_rho,0))\n\nsummary_stats \n\n                   type   k     n  mean_rho         SE    SD_rho\n1 Artifact Distribution 160 25244 0.3201828 0.02108407 0.1985571\n2            Bare-Bones 160 25244 0.2205043 0.01452023 0.0000000\n\n\nWe can also obtain credibility intervals by using the credibility function in the psychmeta package. The interval defaults to 80% intervals, however we can change that to 90% by inputting .90 into the cred_level argument.\n\ncredibility(mean = summary_stats$mean_rho[1],\n            sd = summary_stats$SD_rho[1],\n            cred_method = \"norm\",\n            cred_level = .90)\n\n         CR_LL_90  CR_UL_90\n[1,] -0.006414571 0.6467802\n\n\nLets compare these results to the bare-bones model. In psychmeta the bare-bones model can be conduced using ma_r_bb. However, the ma_r_ic function also reports the bare-bones results as well. Therefore we can just extract the necessary statistics from the model.\n\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\n———. 2020. “Not Restricted to Selection Research: Accounting for Indirect Range Restriction in Organizational Research.” Organizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of meta-analysis: correcting error and bias in research findings (third). Third. Thousand Oaks, California: Sage Publications.\n\n\nMcDaniel, Michael A., Deborah L. Whetzel, Frank L. Schmidt, and Steven D. Maurer. 1994. “The Validity of Employment Interviews: A Comprehensive Review and Meta-Analysis.” Journal of Applied Psychology 79 (4): 599–616. https://doi.org/10.1037/0021-9010.79.4.599.\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A Meta-Analysis.”\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General Solution to the Problem of Validity Generalization.” Journal of Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "13  Conclusion",
    "section": "",
    "text": "Future Directions\nThis book is a living resource and it will add new content with each version. Versions will be released in the Github repository as on the Open Science Framework (DOI here when created). In future releases, there are a number of features that are planned to be added. Here is a list of those features:\n\nInteractive visualizations of artifacts and bias using RShinyLive\nMore effect size types with corrections: Repeated measures SMDs, Variability ratios, and unstandardized effect sizes (mean differences and regression coefficients).\nA chapter on the artifact of treatment non-compliance.\nZ-transformations for corrected correlations.\nPractice problems.\nSmall simulations for assessing robustness of corrections in non-normal distributions (e.g., heavy-tails, skew)\n\nSuggestions for added features are welcome. Suggestions can be made by submitting an issue to the Github repository.\n\n\nLimitations\nApplying artifact corrections has trade-offs that should be considered before putting them into use. Obtaining more accurate estimates of our target effect sizes is the obvious major benefit of applying corrections, however it is also true that these artifact corrections can impose assumptions that may not be met in practice. The assumptions were laid out in each of the chapters, but it is important for the researcher to consider how reasonable these assumptions are for their research context. If the assumption violations are extreme, than the correction can potentially over- or under-correct the effect size estimates.\nDefining the scientific estimand is the most important step before proceeding with artifact corrections. Each correction procedure is meant to estimate very specific quantity. If a researcher’s target quantity is different from the one that the correction procedure is trying to estimate, then the correction may further bias the effect size. A good example of this is in college admissions testing, where it can be quite tempting to correct the correlation between test scores and college grade-point average (GPA) for measurement error in the test scores. However, the practical utility of test scores is limited to the prediction capability of observed test scores since admission committees do not have access to true scores. Therefore if our quantity of interest is the on-the-ground predictive utility of test scores on GPA, then correcting the correlation for measurement errors in test scores will over-estimate the predictive utility. Researchers should ensure that the ultimate research goals align with the correction procedure.\n\n\nConcluding Remarks\nImperfections in studies can obscure our view of reality, inhibit us from making sense of our observations, and they can mislead us. Corrections to these imperfections can allow us to see what findings may look like in an ideal study with perfect conditions. Meta-analyses will inevitably incorporate imperfect studies that vary in design and methodology that may restrict our ability to estimate a specific estimand. However, this should not limit the goal of meta-analysis to be merely a description of the literature. Corrections can allow meta-analysts to estimate specific scientifically-relevant quantities. Donald Rubin (1992) discusses the importance of estimating effect sizes in theoretically perfect studies:\n\nLiterature synthesis is fine, but, before scientifically valid statistical inference can take place, scientifically relevant quantities (i.e., estimands) must be defined, and the population estimands in the traditional view may be of limited scientific interest. The scientifically most interesting estimands involve the hypothetical results of technically perfect studies, rather than the average of results from some population of fallible studies.\n\nMost artifact correction meta-analyses have been conducted almost exclusively in industrial and organizational psychology (e.g., personnel selection) and education assessment research (e.g., college admissions testing). However, the concepts in this book apply to many fields including biomedical, social science, clinical research, among others. No discipline is immune to measurement error and selection effects, therefore it is important to address these artifacts in every field of research.\nThis book could not have been accomplished without the pioneering work of John Hunter and Frank Schmidt. This book stands as a testament to their foundational contributions to artifact corrections and meta-analysis. The goal of this current book was to expand the methodologies and applications presented in their book (Hunter and Schmidt 2015). It also aims to provide a comprehensive guide, drawing upon previous scholarship while adapting to the evolving landscape of research methodologies.\nI hope reading this book will provide invaluable insights and tools that empower people to address the bias in our studies. May it encourage a deeper awareness of these issues when confronted with them.\nWe want our research findings to accurately describe reality. Artifact corrections help us get closer to that goal.\n\n\nAcknowledgements\nThank you to Dr. Blair T. Johnson, Dr. Christopher Rhoads, and Dr. Elizabeth Schifano for taking the time to review and provide extremely valuable feedback over the course of writing this book.\n\nMatthew B. Jané\n @MatthewBJane\n\n\n\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of meta-analysis: correcting error and bias in research findings (third). Third. Thousand Oaks, California: Sage Publications.\n\n\nRubin, Donald B. 1992. “Meta-Analysis: Literature Synthesis or Effect-Size Surface Estimation?” Journal of Educational Statistics 17 (4): 363–74. https://doi.org/10.3102/10769986017004363."
  },
  {
    "objectID": "references_page.html",
    "href": "references_page.html",
    "title": "References",
    "section": "",
    "text": "Aguinis, Herman, Charles A Pierce, and Steven A Culpepper. 2009.\n“Scale Coarseness as a Methodological Artifact,” September.\n\n\nBangert-Drowns, Robert L, Marlene M Hurley, and Barbara Wilkinson. 2004.\n“The Effects of School-Based Writing-to-Learn Interventions on\nAcademic Achievement: A Meta-Analysis.” Review of Educational\nResearch 74 (1): 29–58. https://doi.org/10.3102/00346543074001029.\n\n\nBarraza, Felipe, Marcelo Arancibia, Eva Madrid, and Cristian Papuzinski.\n2019. “General Concepts in Biostatistics and Clinical\nEpidemiology: Random Error and Systematic Error.”\nMedwave 19 (7): e7687. https://doi.org/10.5867/medwave.2019.07.7687.\n\n\nBobko, Philip, and Angela Rieck. 1980. “Large Sample Estimators\nfor Standard Errors of Functions of Correlation Coefficients.”\nApplied Psychological Measurement 4 (3): 385–98. https://doi.org/10.1177/014662168000400309.\n\n\nBonett, Douglas G., and Robert M. Price. 2005. “Inferential\nMethods for the Tetrachoric Correlation Coefficient.” Journal\nof Educational and Behavioral Statistics 30 (2): 213–25. https://www.jstor.org/stable/3701350.\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah\nR. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and\nRandom-Effects Models for Meta-Analysis.” Research Synthesis\nMethods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nBorsboom, Denny, and Gideon J Mellenbergh. 2002. “True Scores,\nLatent Variables, and Constructs: A Comment on Schmidt and\nHunter.”\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap Van Heerden. 2004.\n“The Concept of Validity.” Psychological Review\n111 (4): 1061–71. https://doi.org/10.1037/0033-295X.111.4.1061.\n\n\nBrannick, Michael T., Sean M. Potter, Bryan Benitez, and Scott B.\nMorris. 2019. “Bias and Precision of Alternate Estimators in\nMeta-Analysis: Benefits of Blending Schmidt-Hunter and Hedges\nApproaches.” Organizational Research Methods 22 (2):\n490–514. https://doi.org/10.1177/1094428117741966.\n\n\nBravais, A. 1844. Analyse mathématique sur les probabilités des\nerreurs de situation d’un point. Impr. Royale.\n\n\nBrown, William. 1910. “Some Experimental Results in the\nCorrelation of Mental Abilities1.” British Journal of\nPsychology, 1904-1920 3 (3): 296–322. https://doi.org/10.1111/j.2044-8295.1910.tb00207.x.\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test\nof a New Model for Validity Generalization.” Journal of\nApplied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. Academic Press.\n\n\n———. 2013. Statistical Power Analysis for the Behavioral\nSciences. Academic Press.\n\n\nCooper, Harris M., Larry V. Hedges, and Jeff C. Valentine, eds. 2009.\nThe Handbook of Research Synthesis and Meta-Analysis. 2nd ed.\nNew York: Russell Sage Foundation.\n\n\nCox, D. R. 1989. Analysis of Binary Data. 2nd ed. New York:\nRoutledge. https://doi.org/10.1201/9781315137391.\n\n\nCronbach, Lee J. 1951. “Coefficient Alpha and the Internal\nStructure of Tests.” Psychometrika 16 (3): 297–334. https://doi.org/10.1007/BF02310555.\n\n\nCronbach, Lee J., and Paul E. Meehl. 1955. “Construct Validity in\nPsychological Tests.” Psychological Bulletin 52 (4):\n281–302. https://doi.org/10.1037/h0040957.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R\nPackage for Psychometric Meta-Analysis.” Applied\nPsychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\n———. 2020. “Not Restricted to Selection Research: Accounting for\nIndirect Range Restriction in Organizational Research.”\nOrganizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects\nModel for Meta-Analysis of Clinical Trials: An Update.”\nNIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75\n(1949): 450–51. https://doi.org/10.1038/075450a0.\n\n\nGamer, Matthias, Jim Lemon, Ian Fellows, and Puspendra Singh. 2019.\nIrr: Various Coefficients of Interrater Reliability and\nAgreement. https://CRAN.R-project.org/package=irr.\n\n\nGoulet-Pelletier, Jean-Christophe, and Denis Cousineau. 2018. “A\nReview of Effect Sizes and Their Confidence Intervals, Part i: The\nCohen’s d Family.” The Quantitative Methods for\nPsychology 14 (4): 242–65. https://doi.org/10.20982/tqmp.14.4.p242.\n\n\nHaddock, C. Keith, David Rindskopf, and William R. Shadish. 1998.\n“Using Odds Ratios as Effect Sizes for Meta-Analysis of\nDichotomous Data: A Primer on Methods and Issues.”\nPsychological Methods 3 (3): 339–53. https://doi.org/10.1037/1082-989X.3.3.339.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator\nof Effect Size and Related Estimators.” Journal of\nEducational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\n———. 1989. “An Unbiased Correction for Sampling Error in Validity\nGeneralization Studies.” Journal of Applied Psychology\n74 (3): 469–77. https://doi.org/10.1037/0021-9010.74.3.469.\n\n\nHedges, Larry V., and Ingram Olkin. 1985. Statistical Methods for\nMeta-Analysis. Academic press. https://books.google.com/books?hl=en&lr=&id=7GviBQAAQBAJ&oi=fnd&pg=PP1&dq=info:e6P1zfh2T6QJ:scholar.google.com&ots=Dx-YqN6_9B&sig=-39HgbYdWPp_BwSTzA9cRODs2Q0.\n\n\nHedges, Larry V., and Jack L. Vevea. 1998. “Fixed- and\nRandom-Effects Models in Meta-Analysis.” Psychological\nMethods 3 (4): 486–504. https://doi.org/10.1037/1082-989X.3.4.486.\n\n\nHoltzman, Wayne H. 1950. “The Unbiased Estimate of the Population\nVariance and Standard Deviation.” The American Journal of\nPsychology 63 (4): 615–17. https://doi.org/10.2307/1418879.\n\n\nHunter, John E., and Frank L. Schmidt. 2015. Methods of\nmeta-analysis: correcting error and bias in research findings\n(third). Third. Thousand Oaks, California: Sage Publications.\n\n\nHunter, John E., Frank L. Schmidt, and Huy Le. 2006. “Implications\nof Direct and Indirect Range Restriction for Meta-Analysis Methods and\nFindings.” Journal of Applied Psychology 91 (3):\n594–612. https://doi.org/10.1037/0021-9010.91.3.594.\n\n\nHunter, John, and Frank Schmidt. 1990. “Dichotomization of\nContinuous Variables: The Implications for Meta-Analysis.”\nJournal of Applied Psychology 75 (June): 334–49. https://doi.org/10.1037/0021-9010.75.3.334.\n\n\nJacobs, Perke, and Wolfgang Viechtbauer. 2017. “Estimation of the\nBiserial Correlation and Its Sampling Variance for Use in\nMeta-Analysis.” Research Synthesis Methods 8 (2):\n161–80. https://doi.org/10.1002/jrsm.1218.\n\n\nJohnson, Blair T., Brian Mullen, and Eduardo Salas. 1995.\n“Comparison of Three Major Meta-Analytic Approaches.”\nJournal of Applied Psychology 80 (1): 94–106. https://doi.org/10.1037/0021-9010.80.1.94.\n\n\nKelley, Truman Lee. 1927. Interpretation of Educational\nMeasurements. World Book Company.\n\n\nKempen, G.m.p. van, and L.j. van Vliet. 2000. “Mean and Variance\nof Ratio Estimators Used in Fluorescence Ratio Imaging.”\nCytometry 39 (4): 300–305. https://doi.org/10.1002/(SICI)1097-0320(20000401)39:4&lt;300::AID-CYTO8&gt;3.0.CO;2-O.\n\n\nKirk, David B. 1973. “On the Numerical Approximation of the\nBivariate Normal (Tetrachoric) Correlation Coefficient.”\nPsychometrika 38 (2): 259–68. https://doi.org/10.1007/BF02291118.\n\n\nKroenke, Kurt, Robert L. Spitzer, and Janet B. W. Williams. 2003.\n“The Patient Health Questionnaire-2: Validity of a Two-Item\nDepression Screener.” Medical Care 41 (11): 1284–92. https://www.jstor.org/stable/3768417.\n\n\nLaird, Nan M., and Frederick Mosteller. 1990. “Some Statistical\nMethods for Combining Experimental Results.” International\nJournal of Technology Assessment in Health Care 6 (1): 5–30. https://doi.org/10.1017/S0266462300008916.\n\n\nLe, Huy, and Frank L. Schmidt. 2006. “Correcting for Indirect\nRange Restriction in Meta-Analysis: Testing a New Meta-Analytic\nProcedure.” Psychological Methods 11 (4): 416–38. https://doi.org/10.1037/1082-989X.11.4.416.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021.\n“What Is Your Estimand? Defining the Target Quantity Connects\nStatistical Evidence to Theory.” American Sociological\nReview 86 (3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nMacCallum, Robert C., Shaobo Zhang, Kristopher J. Preacher, and Derek D.\nRucker. 2002. “On the Practice of Dichotomization of Quantitative\nVariables.” Psychological Methods 7: 19–40. https://doi.org/10.1037/1082-989X.7.1.19.\n\n\nMcDaniel, Michael A., Deborah L. Whetzel, Frank L. Schmidt, and Steven\nD. Maurer. 1994. “The Validity of Employment Interviews: A\nComprehensive Review and Meta-Analysis.” Journal of Applied\nPsychology 79 (4): 599–616. https://doi.org/10.1037/0021-9010.79.4.599.\n\n\nMorris, Scott, Rebecca Daisley, Megan Wheeler, and Peggy Boyer. 2014.\n“A Meta-Analysis of the Relationship Between Individual\nAssessments and Job Performance.” The Journal of Applied\nPsychology 100 (May). https://doi.org/10.1037/a0036938.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of\nCertain Correlation Coefficients.” The Annals of Mathematical\nStatistics 29 (1): 201–11. https://www.jstor.org/stable/2237306.\n\n\nPearson, Karl. 1903. “I. Mathematical Contributions to the Theory\nof Evolution. XI. On the Influence of Natural Selection on\nthe Variability and Correlation of Organs.” Philosophical\nTransactions of the Royal Society of London. Series A, Containing Papers\nof a Mathematical or Physical Character 200 (321-330): 1–66. https://doi.org/10.1098/rsta.1903.0001.\n\n\nPearson, Karl, and L. N. G. Filon. 1898. “Mathematical\nContributions to the Theory of Evolution. IV. On the Probable Errors of\nFrequency Constants and on the Influence of Random Selection on\nVariation and Correlation. [Abstract].” Proceedings of the\nRoyal Society of London 62: 173–76. https://www.jstor.org/stable/115709.\n\n\nPeters, Charles C., and Walter R. Van Voorhis. 1940. “Further\nMethods of Correlation.” In, 362–403. New York, NY, US:\nMcGraw-Hill Book Company. https://doi.org/10.1037/13596-013.\n\n\nRaju, Nambury, Michael Burke, Jacques Normand, and George Langlois.\n1991. “A New Meta-Analytic Approach.” Journal of\nApplied Psychology 76 (June): 432–46. https://doi.org/10.1037/0021-9010.76.3.432.\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A\nMeta-Analysis.”\n\n\nRubin, Donald B. 1992. “Meta-Analysis: Literature Synthesis or\nEffect-Size Surface Estimation?” Journal of Educational\nStatistics 17 (4): 363–74. https://doi.org/10.3102/10769986017004363.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range\nRestriction: An Expanded Typology.” Journal of Applied\nPsychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General\nSolution to the Problem of Validity Generalization.” Journal\nof Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nSoper, H. E. 1914. “On the Probable Error of the Bi-Serial\nExpression for the Correlation Coefficient.” Biometrika\n10 (2/3): 384–90. https://doi.org/10.2307/2331789.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association\nBetween Two Things.” International Journal of\nEpidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nSpearman, Charles. 1910. “Correlation Calculated from Faulty\nData.” British Journal of Psychology 3 (3): 271295. https://www.proquest.com/docview/1293688112/citation/7E133DC1091D4E47PQ/1.\n\n\nTaboga, Marco. 2021. “Gamma Function.” https://www.statlect.com/mathematical-tools/gamma-function.\n\n\nTaylor, Erwin K., and Thomas Griess. 1976. “The Missing Middle in\nValidation Research.” Personnel Psychology 29 (1): 5–11.\nhttps://doi.org/10.1111/j.1744-6570.1976.tb00397.x.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting meta-analyses in R with\nthe metafor package.” Journal of Statistical Software 36\n(3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\n———. n.d. “Fixed-Effects and Random-Effects Models in\nMeta-Analysis.” https://wviechtb.github.io/metafor/index.html.\n\n\nVos, Paul, and Don Holbert. 2022. “Frequentist Statistical\nInference Without Repeated Sampling.” Synthese 200 (2):\n89. https://doi.org/10.1007/s11229-022-03560-x.\n\n\nWechsler, David. 2008. Wechsler Adult Intelligence Scale–Fourth\nEdition. 4th ed. https://doi.org/10.1037/t15169-000.\n\n\nWhite, Thomas, Daniel Noble, Alistair Senior, W. Kyle Hamilton, and\nWolfgang Viechtbauer. 2022. Metadat: Meta-Analysis Datasets. https://CRAN.R-project.org/package=metadat.\n\n\nWhitener, Ellen M. 1990. “Confusion of Confidence Intervals and\nCredibility Intervals in Meta-Analysis.” Journal of Applied\nPsychology 75 (3): 315–21. https://doi.org/10.1037/0021-9010.75.3.315.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining\nUnbiased Results in Meta-Analysis: The Importance of Correcting for\nStatistical Artifacts.” Advances in Methods and Practices in\nPsychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611.\n\n\nWilliam Revelle. 2023. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University. https://CRAN.R-project.org/package=psych.\n\n\nWylie, Peter B. 1976. “Effects of Coarse Grouping and Skewed\nMarginal Distributions on the Pearson Product Moment Correlation\nCoefficient.” Educational and Psychological Measurement\n36 (1): 1–7. https://doi.org/10.1177/001316447603600101."
  }
]