[
  {
    "objectID": "artificial_dichotomization.html#introduction",
    "href": "artificial_dichotomization.html#introduction",
    "title": "7  Artificial Dichotomization",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nResearchers occasionally split naturally continuous variables into two discrete groups to increase interpretability or conduct specific analyses (e.g., t-tests). However, artificially dichotomizing variables introduces measurement error variance thus attenuating effect size estimates Maxwell and Delaney (1993). The obvious solution to this problem is to simply not dichotomize variables, however if only summary data is available to us, then we may not have this luxury. Dichotomization can also be practical in some instances. For example, clinical disorder diagnoses such as generalized anxiety disorder, are examples of dichotomization where individuals are separated into either having the disorder or not even though individual differences in anxiety exist as a continuum."
  },
  {
    "objectID": "artificial_dichotomization.html#artificial-dichotomization-induced-measurement-error",
    "href": "artificial_dichotomization.html#artificial-dichotomization-induced-measurement-error",
    "title": "7  Artificial Dichotomization",
    "section": "7.2 Artificial Dichotomization Induced Measurement Error",
    "text": "7.2 Artificial Dichotomization Induced Measurement Error\nVariables that are dichotomized contain measurement error. This can be demonstrated by the simple fact that dichotomized scores are not perfectly correlated with their underlying continuous scores. To demonstrate this, we can draw a sample of scores and then split the data into high and low scorers and then calculate the correlation coefficient between the two (see figure below). It becomes apparent that the dichotomized scores leave a lot of the variation in scores unaccounted for.\n\n\n\n\n\nEven with a perfectly reliable measure, dichotomization will introduce measurement error variance. We can define naturally continuous scores (\\(\\ddagger\\)) that have been artificially dichotomized as,\n\\[\nx_\\ddagger=\n\\begin{cases}\n    1,& \\text{if } x&gt;C_x\\\\\n    0,& \\text{if } x&lt;C_x\n\\end{cases}\n\\]\nWhere \\(C_x\\) is the cut-score on the standard normal distribution (assume \\(x\\) is standardized such that Mean = 0 and Variance = 1). The reliability can be defined as the correlation between dichotomized scores and the underlying continuous scores (\\(r_{x_\\ddagger x}\\))."
  },
  {
    "objectID": "artificial_dichotomization.html#correcting-correlations-for-artificial-dichotomization",
    "href": "artificial_dichotomization.html#correcting-correlations-for-artificial-dichotomization",
    "title": "7  Artificial Dichotomization",
    "section": "7.3 Correcting Correlations for Artificial Dichotomization",
    "text": "7.3 Correcting Correlations for Artificial Dichotomization\n\n7.3.1 Defining our estimand\nUltimately, we would like to know the correlation coefficient between two naturally continuous variables. Sticking with our notation for true scores, our estimand can be defined as the population correlation between continuous observed scores of the independent (\\(x\\)) and dependent variable (\\(y\\)), \\(\\rho_{xy}\\). Where dichotomized scores can be defined as,\n\\[\nx_\\ddagger=\n\\begin{cases}\n    1,& \\text{if } x&gt;C_x\\\\\n    0,& \\text{if } x&lt;C_x\n\\end{cases}\n\\]\n\\[\ny_\\ddagger=\n\\begin{cases}\n    1,& \\text{if } y&gt;C_y\\\\\n    0,& \\text{if } y&lt;C_y\n\\end{cases}.\n\\]\nWhere \\(C_x\\) and \\(C_y\\) is the cut-score where the split took place for each variable. There are two cases of dichotomization that may occur in a given study: the univariate case where only one variable (either dependent or independent) is dichotomized and the bivariate case where both variables are dichotomized. In both cases, dichotomization will have a biasing effect on the observed study correlation coefficient. The observed study correlation can be modeled as a function of the population correlation on continuous scores (\\(\\rho_{xy}\\); i.e., the estimand), an artifact biasing factor \\(a\\), and sampling error, \\(\\varepsilon\\),\n\\[\nr_{x_\\ddagger y_\\ddagger} = a\\rho_{xy}+\\varepsilon.\n\\] Where unbiased estimate of \\(\\rho_{xy}\\) can be calculated by dividing the observed correlation by the biasing factor, \\(a\\),\n\\[\nr_c = \\frac{r_{x_\\ddagger y_\\ddagger}}{a}.\n\\]\n\n\n7.3.2 Artifact Correction for Correlations\n\nThe Univariate Case\nIn the simplest case of dichotomization, only one variable is dichotomized and the other is left continuous. In this case, a Pearson product-moment correlation is equivalent to the point-biserial correlation coefficient, however for dichotomized data, the biserial correlation is a relatively unbiased estimate of the pearson correlation on the underlying continuous data (assuming normality). Therefore in the population, the observed correlation \\(\\rho_{x_\\ddagger y}\\) is biased by some artifact biasing factor, \\(a\\),\n\\[\n\\rho_{x_\\ddagger y} = a\\rho_{xy}.\n\\]\nThe first step in estimating the bias in the correlation is to first identify the cut-point, \\(C_x\\), of the standard normal distribution where the split of the data occurred. This can be calculated by first obtaining the percent of the of the individuals in the low or high scoring group:\n\\[\np_x = \\frac{ n_{\\text{high}} }{n_{\\text{high}} + n_{\\text{low}}}\n\\] or\n\\[\np_x = \\frac{ n_{\\text{low}} }{n_{\\text{high}} + n_{\\text{low}}}.\n\\]\nWhere \\(n\\) indicates the sample size. Then we can use the quantile function (\\(\\phi^{-1}\\); i.e., the inverse of the cumulative density of the standard normal distribution) to obtain the cut-point on the standard normal distribution,\n\\[\nC_x = \\phi^{-1}(p_x).\n\\]\nUsing the cut-point and the proportion of group membership in either the low or high scoring group (\\(p_x\\)), the attenuation factor can be defined as (Hunter and Schmidt 1990),\n\\[\na =\\frac{\\varphi(C_x)}{\\sqrt{p_x(1-p_x)}}.\n\\]\nWhere \\(\\varphi\\) is the normal ordinate function (i.e., probability density function of a standard normal distribution). Since a standard normal distribution is symmetric, the sign of \\(C_x\\) does not matter. In the case of a median split, where the cut-point would be placed at zero of a standard normal (splitting the distribution in equal halves), the attenuation factor would simplify to \\(a =\\frac{\\varphi(0)}{\\sqrt{.5(.5)}}\\) \\(=\\frac{2}{\\sqrt{2\\pi}}\\). To correct the pearson correlation when one of the variables is dichotomized, we can divide the observed correlation by the attenuation factor such that, \\(r_c = \\frac{r_{x_\\ddagger y}}{a}\\). Therefore the full correction equation is,\n\\[\nr_c = \\frac{r_{x_\\ddagger y}}{\\left[\\frac{\\varphi(C_x)}{\\sqrt{p_x(1-p_x)}} \\right]}.\n\\tag{7.1}\\]\nWhere the sampling variance of the corrected correlation (\\(\\sigma^2_{\\varepsilon_c}\\)) must also be adjusted using the compound attenuation factor and the observed sampling variance (\\(\\sigma^2_{\\varepsilon_o}\\)),\n\\[\n\\sigma^2_{\\varepsilon_c} =\\frac{\\sigma^2_{\\varepsilon_o}} {a^2} =\\frac{\\sigma^2_{\\varepsilon_o}} {\\left[\\frac{\\varphi(C_x)}{\\sqrt{p_x(1-p_x)}} \\right]^2}.\n\\]\n\n\n\n\n\n\n\nThe Bivariate Case\nIn some cases, both independent and dependent variables are dichotomized inducing measurement error in both variables. A pearson correlation calculated on these two dichotomized variables would be equal to the phi coefficient (or Matthew’s correlation coefficient) and we can denote it with our notation for dichotomized variables, \\(r_{x_\\ddagger y_\\ddagger}\\). The data can be structured in a contingency table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_\\ddagger=\\text{Low}\\)\n\\(x_\\ddagger=\\text{High}\\)\n\n\n\n\n\\(y_\\ddagger=\\text{Low}\\)\n\\(n_{LL}\\)\n\\(n_{HL}\\)\n\n\n\\(y_\\ddagger=\\text{High}\\)\n\\(n_{LH}\\)\n\\(n_{HH}\\)\n\n\n\n\n\n\n\nWe can also visualize how this contingency table relates to an underlying continuous bivariate normal distribution. The ellipse indicates the correlation between \\(x\\) and \\(y\\). If \\(x\\) and \\(y\\) are positively correlated then we should more individuals populating the high-high and low-low cells rather than the high-low and low-high cells.\n\n\n\n\n\n\n\n\n\nThe proper correction is to calculate the tetrachoric correlation coefficient. The tetrachoric correlation is specifically meant for the relationship between two dichotomous scores that represent continuous underlying normal distribution. To calculate the tetrachoric correlation coefficient, the contingency table or the odds ratio must be available. To estimate the correlation of continuous variables (\\(r_{xy}\\)) we can approximate the tetrachoric correlation with the following formulation,\n\\[\nr_c = \\text{cos}\\left(\\frac{\\pi}{1+\\sqrt{\\frac{n_{HH}n_{LL}}{n_{HL}n_{LH}}}}\\right)\n\\tag{7.2}\\]\nIf the contingency table is not provided, but the odds ratio (\\(OR=\\frac{n_{HH}n_{LL}}{n_{HL}n_{LH}}\\)) is, then we can calculate \\(r_{xy}\\) in terms of the odds ratio,\n\\[\nr_c = \\text{cos}\\left(\\frac{\\pi}{1+\\sqrt{OR}}\\right)\n\\]\nThe sampling variance must be calculated from the contingency table as well. A sampling variance approximation can be obtained from Pearson (1913), however due to the complexity of the formulation and because it is only an approximation, instead I recommend that researchers use a bootstrap procedure to obtain approximate confidence intervals. To do this, we must resample the contingency table (&gt;10,000 iterations), calculating the tetrachoric correlation using Equation 7.2 upon each iteration. Once you obtain a tetrachoric correlation from each iteration, the standard deviation of all correlations can be used as an estimate of the standard error and the square of the standard error is the sampling variance.\nUnfortunately, studies may not report the full contingency table. Instead they may report summary statistics like a chi-squared value or a phi coefficient (i.e., the pearson correlation on binary variables). If the \\(\\chi^2\\)-statistic is reported, we can first convert that to a phi coefficient by using,\n\\[\nr_{x_\\ddagger y_\\ddagger} = \\sqrt{\\frac{\\chi^2}{n}}\n\\]\nWhere \\(n\\) is the total sample size. From the phi coefficient, we can estimate the correlation of the continuous variables with a formula similar to Equation 7.1,\n\\[\nr_c = \\frac{r_{x_\\ddagger y_\\ddagger}}{\\left[\\frac{\\varphi(C_x)}{\\sqrt{p_x(1-p_x)}} \\right]\\left[\\frac{\\varphi(C_y)}{\\sqrt{p_y(1-p_y)}} \\right]}\n\\tag{7.3}\\]\nThis formula was introduced by Hunter and Schmidt (1990) and is a rough approximation of the correlation between the continuous independent and dependent variables (\\(r_{xy}\\)). The corresponding sampling variance of the corrected correlation coefficient is,\n\\[\n\\sigma^2_{\\varepsilon_c} =\\frac{\\sigma^2_{\\varepsilon_o}} {a^2} =\\frac{\\sigma^2_{\\varepsilon_o}} {\\left[\\frac{\\varphi(C_x)}{\\sqrt{p_x(1-p_x)}} \\right]^2\\left[\\frac{\\varphi(C_y)}{\\sqrt{p_y(1-p_y)}} \\right]^2}.\n\\]\n\n\n\n7.3.3 Correcting Correlations for Dichotomization in R\nTo correct correlations for dichotomization in R, we can use the correct_r_dich in the psychmeta package. First let us simulate a data set of 100 individuals that are then dichotomized on both \\(x\\) and \\(y\\). We will use a median (50-50) split for each dichotomization.\n\n# load packages\n# install.packages('psychmeta')\n# install.packages('MASS')\nlibrary(psychmeta)\nlibrary(MASS)\n\n# set seed\nset.seed(343)\n\n# define parameters\nrho = .50\nn &lt;- 500\n\n# simulate data\ndata &lt;- mvrnorm(n = n,\n                mu = c(0,0),\n                Sigma = data.frame(x=c(1,rho),\n                                   y=c(rho,1)),\n                empirical = TRUE)\n\n# obtain scores\nx = data[,1]\ny = data[,2]\n\n# dichotomize data\nx_dich &lt;- as.numeric(x&gt;0)\ny_dich &lt;- as.numeric(y&gt;0)\n  \n# compute observed correlation\nro &lt;- cor(x_dich,y_dich)\n\n# compute observed sampling variance\nvar_e_o &lt;- (1 - ro^2)^2 / (n-1)\n\n# compute split proportions\npx = sum(x_dich==1)/length(x_dich)\npy = sum(y_dich==1)/length(y_dich)\n\nWe can correct the correlation using base R. In order to correct for dichotomization, we can use the equations presented in the previous section.\n\n# get cut-point\nCy &lt;- qnorm(py)\nCx &lt;- qnorm(px)\n\n# calculate attenuation factors\nax &lt;- dnorm(Cx)/sqrt(px*(1-px)) \nay &lt;- dnorm(Cy)/sqrt(py*(1-py)) \n\n# correct r\nrc &lt;- ro / (ax*ay)\n\n# adjust standard error for rc\nvar_e_c &lt;- var_e_o * (rc/ro)^2\n\n# print results\nrbind(paste0('Observed: ro = ',round(ro,3)),\n      paste0('Observed: var = ',round(var_e_o,5)),\n      paste0('Corrected: rc = ',round(rc,3)),\n      paste0('Corrected var = ',round(var_e_c,5)))\n\n     [,1]                     \n[1,] \"Observed: ro = 0.321\"   \n[2,] \"Observed: var = 0.00161\"\n[3,] \"Corrected: rc = 0.504\"  \n[4,] \"Corrected var = 0.00398\"\n\n\nWe can also correct for dichotomization using the correct_r_dich function in the psychmeta package (Dahlke and Wiernik 2019).\n\ncorrect_r_dich(ro,px=px,py=py,n=n)\n\n  r_corrected var_e_corrected    n_adj\n1   0.5035871     0.003982818 140.8791"
  },
  {
    "objectID": "artificial_dichotomization.html#correcting-standardized-mean-differences-for-artificial-dichotomization",
    "href": "artificial_dichotomization.html#correcting-standardized-mean-differences-for-artificial-dichotomization",
    "title": "7  Artificial Dichotomization",
    "section": "7.4 Correcting Standardized Mean Differences for Artificial Dichotomization",
    "text": "7.4 Correcting Standardized Mean Differences for Artificial Dichotomization\n\n7.4.1 Defining our estimand\nWe would like to know the group difference between scores of a naturally continuous variable. Our estimand can thus be defined as the population standardized mean difference between groups \\(A\\) and \\(B\\) on continuous scores of the dependent variable (\\(y\\)), \\(\\delta_{y}\\). Where dichotomized scores can be defined as\n\\[\ny_{A\\ddagger}=\n\\begin{cases}\n    1,& \\text{if } y_A&gt;C_y\\\\\n    0,& \\text{if } y_A&lt;C_y\n\\end{cases}\n\\]\n\\[\ny_{B\\ddagger}=\n\\begin{cases}\n    1,& \\text{if } y_B&gt;C_y\\\\\n    0,& \\text{if } y_B&lt;C_y\n\\end{cases}\n\\]\nIn studies of group differences, since the independent variable is already dichotomous, the only dichotomization that can occur is on the dependent variable. The observed study standardized mean difference can be modeled as a function of the population standardized mean difference on continuous scores (\\(\\delta_{y}\\); i.e., the estimand), an artifact biasing factor \\(a\\), and sampling error, \\(\\varepsilon\\),\n\\[\nd_{y_\\ddagger} = a\\delta_{y}+\\varepsilon.\n\\] Where unbiased estimate of \\(\\delta_{y}\\) can be calculated by dividing the observed standardized mean difference by the biasing factor, \\(a\\),\n\\[\nd_c = \\frac{d_{y_\\ddagger}}{a}.\n\\]\n\n\n7.4.2 Artifact Correction for Standardized Mean Differences\nThe simplest way to correct for dichotomization in a standardized mean difference is to first convert the observed \\(d\\) value of the dichotomized dependent variable to a correlation coefficient. When converting to a correlation coefficient, it’s important to note the binary nature of both variables, leading us to estimate the phi coefficient rather than the point-biserial correlation that we would be estimating if the dependent variable was continuous. To calculate the phi coefficient from a \\(d\\) value we can use the proportion of group membership in group \\(A\\) or group \\(B\\) (\\(p\\); it does not matter which one is chosen, as long as it is consistent for every instance of \\(p\\)),\n\\[\nr_o = \\frac{d_{y_\\ddagger}}{\\sqrt{d_{y_\\ddagger}^2+\\frac{1}{p(1-p)}}}\n\\]\nWe can then correct the phi coefficient similar to how we correct the point-biserial correlation in Section 7.3.2,\n\\[\nr_c = \\frac{r_o}{\\left[\\frac{\\varphi(C_y)}{\\sqrt{p_y (1-p_y)}}\\right]}.\n\\]\nThen we can convert the corrected correlation back into a standardized mean difference,\n\\[\nd_c = \\frac{r_c}{\\sqrt{p\\left(1-p\\right)\\left(1-r_c^2\\right)}}.\n\\]\nWhere \\(d_c\\) is our corrected correlation. The observed sampling variance (\\(\\sigma_{\\varepsilon_o}\\)) must also be corrected using the same three step procedure. For simplicity, we will consolidate this into one formula,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac {\\sigma^2_{\\varepsilon_o} \\left(\\frac{r_c}{r_o}\\right)^2} {\\left(1+d_{y_\\ddagger}^2p[1-p]\\right)^3(1-r_c^2)^3}.\n\\]\n\nObtaining Standardized Mean Difference from Odds Ratio\nIn most cases, difference in dichotomized outcomes between two groups is unlikely to be reported as a standardized mean difference, instead it will be more commonly reported as an odds ratio (\\(OR\\)). The odds ratio is the difference in the ratio of high and low scorers of group \\(A\\) and group \\(B\\), such that,\n\\[\nOR = \\frac{\\left(\\frac{n_{A,\\text{high}}}{n_{A,\\text{low}}}\\right)}{\\left(\\frac{n_{B,\\text{high}}}{n_{B,\\text{low}}}\\right)}\n\\] Where \\(n_{Group,\\text{high/low}}\\) is the corresponding sample size. The odds ratio is asymmetric about \\(1\\) (null), but we can make it symmetric by log transforming it (\\(\\log(OR)\\)). The sampling variance of the log odds ratio \\(\\sigma^2_{\\log(OR)}\\) can be defined as,\n\\[\n\\sigma^2_{\\log(OR)} = \\frac{1}{n_{A,\\text{low}}} + \\frac{1}{n_{A,\\text{high}}} + \\frac{1}{n_{B,\\text{low}}} + \\frac{1}{n_{B,\\text{high}}}\n\\]\nThe equation above requires the full contingency table to compute.\n\n\n\n7.4.3 Correcting Standardized Mean Differences in R\nTo correct standardized mean differences for dichotomization in R . At the moment the psychmeta package does not have a correct_d_dich function. In order to correct for dichotomization, we can use the three step process from Section 7.4.2. Lets simulate data based on a true standardized mean difference of \\(\\delta_y=0.50\\) and \\(y\\) scores that have been cut in half (i.e., a 50-50 median split).\n\n# set seed\nset.seed(343)\n\n# define parameters\ndelta = .50\nnA &lt;- nB &lt;-  1000\nn &lt;- nA + nB\np &lt;- nA / n\n\n# simulate data\nyA &lt;- rnorm(nA,.50,1)\nyB &lt;- rnorm(nB,0,1)\ny_dich &lt;- as.numeric(c(yA,yB)&gt;0)\nyA_dich &lt;- y_dich[1:nA]\nyB_dich &lt;- y_dich[(nA+1):n]\n\n# calculate observed standardized mean difference\ndo = (mean(yA_dich) - mean(yB_dich)) / sqrt((var(yA_dich) + var(yB_dich))/2)\n\n# calculate observed sampling variance\nvar_e_o = (n / (nA * nB) + do^2 / (2 * n))\n\n# get cut-point\npy = sum(y_dich==1)/length(y_dich)\nCy = qnorm(py)\n\n# calculate attenuation factor of y\nay &lt;- dnorm(Cy)/sqrt(py*(1-py)) # attenuation factor for dichotomization in y\n\n# convert d to r\nro &lt;- do / sqrt(do^2 + (1 / (p*(1-p))))\n\n# correct r\nrc &lt;- ro / ay\n\n# convert r to d\ndc &lt;- rc / sqrt(p*(1-p)*(1-rc^2))\n\n# correct sampling variance\nvar_e_c &lt;- (var_e_o * (rc/ro)^2) / ((1+do^2*p*(1-p))^3 * (1-rc^2)^3)\n\n# print results\nrbind(paste0('Observed: do = ',round(do,3)),\n      paste0('Observed: var = ',round(var_e_o,6)),\n      paste0('Corrected: dc = ',round(dc,3)),\n      paste0('Corrected: var = ',round(var_e_c,6)))\n\n     [,1]                       \n[1,] \"Observed: do = 0.43\"      \n[2,] \"Observed: var = 0.002046\" \n[3,] \"Corrected: dc = 0.553\"    \n[4,] \"Corrected: var = 0.003586\"\n\n\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nHunter, John, and Frank Schmidt. 1990. “Dichotomization of Continuous Variables: The Implications for Meta-Analysis.” Journal of Applied Psychology 75 (June): 334–49. https://doi.org/10.1037/0021-9010.75.3.334.\n\n\nMaxwell, Scott, and Harold Delaney. 1993. “Bivariate Median Splits and Spurious Statistical Significance.” Psychological Bulletin 113 (January): 181–90. https://doi.org/10.1037/0033-2909.113.1.181.\n\n\nPearson, Karl. 1913. “On the Probable Error of a Coefficient of Correlation as Found from a Fourfold Table.” Biometrika 9 (1/2): 22–33. https://doi.org/10.2307/2331798."
  },
  {
    "objectID": "dedication.html",
    "href": "dedication.html",
    "title": "2  Dedication",
    "section": "",
    "text": "In Loving Memory of Haley Jané\n\n\nMy companion, whose love and presence have filled my life with joy and comfort."
  },
  {
    "objectID": "intro.html#what-are-effect-sizes",
    "href": "intro.html#what-are-effect-sizes",
    "title": "3  Effect Sizes and Notation",
    "section": "3.1 What are Effect Sizes?",
    "text": "3.1 What are Effect Sizes?\nEffect sizes are statistics that measure the magnitude of a relationship between two variables. It’s important to remember that effect sizes are a valuable tool, enabling researchers to extract meaningful insights from data, rather than being the ultimate objective themselves. Effect sizes aide in researcher’s ability to draw meaningful inferences from data and therefore it is crucial that they are accurate. Biased effect sizes can be likened to a foggy windshield. Just as condensation on glass obstructs a clear view of the road, biased effect sizes can obscure the true association between variables. Similar to how one must clean the windshield to drive safely, researchers must correct for biases in effect sizes to attain a clear and accurate perspective on their data. Correlation coefficients and standardized mean differences are two of the most common effect sizes and so they will be the primary focus of this book. To see how an effect size may look in practice, the example below will illustrate how calculating one may look in a clinical setting.\n\n3.1.1 Applied Example\nLets say we want to test whether a new drug can alleviate anxiety, therefore we decide to conduct an experiment to see how well this drug performs. We first randomly assign each participant in the study to either a treatment group (\\(T\\)) or a control group (\\(C\\)). In our experiment we want test how well the experimental drug reduces anxiety, therefore we measure the subjects’ self reported anxiety after under-going the treatment. To see if the drug actually worked in alleviating anxiety, we want to compare the scores from the treatment group and the control group. To do this we can estimate the average treatment effect (\\(ATE\\)), which is the difference in the mean value of self-reported anxiety scores between the treatment group and the control group such that, \\(ATE = \\text{Mean}(X_T) - \\text{Mean}(X_C)\\). However, anxiety scores have no meaningful units, so if we obtain an \\(ATE\\) value of \\(-3\\) there is no way to tell if this value is large or small, since it is entirely dependent on how the anxiety scores are scaled. Standardization can allow us to draw meaningful inferences about the size of the effect that can be comparable across scales. We can standardize the \\(ATE\\) by dividing by the standard deviation of scores in the control group, (\\(\\text{SD}\\)): \\(\\text{Effect Size} = \\frac{ATE}{\\text{SD}_C}\\). The effect size is now on an interpretable scale (standard deviations). If we achieve an standardized effect size value of \\(-0.50\\), we can interpret this as the treatment group exhibiting a reduction in anxiety equivalent to half a standard deviation compared to the control group.\n\n\n\n\n\nSimulated experimental data\n\n\n\n\n\n\n3.1.2 Defining effect sizes\nLets say we have an effect size of interest that quantifies the relationship between an independent and dependent variable. The population effect size can be denoted as \\(\\vartheta\\), however this population value is unknown. We can obtain an estimate the population effect size by conducting a study on a sample drawn from the population and then calculating a study effect size, \\(\\theta\\). The study effect size is a function of the population effect size and sampling error (\\(\\varepsilon\\)) such that,\n\\[\n\\theta = \\vartheta + \\varepsilon\n\\tag{3.1}\\]\nEffect sizes will differ from study to study, this can be due to two reasons: variance in population effect sizes (\\(\\sigma^2_\\vartheta\\)) or variance in sampling error (\\(\\sigma_\\varepsilon\\)). Accordingly, we can express the variance in study effect sizes (\\(\\sigma_\\theta\\)) as,\n\\[\n\\sigma^2_\\theta = \\sigma^2_\\vartheta + \\sigma^2_\\varepsilon\n\\]\nIf studies were drawing samples from the same population, the variance in the population effect size would be zero (\\(\\sigma^2_\\vartheta = 0\\)) and the expected value (i.e., the mean) of study effect sizes would be equal to the population effect size, \\(\\mathbb{E}[\\theta]=\\vartheta\\)."
  },
  {
    "objectID": "intro.html#effect-sizes-and-artifacts",
    "href": "intro.html#effect-sizes-and-artifacts",
    "title": "3  Effect Sizes and Notation",
    "section": "3.2 Effect Sizes and Artifacts",
    "text": "3.2 Effect Sizes and Artifacts\nIn practice, observed effect size estimates are often biased relative to the true effect size of interest, that is, the observed population effect size (\\(\\vartheta_o\\)) is a product of the true population effect size (\\(\\vartheta\\)) and artifactual bias (\\(a\\)):\n\\[\n\\vartheta_o = a\\vartheta\n\\tag{3.2}\\]\nNote that if \\(a=1\\) this would indicate that there is no artifactual bias (\\(\\vartheta_o=\\vartheta\\)), if \\(a&gt;1\\) then it would indicate effect size inflation (i.e., biased away from zero), and if \\(a&lt;1\\) that would indicate effect size attenuation (i.e., biased toward zero). It can be seen in Equation 3.2 that we can re-arrange the formula to obtain the true population effect size by dividing the observed population effect size by \\(a\\),\n\\[\n\\vartheta = \\frac{\\vartheta_o}{a}.\n\\]\nFor a single study that computes an effect size from a sample drawn from the population, the observed study effect size (\\(\\theta_o\\)) would be expressed by\n\\[\n\\theta_o = \\vartheta_o + \\varepsilon_o\n\\] Using Equation 3.2 we can express the observed effect size in terms of the true population effect size rather than the observed population effect size,\n\\[\n\\theta_o = a\\vartheta + \\varepsilon_o\n\\] Then we can correct the observed effect size by dividing by the biasing factor, \\(a\\), to obtain an unbiased estimate of the true effect size:\n\\[\n\\theta_c = \\frac{\\theta_o}{a}\n\\] The sampling error and it’s variance must also be corrected,\n\\[\n\\varepsilon_c = \\frac{\\varepsilon_o}{a}\n\\] \\[\n\\sigma_{\\varepsilon_c}^2 = \\frac{\\sigma^2_{\\varepsilon_o}}{a^2}.\n\\] The corrected effect size should be an unbiased estimate of the true population effect size as long as the systematic bias multiplier is accurately measured (which is not a trivial task). It is important to note that the corrected effect size will not yield additional statistical power, that is, test-statistics and p-values will remain unchanged. We can demonstrate this mathematically that the z-statistic of the observed effect size (\\(z_{\\theta_o}\\)) is identical to the z-statistic of the corrected effecct size (\\(z_{\\theta_c}\\)),\n\\[\nz_{\\theta_o} = \\frac{\\theta_o}{\\sigma_{\\varepsilon_o}} = \\frac{\\frac{\\theta_o}{a}}{\\frac{\\sigma_{\\theta_o}}{a}} = \\frac{\\theta_c}{\\sigma_{\\varepsilon_c}} = z_{\\theta_c}\n\\]"
  },
  {
    "objectID": "intro.html#defining-an-effect-size-estimand",
    "href": "intro.html#defining-an-effect-size-estimand",
    "title": "3  Effect Sizes and Notation",
    "section": "3.3 Defining an Effect Size Estimand",
    "text": "3.3 Defining an Effect Size Estimand\nAn effect size estimand is the theoretical quantity that we are trying to estimate. Before delving into the application of correction factors, it is important to clearly define the effect size estimand you aim to capture, including the summary statistic, relevant variables, and the target population. This preliminary step might appear trivial, but it is crucial, as it determines the accuracy and relevance of any subsequent artifact corrections. For instance, consider a scenario where we conduct a study involving a sample of college students with the aim of generalizing our findings to the broader general population. In this context, it is important to correct for range restriction, given the evident selection effects that exist in the college student populations. However, if our sole objective is to draw conclusions pertaining exclusively to the college student demographic, correcting for range restriction would be inappropriate. Furthermore, let’s examine the variable of interest, such as grade-point average (GPA), within this population. Do we intend to focus solely on the raw GPA score, or is our goal to capture what GPA represents, namely, academic achievement? If our aim is to investigate the raw GPA score, then correcting for measurement error would be inappropriate. However, if our primary focus lies in assessing the student’s academic achievement, then it may be relevant to correct GPA scores for measurement error. Defining our estimand guides our approach to artifact correction and ensures that these correction procedures align with the underlying research goals."
  },
  {
    "objectID": "intro.html#effect-size-notation",
    "href": "intro.html#effect-size-notation",
    "title": "3  Effect Sizes and Notation",
    "section": "3.4 Effect Size Notation",
    "text": "3.4 Effect Size Notation\nBecause of the nature of the topic, this book will cover a large amount of equations and computer code. Therefore to make it as straight-forward as possible the notation will follow a systematic framework to distinguish between types of effect sizes. This book will only be covering two main types of effect sizes: correlations (\\(r\\)) and standardized mean differences (\\(d\\)). Throughout the book variations of \\(r\\) and \\(d\\) will show up frequently, these variations will be differentiated with subscripts that are consistent with that section. Also, to distinguish between population-level values (i.e., the effect size across all potential observations) and effect sizes specific to a study or sample (i.e., the effect size observed within a single sample drawn from the population), we will use the following notation:\n\nArbitrary Effect Size\n\nPopulation value: \\(\\vartheta\\)\nStudy/sample value: \\(\\theta\\)\n\nCorrelations\n\nPopulation value: \\(\\rho\\)\nStudy/sample value: \\(r\\)\n\nStandardized Mean Differences\n\nPopulation value: \\(\\delta\\)\nStudy/sample value: \\(d\\)\n\n\nIn most cases, continuous independent variables will be denoted with \\(x\\) and dependent variables with \\(y\\) (note that this notation may differ when referring to observed and true scores). Categorical (i.e., groupings) variables will be denoted with \\(g\\) (this notation will be used primarily for standardized mean differences)."
  },
  {
    "objectID": "intro.html#correlations",
    "href": "intro.html#correlations",
    "title": "3  Effect Sizes and Notation",
    "section": "3.5 Correlations",
    "text": "3.5 Correlations\nA correlation describes the relationship between two continuous variables. The Pearson correlation coefficient was first introduced by Auguste Bravais (1844). Later developed by Karl Pearson, lending itself to the name.\n\n3.5.1 Technical Overview Correlations (r)\nIf we draw a sample of \\(n\\) observations from a population, we can calculate the study correlation (\\(r\\)) between variables \\(x\\) and \\(y\\) using the following Pearson’s product-moment estimator,\n\\[\nr = \\frac{\n\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n}{\n\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n}.\n\\tag{3.3}\\]\nFor digestibility, we can break down the formula into parts. The correlation coefficient can be defined as the covariance between \\(x\\) and \\(y\\) standardized by the product of their standard deviations (i.e., square root variance),\n\\[\nr = \\frac{\\sigma_{xy}}\n{\\sigma_x\\sigma_y}.\n\\tag{3.4}\\]\nwe can first define the covariance (\\(\\sigma_{xy}\\)) as the average product of errors for \\(x\\) and \\(y\\),\n\\[\n\\sigma_{xy} =\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y}).\n\\tag{3.5}\\]\nThen we can find the variance for \\(x\\) and \\(y\\) by taking the average squared error from the mean for \\(x\\) and \\(y\\),\n\\[\n\\sigma^2_x = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\n\\tag{3.6}\\]\n\\[\n\\sigma^2_y = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2.\n\\tag{3.7}\\]\nPlugging in Equation 3.5, Equation 3.6, and Equation 3.7 into Equation 3.4 we can see that the term, \\(\\frac{1}{n-1}\\), will cancel out and we will be left with the original pearson correlation coefficient formula from Equation 3.3,\n\\[\nr = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}}.\n\\tag{3.8}\\]\nIn the absence of artifacts, the Pearson correlation \\(r\\) is an asymptotically (i.e., as \\(n\\) approaches infinity) unbiased estimator (in small sample sizes, it is biased see chapter 4). We can express \\(r\\) similarly to Equation 3.1,\n\\[\nr = \\rho + \\varepsilon,\\;\\; \\sigma_\\varepsilon^2 = \\text{Var}(\\varepsilon).\n\\tag{3.9}\\]\nWhere \\(\\sigma^2_\\varepsilon\\) is the sampling variance of the observed correlation. The sampling variance can be calculated from the sample size (\\(n\\)) and the population correlation,\n\\[\n\\sigma_\\varepsilon^2 =\\frac{(1 - \\rho^2)^2}{n}.\n\\tag{3.10}\\]\nIn practice, the population correlation is unknown so the study correlation can be used instead (\\(r\\)) in the above formula as an estimate of the population correlation. Note that the sampling variance is the square of the standard error.\nIn the context of artifacts, if the observed correlation is biased relative to the true correlation, we can see model the observed population (\\(\\rho_o\\)) correlation as a function of the true population correlation (\\(\\rho\\)) and a artifact biasing factor, \\(a\\),\n\\[\n\\rho_o = a\\rho.\n\\]\nThe observed study (sample) correlation would then be defined as,\n\\[\nr_o = \\rho_o + \\varepsilon_o = a\\rho + \\varepsilon_o.\n\\]\nTo obtain an unbiased estimate of the true population correlation, we can correct the correlation coefficient (\\(r_c\\)) by dividing by the biasing factor, \\(a\\),\n\\[\nr_{c} = \\frac{r_o}{a}.\n\\]\nNote that the sampling error would also need to be adjusted and therefore it’s sampling variance (\\(\\sigma_{\\varepsilon_c}\\)) would be corrected to be,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o} }{a^2}.\n\\]"
  },
  {
    "objectID": "intro.html#standardized-mean-differences",
    "href": "intro.html#standardized-mean-differences",
    "title": "3  Effect Sizes and Notation",
    "section": "3.6 Standardized Mean Differences",
    "text": "3.6 Standardized Mean Differences\nStandardized mean differences are used to quantify the average difference in some variable between groups. The most commonly used formulation is Cohen’s \\(d\\) (Cohen 1988) which quantifies the average difference between groups (e.g., men vs. women) and standardizes by the pooled standard deviation. Note that the other most commonly used estimator is Hedges’ \\(g\\), but the difference between the two is a small sample correction factor that can be found in the chapter on small samples.\n\n3.6.1 Technical Overview of Standardized Mean Difference (d)\nIf we draw a sample of \\(n_A\\) subjects from group \\(A\\) and \\(n_B\\) subjects from group \\(B\\), the mean difference between groups (\\(d\\)) on variable \\(y\\) can be defined as,\n\\[\nd=\\frac{\\bar{y}_A - \\bar{y}_B}{\\sigma_p}.\n\\]\nWhere the standardizer, \\(\\sigma_p\\) is the pooled standard deviation between the two groups. The pooled standard deviation is calculated by taking the square root of the average variance between the two groups weighted by the degrees of freedom (e.g., \\(\\text{df}_A=n_A-1\\)),\n\\[\n\\sigma_p=\\sqrt{\\frac{(n_A-1)\\sigma^2_{A} + (n_B-1)\\sigma^2_{B}}{n_A + n_B - 2}}.\n\\]\nWhere \\(\\sigma_{A}\\) and \\(\\sigma_{B}\\) are the standard deviations of \\(y\\) within groups \\(A\\) and \\(B\\) respectively. This SMD estimator is commonly referred to as Cohen’s \\(d\\). We can define the study/sample \\(d\\) value as a function of the population \\(d\\) value (\\(\\delta\\)): \\[\nd = \\delta + \\varepsilon.\n\\] Similar to the previous section on correlation coefficients, the observed \\(d\\) value is a function of the true population value and an artifactual biasing factor (\\(a\\)),\n\\[\n\\delta_o = a\\delta.\n\\]\nTherefore the observed study/sample \\(d\\) value can be defined as a function of the observed population value or the true population value and bias:\n\\[\nd = \\delta_o + \\varepsilon_o = a\\delta + \\varepsilon_o.\n\\]\nThus the corrected standardized mean difference (\\(d_c\\)) and it’s corresponding sampling variance (\\(\\sigma^2_{\\varepsilon_c}\\)) can both be defined as:\n\\[\nd_c = \\frac{d_o}{a}\n\\]\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o}}{a^2}.\n\\]\n\n\n\n\nBravais, A. 1844. Analyse mathématique sur les probabilités des erreurs de situation d’un point. Impr. Royale.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. Academic Press."
  },
  {
    "objectID": "small_samples.html#introduction",
    "href": "small_samples.html#introduction",
    "title": "4  Small Samples",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThe purpose of sample statistics such as correlations and standardized mean differences is to draw meaningful inferences about the population. However, effect size estimators such as Pearson’s correlation coefficient and Cohen’s \\(d\\) are biased in small sample sizes. This small sample bias will impair our ability to draw accurate inferences about the population."
  },
  {
    "objectID": "small_samples.html#when-correcting-alongside-other-artifacts",
    "href": "small_samples.html#when-correcting-alongside-other-artifacts",
    "title": "4  Small Samples",
    "section": "4.2 When Correcting alongside other Artifacts",
    "text": "4.2 When Correcting alongside other Artifacts\nThe small sample bias should always be corrected for prior to applying any other artifact correction. It is independent of all other artifact corrections and therefore the corrected effect sizes in this section can be treated as the observed effect sizes (and observed sampling variance) in other sections."
  },
  {
    "objectID": "small_samples.html#correcting-for-small-sample-bias-in-standardized-mean-differences",
    "href": "small_samples.html#correcting-for-small-sample-bias-in-standardized-mean-differences",
    "title": "4  Small Samples",
    "section": "4.3 Correcting for Small Sample Bias in Standardized Mean Differences",
    "text": "4.3 Correcting for Small Sample Bias in Standardized Mean Differences\n\n4.3.1 Defining our Estimand\nOur quantity of interest is the population standardized mean difference, \\(\\delta\\), between groups \\(A\\) and \\(B\\) on variable, \\(y\\). We can model the relationship between the population standardized mean difference and our observed standardized mean difference (\\(d_o\\)) with,\n\\[\nd_o = a\\delta+\\varepsilon.\n\\]\nWhere \\(a\\) is our small sample biasing factor and \\(\\varepsilon\\) is our sampling error term. Ultimately, we can obtain an unbiased estimate of the population standardized mean difference by correcting the observed standardized mean difference as follows,\n\\[\nd_c = \\frac{d_o}{a}.\n\\]\n\n\n4.3.2 Artifact Correction for Small Sample Bias\nAs the sample size approaches infinity, Cohen’s standardized mean difference estimator is an unbiased estimator of the population standardized mean difference (Hedges 1981; Cohen 2013). However, in small sample sizes Cohen’s estimator is biased upward, that is, on average, it overestimates the population standardized mean difference. To see why this is the case, we can first define the standardized mean difference between group \\(A\\) and group \\(B\\) such that,\n\\[\nd = \\frac{\\bar{y_A}-\\bar{y}_B}{\\sigma_p}.\n\\]\nWhere \\(\\bar{y}_A\\) and \\(\\bar{y}_B\\) are the observed arithmetic means of group \\(A\\) and group \\(B\\), respectively. The raw difference between these two means will be unbiased, since the arithmetic means themselves are unbiased estimators at all sample sizes. However, the pooled standard deviation, \\(\\sigma_p\\), is biased when sample sizes are small. Statisticians ultimately had to choose whether the estimator of the standard deviation or the variance (the square of the standard deviation) would be unbiased. Since the variance has more utility in much of statistics, it was more important for the estimator of variance to be unbiased. Therefore the resulting bias in the standard deviation will bleed over into the equation for standardized mean differences. This bias can be visualized in the figure below. Notice that the sample standard deviation is under-estimated in small sample sizes (left plot), and the standardized mean difference is over-estimated in small sample sizes (right plot).\n\n\n\n\n\nNote. dotted lines in both plots indicate the population value and the dots indicate the sample value at each sample size integer (n=1,2,3,…)\n\n\n\n\nTo obtain an unbiased estimate of the population standardized mean difference, we need to estimate a correction factor that can account for this bias. The small sample correction factor has been derived previously by Hedges (1989). When applied to a \\(d\\) value, it is common convention to refer to resulting corrected value as “Hedges’ \\(g\\)”, giving credit to the originator and to keep it similar in style to the conventionally termed “Cohen’s \\(d\\)”. However this convention will not be used here, instead we will denote it in this section as the small sample corrected \\(d\\) value (\\(d_c\\)). We can compute the artifact biasing factor, \\(a\\), with the total sample size (\\(n=n_A+n_B\\)) and Gamma functions (\\(\\Gamma(\\cdot)\\)),\n\\[\na = \\frac{\\Gamma\\left(\\frac{n-3}{2}\\right)\\sqrt{\\frac{n-2}{2}}}{\\Gamma\\left(\\frac{n-2}{2}\\right)}.\n\\]\nThen to correct the observed standardized mean difference for small sample bias we can divide by \\(a\\),\n\\[\nd_c = \\frac{d_o}{a} = \\frac{d_o}{ \\left[\\frac{\\Gamma\\left(\\frac{n-3}{2}\\right)\\sqrt{\\frac{n-2}{2}}}{\\Gamma\\left(\\frac{n-2}{2}\\right)}\\right]}.\n\\]\nIt is important to point out that there is not a sample size threshold in which this correction does not apply, therefore common suggestions such as “small sample correction should be applied when n&lt;30” is misguided since this correction can (and should) be applied at any sample size. Since the formula is quite complicated, there is also a simpler approximation of the this formula also given by Hedges (1989), \\(a\\approx 1/\\left(1-\\frac{3}{4n-3}\\right)\\). When this correction is made, we must also adjust the sampling variance (\\(\\sigma_{\\varepsilon_o}\\)),\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o}}{a^2} =\\frac{\\sigma^2_{\\varepsilon_o}}{\\left[\\frac{\\Gamma\\left(\\frac{n-3}{2}\\right)\\sqrt{\\frac{n-2}{2}}}{\\Gamma\\left(\\frac{n-2}{2}\\right)}\\right]^2}.\n\\]\n\n\n4.3.3 Correcting Standardized Mean Differences in R\nTo compute the correction in R, we can first simulate 20 data points, 10 in group \\(A\\) and 10 in group \\(B\\). Then we can calculate the observed \\(d\\) value.\n\n# set seed\nset.seed(120)\n\n# define parameters\nnA &lt;- 10\nnB &lt;- 10\nn &lt;- nA + nB\ndelta &lt;- .5\n\n# simulate data\nyA &lt;- rnorm(nA,delta,1)\nyB &lt;- rnorm(nB,0,1)\n\n# calculate observed d value\nSD_pooled = ((nA-1)*var(yA) + (nB-1)*var(yB)) / (nA+nB-2)\ndo = (mean(yA) - mean(yB)) / SD_pooled\n\nOnce we have obtained \\(d_o\\), we can then correct it with the equations in the previous section.\n\n# calculate bias factor\na &lt;- (gamma((n-3)/2)*sqrt((n-2)/2)) / gamma((n-2)/2) \n\n# correct d value\ndc = do / a\n\n# print results\nrbind(paste0('Observed: do = ',round(do,3)),\n      paste0('Corrected: dc = ',round(dc,3)))\n\n     [,1]                   \n[1,] \"Observed: do = 0.889\" \n[2,] \"Corrected: dc = 0.851\"\n\n\nYou will notice that the small sample correction reduced the observed \\(d\\) value however it is still far away from the true value. This is simply due to the fact that there is a large amount of sampling error on top of the bias."
  },
  {
    "objectID": "small_samples.html#correcting-for-small-sample-bias-in-correlations",
    "href": "small_samples.html#correcting-for-small-sample-bias-in-correlations",
    "title": "4  Small Samples",
    "section": "4.4 Correcting for Small Sample Bias in Correlations",
    "text": "4.4 Correcting for Small Sample Bias in Correlations\n\n4.4.1 Defining our Estimand\nOur quantity of interest is the population correlation, \\(\\rho\\), between independent variable, \\(x\\) and dependent variable, \\(y\\). We can model the relationship between the population correlation and our observed correlation (\\(r_o\\)) with,\n\\[\nr_o = a\\rho+\\varepsilon\n\\]\nWhere \\(a\\) is our small sample biasing factor and \\(\\varepsilon\\) is our sampling error term. Ultimately, we can obtain an unbiased estimate of the population correlation by correcting the observed correlation as follows,\n\\[\nr_c = \\frac{r_o}{a}\n\\]\n\n\n4.4.2 Artifact Corrections\nCorrelation coefficients also are biased in small sample sizes (Olkin and Pratt 1958). As opposed to standardized mean differences, correlations are under-estimated, rather than over-estimated, in small samples. The bias is quite small, however we can apply a correction factor to obtain unbiased estimates of the population correlation. Because the bias is so small and the exact formula is a hypergometric function using infinite power series, we will instead focus on the extremely close approximation provided in Olkin and Pratt (1958). Therefore, the biasing factor can be calculated such that,\n\\[\na \\approx \\frac{1}{1+\\frac{1-r_o^2}{2(n-3)}}.\n\\]\nThen we can correct the point-estimate the sampling variance for small sample bias\n\\[\nr_c = \\frac{r_o}{\\left[ 1+\\frac{1-r_o^2}{2(n-3)}\\right]}\n\\]\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o}}{\\left[ 1+\\frac{1-r_o^2}{2(n-3)}\\right]^2}.\n\\]\n\n\n4.4.3 Correcting Correlations in R\nTo compute the correction in R, we can first simulate 10 correlated data points. Then we can calculate the observed correlation.\n\n# load packages\n# install.packages('MASS')\nlibrary(MASS)\n\n# set seed\nset.seed(1)\n\n# define parameters\nn &lt;- 10\nrho &lt;- .5\n\n# simulate data\ndata = mvrnorm(n = 10,\n               mu=c(0,0),\n               Sigma = data.frame(x=c(1,rho),x=c(rho,1)))\n\n# calculate observed d value\nro = cor(data[,1],data[,2])\n\nOnce we have obtained \\(r_o\\), we can then correct it with the equations in the previous section.\n\n# calculate bias factor\na &lt;- 1/(1 + (1-ro^2)/(2*(n-3)))\n\n# correct d value\nrc = ro / a\n\n# print results\nrbind(paste0('Observed: ro = ',round(ro,3)),\n      paste0('Corrected: rc = ',round(rc,3)))\n\n     [,1]                   \n[1,] \"Observed: ro = 0.247\" \n[2,] \"Corrected: rc = 0.264\"\n\n\nYou will notice that the small sample correction increases the observed correlation however it is still far away from the true value. This is simply due to the fact that there is a large amount of sampling error on top of the bias.\n\n\n\n\nCohen, Jacob. 2013. Statistical Power Analysis for the Behavioral Sciences. Academic Press.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator of Effect Size and Related Estimators.” Journal of Educational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\n———. 1989. “An Unbiased Correction for Sampling Error in Validity Generalization Studies.” Journal of Applied Psychology 74 (3): 469–77. https://doi.org/10.1037/0021-9010.74.3.469.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of Certain Correlation Coefficients.” The Annals of Mathematical Statistics 29 (1): 201–11. https://www.jstor.org/stable/2237306."
  },
  {
    "objectID": "unreliability.html#introduction",
    "href": "unreliability.html#introduction",
    "title": "5  Unreliability",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn general terms, measurement is the process of quantifying an attribute or characteristic of something. In scientific measurement, the measurand is the quantity or the attribute we intend to measure. In the psychological sciences, measurands usually take the form of constructs such as intelligence or anxiety. Often the goal of measurement is to produce quantities (i.e., scores) that accurately reflect the measurand. However, quantities that do not reflect a real attribute can still have useful predictive value (e.g., socio-economic status). It is important to note that measures are not all created equal, some perform better than others. Ideally, measures should produce scores that are consistent and repeatable, this is referred to as the reliability of a measure. A high quality measure should produce highly reliable scores. This section will review what reliability is in theory, how to estimate reliability, and how to correct effect sizes for measurement error."
  },
  {
    "objectID": "unreliability.html#sec-true-score-theory",
    "href": "unreliability.html#sec-true-score-theory",
    "title": "5  Unreliability",
    "section": "5.2 Reliability in True Score Theory",
    "text": "5.2 Reliability in True Score Theory\nTrue score theory (or classical test theory) is a mathematical formalization of observed scores obtained from a measurement procedure. Observed scores, \\(x_{im}\\), is defined as a score obtained from individual \\(i\\) upon measurement \\(m\\). The true score model assumes that each individual, has a true score, \\(T_i\\), that stays constant over repeated measurements. Variation in observed scores over repeated measurements is due to measurement-specific error, \\(e_{im}\\),\n\\[\nx_{im} = T_i+e_{im}.\n\\]\nHere, measurements are strictly parallel. Strictly parallel measurements have the following four properties (p. 69, Haertel 2006):\n\nMeasurements have identical specifications. That is, each measurement is obtained with an identical format and procedure.\nThe distribution of observed scores for each measurement are identical: \\(f(x_1) = f(x_2) = \\ldots\\).\nAny set of two measurements are assumed to covary the same as any other set of two measurements: \\(\\sigma_{x_1 x_2} = \\sigma_{x_2 x_3} = \\sigma_{x_1 x_3} = \\ldots\\).\nEach measurement equally covaries with any other variable: \\(\\sigma_{x_1 y} = \\sigma_{x_2 y} = \\ldots\\).\n\nTrue scores can be defined as the expected value (i.e., the mean) of observed scores over repeated measurements such that, \\(\\mathbb{E}_m[x_{im}]=T_{i}\\). Given this assumption, it can be inferred that the average of the resultant errors is zero across repeated measurements, \\(\\mathbb{E}_m[e_{im}]=0\\) and therefore the covariance between errors on repeated measurements is zero and the covariance between errors in parallel measurements is zero (\\(\\sigma_{e e'}=0\\)). It follows that the covariance between errors and true scores is also zero (\\(\\sigma_{eT}=0\\)). The independence between true scores and errors provide convenient parsing of the variance in observed scores (\\(\\sigma^2_{x}\\)) into components of variance in true scores (\\(\\sigma_T^2\\)) and measurement errors (\\(\\sigma_{e}^2\\)),\n\\[\n\\sigma_{x}^2 = \\sigma_T^2 + \\sigma_{e}^2.\n\\tag{5.1}\\]\nUltimately we desire to have observed scores that closely resemble true scores, therefore it is important to minimize measurement error variance (\\(\\sigma^2_e\\)). If \\(\\sigma_{e}^2 = 0\\) then the scores can be said to have perfect reliability, that is, observed scores do not vary upon repeated measurements and thus are identical to true scores. In practice, this is virtually never the case. Since we assume that the covariance between errors in parallel measurements is zero, it becomes apparent that the covariance between observed scores in parallel measurements must solely be attributable to variance in true scores, \\(\\sigma_{xx'}=\\,\\)\\(\\sigma_{TT'} + \\sigma_{ee'}=\\,\\)\\(\\sigma_{TT'}=\\,\\)\\(\\sigma_T^2\\,\\). In true score theory, reliability can be defined as the proportion of true variance in the total observed variance (\\(\\frac{\\sigma_T^2}{\\sigma_x^2}\\)) or the correlation between observed scores in parallel measurements (\\(r_{xx'}\\)).\n\\[\nr_{xx'}=\\frac{\\sigma_{xx'}}{\\sigma_x\\sigma_{x'}}  = \\frac{\\sigma_T^2}{\\sigma^2_{x}}.\n\\] The reliability is also equivalent to the square of the correlation between observed scores and true scores. To understand why this is the case, note that the covariance between parallel forms of a measure is equivalent to the covariance between observed scores and true scores, \\(\\sigma_{xT}=\\)\\(\\sigma_{(T+e)T}=\\)\\(\\sigma^2_T + \\sigma_{Te}=\\)\\(\\sigma^2_T = \\sigma_{xx'}\\).\n\\[\nr_{xx'} = \\frac{\\sigma_T^2}{\\sigma_{x}^2} = \\frac{(\\sigma_T^2)^2}{\\sigma_x^2 \\sigma_T^2}= \\frac{\\sigma_{xT}^2}{\\sigma^2_x\\sigma^2_T} = r^2_{xT}.\n\\tag{5.2}\\]\nIt is important to emphasize that true scores are expected values over repeated observations and they do not necessarily correspond to an actual, tangible quantity of interest (Borsboom and Mellenbergh 2002). As a result, every measurement has a true score, regardless of whether it gauges a concrete attribute or not. For example, if we construct a test by summing the responses to the items: “how many languages can you confidently hold a conversation in?” and “Estimate the number of photos you’ve taken in the last year across all devices”. Even in such nonsensical cases, the test’s composite score retains a true score, but this true score does not mirror a tangible reality.\n\n\n\nStructural diagram illustrating the relationship between true scores, observed scores, and error scores. The pink circle labeled \\(t\\) indicates the true scores, the blue squares labeled with \\(x\\) and \\(x'\\) represent observed scores on parallel measurements, and the red \\(e\\) denotes error. Correlations between \\(T\\), \\(x\\), and \\(x'\\) are in terms of reliability (\\(r_{xx'}\\)). Note that \\(\\sqrt{r_{xx'}}=r_{xT}\\)."
  },
  {
    "objectID": "unreliability.html#reliability-vs-validity",
    "href": "unreliability.html#reliability-vs-validity",
    "title": "5  Unreliability",
    "section": "5.3 Reliability vs Validity",
    "text": "5.3 Reliability vs Validity\nReliability and validity are distinct properties in measurement theory. Validity pertains to whether the scores produced by a measure reflects the quantities it is intended to measure (p. 14 Kelley 1927). According to Borsboom, Mellenbergh, and Van Heerden (2004), a measure is valid if both of the following statements are true:\n\nThe attribute exists.\nVariations in the attribute causally produce variations in the outcomes of the measurement procedure.\n\nBorsboom’s formulation of validity is simpler and more practical than other formulations such as Cronbach and Meehl’s (1955) nomological network approach to validity. It is important to note that even if an attribute does not exist (statement 1), scores may still provide predictive utility. For example, socio-economic status (SES) is a formative quantity that is constructed from a composite of education, income, occupation status, etc. Although SES is not causal to these indicators, SES can still be used as a predictor of important life outcomes."
  },
  {
    "objectID": "unreliability.html#estimating-reliability",
    "href": "unreliability.html#estimating-reliability",
    "title": "5  Unreliability",
    "section": "5.4 Estimating Reliability",
    "text": "5.4 Estimating Reliability\nIn practice, reliability must be estimated through indirect methods, this is due to the fact that true scores and errors are unknown. There are many reliability estimators that can be used, however we will go over a selection of internal consistency estimators as well as test-retest stability estimators.\n\n5.4.1 Internal Consistency Estimators\nTaking multiple measurements and then averaging tends to provide a more stable estimate of true values. For instance, let’s consider the case of Francis Galton (1907), who conducted a study involving 787 individuals estimating the weight of an ox. On average, each person’s estimate deviated by approximately 37 pounds from the actual weight of the ox, which was recorded as 1198 pounds. However, when all the guesses were averaged together, the combined estimate was 1207 pounds, just a 9 pound difference from the true value. This principle can be extended to broader applications, such as measuring psychological constructs. If we were to assess someone’s level of extraversion using ratings from their mother, father, friend, and sibling, the average of their combined assessments would yield a more reliable score compared to relying solely on a single evaluator. So to create a more stable composite score (\\(x\\)), we can take the score from \\(\\kappa\\) items (\\(\\mathbb{x}\\)) and sum them such that,\n\\[\nx = \\mathbb{x}_1 + \\mathbb{x}_2 +...+\\mathbb{x}_\\kappa\n\\]\nThe most commonly reported reliability estimator in the psychological sciences is coefficient alpha, also referred to as Cronbach’s alpha. Coefficient alpha, along with other internal consistency estimators, serves the purpose of assessing the reliability of composite scores comprising multiple item scores. Coefficient alpha only requires three parameters to calculate, the number of measurements (\\(\\kappa\\)), the variances of each item (\\(\\sigma^2_{\\text{i}_m}\\)), and the variance of the composite score (\\(\\sigma^2_{x}\\)). Coefficient alpha will estimate the reliability of the composite observed score, \\(x\\) (\\(r_{xx'}\\)),\n\\[\n_\\alpha r_{xx'} = \\frac{\\kappa}{\\kappa-1}\\left( 1 - \\frac{\\sum_{m=1}^\\kappa \\sigma^2_{\\mathbb{x}_m}}{\\sigma^2_{x}} \\right).\n\\tag{5.3}\\]\n\n\n\n\n\nFigures showing the observed scores upon 10 repeated measurements and the composite observed score for a single person (the true score is denoted with the dashed line). The left panel shows 10 observed scores with a lot of variation (i.e., low reliability). The composite score (dark red dot with error bars), shows wide error bars illustrating the low precision of the observed score score. The right panel also shows 10 observed scores with little variation (i.e., high reliability). The composite score (dark blue dot with error bars), shows narrow error bars illustrating the high precision of the observed score.\n\n\n\n\nWith tighter assumptions (see Haertel 2006), the formula for coefficient alpha can be simplified to just two parameters: the number of measurements (\\(\\kappa\\)) and the average correlation between measured scores (\\(\\bar{r}_{\\mathbb{x}_i \\mathbb{x}_j}\\), where \\(i\\neq j\\)). This formula is known as Spearman-Brown’s prophecy,\n\\[\n_\\text{sb} r_{xx'}= \\frac{\\kappa \\bar{r}_{\\mathbb{x}_i \\mathbb{x}_j}}{1+(\\kappa-1)\\bar{r}_{\\mathbb{x}_i \\mathbb{x}_j}}\n\\tag{5.4}\\]\nThis can be simplified further if we have two observed item scores. This formulation is a variation of split-half reliability:\n\\[\n_\\text{sh}r_{xx'}= \\frac{2r_{\\mathbb{x}_1 \\mathbb{x}_2}}{1+r_{\\mathbb{x}_1 \\mathbb{x}_2}}\n\\tag{5.5}\\]\nAll of these reliability estimators measure internal consistency, therefore they do not account for error outside of the measurement-specific error. There are other sources of error that internal consistency reliability estimates do not account for, such as transient error or rater-specific error.\n\n\n\nStructural model illustrating internal consistency. The pink circle labeled \\(T\\) indicates the true scores, the blue squares, \\(\\mathbb{x}_{1...\\kappa}\\), represent the observed scores across multiple measurements, and the red \\(e\\) denotes error. The dark blue hexagon, \\(x\\), indicates a composite score as a sum of the observed scores (\\(\\mathbb{x}_{1...\\kappa}\\)). Note that \\(\\sqrt{r_{xx'}}=r_{xT}\\).\n\n\n\n\n5.4.2 Calculating Internal Consistency in R\nLet us simulate a data set of 50 individuals that were measured four times resulting in four sets of scores (x1,x2,x3,x4) that have the same true score and error variance. Then let us calculate a composite score (x) from these sub-scores.\n\n#set seed\nset.seed(343)\n\n# set sample size\nn &lt;- 50\n\n# simulate data\nT_score &lt;- rnorm(n, 0, 1) # simulate true scores\nx1 &lt;- T_score + rnorm(n, 0, 1) # simulate observed scores for measurement 1\nx2 &lt;- T_score + rnorm(n, 0, 1) # simulate observed scores for measurement 2\nx3 &lt;- T_score + rnorm(n, 0, 1) # simulate observed scores for measurement 3\nx4 &lt;- T_score + rnorm(n, 0, 1) # simulate observed scores for measurement 4\n\n# calculate composite score\nx &lt;- x1 + x2 + x3 + x4\n\nNow let us calculate coefficient alpha from the formula provided in Equation 5.3.\n\n# step 1. calculate variance of observed (measured) scores\nvar_xm &lt;- c(var(x1),var(x2),var(x3),var(x4))\n\n# step 2. get number of items (k)\nk &lt;- length(var_xm)\n\n# step 3. calculate variance of composite score\nvar_x &lt;- var(x)\n\n# step 4. calculate coefficient alpha reliability\nrxx_alpha &lt;- k / (k-1) * (1 - sum(var_xm)/(var_x))\n\n# display reliability\nprint(round(rxx_alpha,3)) \n\n[1] 0.775\n\n\nWith the simplification of Coefficient alpha’s formula, let us calculate the reliability via Spearman-Brown’s prophecy formula provided in Equation 5.4.\n\n# step 1. get correlation matrix between all observed scores\ncorr_mat &lt;- cor(cbind(x1,x2,x3,x4))\n\n# step 2. average off-diagonal elements of matrix\ndiag(corr_mat) &lt;- NA\nrxixj &lt;- mean(corr_mat, na.rm = TRUE)\n\n# step 3. get number of items (k)\nk &lt;- dim(corr_mat)[1]\n\n# step 4. calculate Spearman-Brown reliability\nrxx_SB &lt;- k * rxixj / (1 + (k-1) * rxixj)\n\n# display reliability\nprint(round(rxx_SB,3)) \n\n[1] 0.775\n\n\nIf we simplify even further, we can calculate the Split-Half reliability formula provided in Equation 5.5,\n\n# step 1. make composite scores for each half of the observed scores\nxh1 &lt;- (x1 + x2)/2\nxh2 &lt;- (x3 + x4)/2\n\n# step 2. calculate the correlation between the scores of both halves\nrx1x2 &lt;- cor(xh1,xh2)\n\n# step 3. calculate the split-half reliability\nrxx_SH &lt;- 2*rx1x2 / (1 + rx1x2)\n\n# display reliability\nprint(round(rxx_SH,3)) \n\n[1] 0.824\n\n\nLets see how the results compare to the actual reliability,\n\n# calculate true reliability, true scores must be re-scaled by number of items\nrxx = var(k*T_score) / var_x\n\n# display actual reliability\nprint(round(rxx,3)) \n\n[1] 0.734\n\n\nIn this case, the reliability estimates do a fairly good job of estimating the true reliability of the observed scores. We can also use the alpha function from the psych package (“Psych: Procedures for Personality and Psychological Research” 2017) to estimate coefficient alpha too. It also provides additional item level information that is quite useful:\n\n# load in package\n# install.packages('psych')\nlibrary(psych)\n\n# compute summary reliability (only need first table)\nalpha(cbind(x1,x2,x3,x4))[[1]]\n\n raw_alpha std.alpha   G6(smc) average_r      S/N        ase        mean\n 0.7749847 0.7751377 0.7337024 0.4628829 3.447166 0.05141467 -0.04386823\n        sd  median_r\n 0.9567892 0.4571798\n\n\n\n\n5.4.3 Test-Retest Stability Estimator\nTransient errors represent fluctuations in observed scores over time. These fluctuations, even if they are systematic (e.g., fatigue over the course of a single day), add extraneous within-person variance that can mask true scores. Considering transient fluctuations as error depends on the research goal, so it is important for researchers to take care in considering which variance components should be considered error in their study (see Section 5.4.5). To estimate test-retest reliability, we can compute the correlation between the measurement at time 1 (\\(x_{t_1}\\)) and the second measurement at time 2 (\\(x_{t_2}\\)),\n\\[\n_\\text{tr}r_{xx'}= r_{x_{t_1}x_{t_2}}.\n\\]\nNote that calculating the pearson correlation coefficient between time-points ignores systematic changes (e.g., practice effects).\n\n\n\n\n\nIllustrating test-retest reliability. Top-left and top-right panels show the correlation between observed scores at both time-points for a measure that has low and high reliability, respectively. Bottom-left and bottom-right panels show the within-person change from time-point 1 to time-point 2 for scores with low and high reliability, respectively.\n\n\n\n\n\n\n5.4.4 Calculating Test-Retest Reliability in R\nLets calculate test-retest reliability in R. First, we can simulate observed scores at two time points, xTime1 and xTime2. We can assume that the true scores remain constant between time points. Second, we can calculate the correlation between the observed scores at each time point (rxx).\n\n# set seed\nset.seed(1)\n\n# set sample size\nn = 100\n\n# simulate true scores\nT_score = rnorm(n,0,1)\n\n# simulate scores at time 1\nxTime1 = T_score + rnorm(n,0,.5)\n\n# simulate scores at time 2\nxTime2 = T_score + rnorm(n,0,.5)\n\n# calculate test-retest reliability\nrxx = cor(xTime1,xTime2)\n\n# display reliability\nprint(round(rxx,3))\n\n[1] 0.755\n\n# compare with true reliability\nrxx_true = var(T_score) / var(xTime1)\n\n# display actual reliability\nprint(round(rxx_true,3))\n\n[1] 0.779\n\n\n\n\n5.4.5 Sources of Measurement Error\nMeasurement error variance can itself be broken down into multiple sources of error (e.g., transient, ). Depending on the study, different sources of error may be more relevant than others. It is important for a researcher to choose the right reliability estimator for their study since they account for different sources of measurement error. A description of four of the most common sources of error is adapted from table 1 of Wiernik and Dahlke (2020):\n\nRandom Response Error: Genuine randomness in responses. Examples include: motor errors and variation in response time.\nTime/Environment-Specific (Transient) Error: Fluctuations in scores as a result of the specific time or environment of the measurement. For instance, if researchers administered an ability test to a sample of undergraduate students throughout the course of a day, the student’s who complete the test at the end of the day will likely perform worse than participant’s who completed due to fatigue rather than ability. Errors due to illness, mood, hunger, environmental distractors, etc. all fall under the umbrella of transient errors.\nInstrument-Specific Error: Error due to the specific content or make-up of the measurement instrument. For example, a psychological scale using Likert items may show participant’s idiosyncratic interpretations of questions and response options rather than their standing on the latent construct.\nRater/Observer-Specific Error: Errors induced by idiosyncratic biases of individual raters and rater by ratee interactions (e.g., Teacher A gives higher grades to students who stay after class).\n\nDifferent estimators of reliability account for different sources of measurement error therefore depending on the research design, it is important to carefully choose which reliability is most relevant for your use case. Note that even if two estimators account for the same types of measurement error, they likely hold different assumptions that may be violated in a given research context.\n\nList of reliability coefficients and the sources of error they account for. The sources of error are denoted by the columns labeled 1-4, corresponding to each of the four sources of error.\n\n\n\n\n\n\n\n\n\n\nEstimator\nDescription\n1\n2\n3\n4\n\n\n\n\nCoefficient Alpha\nInternal consistency coefficient for composite measures.\nX️\n\nX️\n\n\n\nCoefficient Omega\nInternal consistency coefficient for composite measures with specified factor structure.\nX️\n\nX️\n\n\n\nSplit-Half\nInternal consistency coefficient for measurements that are split into two halves.\nX️\n\nX️\n\n\n\nKuder-Richardson 20\nInternal consistency when observed scores are binary (special case of coefficient alpha).\nX️\n\nX️\n\n\n\nItem Response Theory Reliability\nReliability coefficient derived from item response theory (as opposed to classical test theory)\nX️\n\nX️\n\n\n\nInter-Rater/Inter-Observer Reliability\nConsistency in scoring between raters/observers.\nX️\n\n\nX️\n\n\nTest-Retest\nStability coefficient for repeated measurements across time\nX️\nX️\n\n\n\n\nDelayed Coefficient Alpha\nAverage of all possible split-half reliabilities\nX️\nX️\nX️\n\n\n\nG-Coefficient\nReliability coefficient derived from generalizability theory (G-theory). Can incorporate any source of error if enough data is present.\nX️\nX️\nX️\nX️"
  },
  {
    "objectID": "unreliability.html#correction-for-bias-in-correlations",
    "href": "unreliability.html#correction-for-bias-in-correlations",
    "title": "5  Unreliability",
    "section": "5.5 Correction for Bias in Correlations",
    "text": "5.5 Correction for Bias in Correlations\n\n5.5.1 Defining the Estimand\nContinuing with our emphasis on clearly defining our quantity of interest (i.e., the estimand) prior to applying any corrections, let us define it. Our estimand here is the population correlation between true scores of our independent and dependent variables. We can define the observed scores of the independent and dependent variables \\(x\\) and \\(y\\) as,\n\\[\nx=T+e_x\n\\]\n\\[\ny=U+e_y.\n\\]\nWhere \\(T\\) and \\(U\\) are the true scores for the independent and dependent variables, respectively. The true score correlation can thus be be denoted by, \\(\\rho_{TU}\\), and can be defined as the standardized covariance,\n\\[\n\\rho_{TU} = \\frac{\\sigma_{TU}}{\\sigma_{T}\\sigma_{U}}.\n\\]\nIn a given study, we will only have knowledge of the observed scores of the independent and dependent variables, \\(x\\) and \\(y\\), therefore the observed study correlation is \\(r_{xy}\\). The relationship between the observed correlation and the true population correlation is,\n\\[\nr_{xy} = a\\rho_{TU} + \\varepsilon.\n\\]\nWhere \\(a\\) is the biasing factor. An unbiased estimate of the true score population correlation (\\(\\rho_{TU}\\)) can then be calculated by dividing the obsereved study correlation by the biasing factor,\n\\[\nr_c = \\frac{r_{xy}}{a}.\n\\]\n\n\n\nThis figure shows the relationship between the true scores, observed scores, and error scores. The true score correlation is denoted by the curved arrow connecting the circles indicating true score variables, T and U.\n\n\n\n\n5.5.2 Artifactual Bias and Correction\nMeasurement error induces systematic bias in effect size estimates such as correlation coefficients Spearman (1904). In the population, let us assume there is some factor \\(a\\) that accounts for the systematic bias in observed score correlations (\\(\\rho_{xy}\\)) relative to true score correlations (\\(\\rho_{TU}\\)), such that\n\\[\n\\rho_{xy} = a \\rho_{TU}.\n\\]\nSince the correlation is defined as the covariance standardized by the standard deviations, the population correlation between true scores, \\(T\\) and \\(U\\), is defined as,\n\\[\n\\rho_{TU}=\\frac{\\sigma_{TU}}{\\sigma_{T} \\sigma_{U}}.\n\\]\nLikewise the correlation between the observed scores, \\(x\\) and \\(y\\), would be the observed covariance divided by the observed standard deviations. \\[\n\\rho_{xy} =\\frac{\\sigma_{xy}}{\\sigma_{x} \\sigma_{y}}.\n\\] However, if we assume that there is no covariance between errors in \\(x\\) and \\(y\\) (\\(\\sigma_{e_x e_y} = 0\\)), then the covariance between observed scores is only attributable to the covariance between true scores, therefore \\(\\sigma_{xy} = \\sigma_{TU}\\). This means that the observed score correlation can be expressed as,\n\\[\n\\rho_{xy} =\\frac{\\sigma_{TU}}{\\sigma_{x} \\sigma_{y}}.\n\\tag{5.6}\\]\nNow the only difference between the observed score correlation and the true score correlation is the standard deviations in the denominator. In the presence of measurement error, the observed score standard deviations (\\(\\sigma_x\\) and \\(\\sigma_y\\)) will be larger than the true score standard deviations (\\(\\sigma_{T}\\) and \\(\\sigma_{U}\\)). Using the definition of reliability, we can show how the observed variance is inflated compared to the true variance as a function of reliability. Since the reliability is defined as the ratio of true variance to total observed variance (see Equation 5.2), we can see how reliability inflates the observed variance\n\\[\\begin{align}\n\\sigma^2_x &=\\sigma^2_{T} \\left(\\frac{\\sigma^2_{x}}{\\sigma^2_{T}} \\right)\n\\\\ &= \\sigma^2_{T}\\left(\\frac{1}{r_{xx'}} \\right)\n\\\\ &= \\frac{\\sigma^2_{T}}{r_{xx'}}.\n\\end{align}\\]\nTherefore the observed standard deviation is,\n\\[\n\\sigma_x = \\frac{\\sigma_{T}}{\\sqrt{r_{xx'}}}.\n\\tag{5.7}\\]\nIf we use the definition of an observed score correlation (Equation 5.6), then we can replace \\(\\sigma_x\\) and \\(\\sigma_y\\) with \\(\\frac{\\sigma_{T}}{\\sqrt{r_{xx'}}}\\) and \\(\\frac{\\sigma_{U}}{\\sqrt{r_{yy'}}}\\), respectively. Now we can see how the observed score correlation differs from the true score correlation:\n\\[\\begin{align}\n\\rho_{xy} &= \\frac{\\sigma_{T U}}{\\left[\\frac{\\sigma_{T}}{\\sqrt{r_{xx'}}} \\right] \\left[ \\frac{\\sigma_{U}}{\\sqrt{r_{yy'}}} \\right] }\n\\\\ &= \\frac{\\sigma_{T U}}{\\sigma_{T}\\sigma_{U}} \\cdot \\sqrt{r_{yy'}}\\sqrt{r_{xx'}}\n\\\\ &= \\rho_{TU} \\sqrt{r_{yy'}}\\sqrt{r_{xx'}}\n\\end{align}\\]\nThis attenuation formula was first derived by Spearman (1904). Note that this formulation requires that there is no correlation between \\(e_x\\) and \\(e_y\\) (\\(r_{e_xe_y}=0\\)). The study observed correlation will also contain sampling error and thus can be expressed by, \\[\nr_{xy} = \\rho_{xy} + \\varepsilon_o\n\\] We can also express it in terms of our estimand, the population true score correlation (\\(\\rho_{TU}\\)),\n\\[\nr_{xy} = \\rho_{TU}\\sqrt{r_{xx'}r_{xx'}} + \\varepsilon_o.\n\\]\n\n\n\n\n\nVisualizing the attenuation of observed correlation (\\(\\rho_{xy}\\)) due to measurement error. The left panel shows a situation where only one variable (\\(x\\)) has measurement error. The observed correlation increases as a function of the true correlation \\(\\rho_{TU}\\) (darker lines indicate a higher true score correlation) and the reliability of \\(x\\) (x-axis). The right panel shows the attenuation of the correlation when both \\(x\\) and \\(y\\) variables are affected by measurement error. The darker end of the gradient shows a higher correlation, while the lighter end represents a smaller correlation (the true score correlation sits on the top where no measurement error is present, \\(r_{xx'}=r_{yy'}=1\\)).\n\n\n\n\nIt becomes apparent that if we have the reliability of \\(x\\) and \\(y\\), we can obtain an unbiased estimate of \\(\\rho_{TU}\\) by dividing both sides of the above equation by \\(\\sqrt{r_{xx'}r_{xx'}}\\) such that,\n\\[\n\\frac{r_{xy}}{\\sqrt{r_{xx'}r_{xx'}}} =\\rho_{TU} + \\frac{\\varepsilon_o}{\\sqrt{r_{xx'}r_{xx'}}}.\n\\]\nTherefore the corrected study correlation, \\(r_c\\), is defined as,\n\\[\nr_c = \\frac{r_{xy}}{\\sqrt{r_{xx'}r_{xx'}}}.\n\\]\nThe sampling error of the corrected study correlation is,\n\\[\n\\varepsilon_c = \\frac{\\varepsilon_{o}}{\\sqrt{r_{xx'}r_{xx'}}}\n\\]\nand thus the sampling variance would be,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_{o}}}{r_{xx'}r_{xx'}}.\n\\]\n\n\n5.5.3 Correcting Correlations in R\nWe can simulate continuous data that contains measurement error by using the simulate_r_sample function in the psychmeta package. Below we will simulate observed scores (x_score and y_score) and true scores (T_score and U_score).\n\n# load packages\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\n# set seed\nset.seed(1)\n\n# define parameters\nrhoTU &lt;- .5\nrxx &lt;- .8\nryy &lt;- .7\nn &lt;- 500\n\n# simulate data\ndata &lt;- simulate_r_sample(n = n, \n                          rho_mat = reshape_vec2mat(rhoTU),\n                          rel_vec = c(rxx, ryy), \n                          sr_vec = c(1, 1), \n                          var_names = c(\"x\",\"y\"))\n\n# obtain observed scrores\nx_score &lt;- data$data$observed$x\ny_score &lt;- data$data$observed$y\n\n# obtain true scores\nT_score &lt;- data$data$true$x\nU_score &lt;- data$data$true$y\n\nThen we can compute observed score (rxy).\n\n# compute observed score correlation and standard error\nrxy &lt;- cor(x_score,y_score)\n\n# compute sampling variance of observed score correlation\nvar_e_o &lt;- (1-rxy^2) / (n-2)\n\n# print results\nprint(paste0('rxy = ',round(rxy,3),'  var_e_o = ',round(var_e_o,4)))\n\n[1] \"rxy = 0.351  var_e_o = 0.0018\"\n\n\nLet us now compare the observed correlation with the true score correlation (rTU).\n\n# compute observed score correlation and standard error\nrTU &lt;- cor(T_score,U_score)\n\n# compute sampling variance of observed score correlation\nvar_e &lt;- (1-rTU^2) / (n-2)\n\n# print results\nprint(paste0('rTU = ',round(rTU,3),'  var_e = ',round(var_e,4)))\n\n[1] \"rTU = 0.463  var_e = 0.0016\"\n\n\nThe observed correlation is substantially lower than the true score correlation. In order to correct the observed score correlation, we can calculate it by hand or use the correct_r() function. Lets first correct by hand using the equations in Section 5.5.2\n\n# correct correlation coefficient\nrc &lt;- rxy / sqrt(rxx*ryy)\n\n# correct sampling variance\nvar_e_c &lt;- var_e_o / sqrt(rxx*ryy)\n\n# print results\nrbind(paste0('rc = ',round(rc,3)),\n      paste0('var_e_c = ',round(var_e_c,4)))\n\n     [,1]              \n[1,] \"rc = 0.47\"       \n[2,] \"var_e_c = 0.0024\"\n\n\nNow lets correct the correlation with the correct_r() function,\n\n# correct correlation\ncorrect_r(rxyi = rxy,\n          rxx = rxx,\n          ryy = ryy,\n          n = n)\n\nCorrelations Corrected for Measurement Error:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1  0.47    0.364    0.569 500         222\n\n\nAs we can see, the corrected correlation (\\(r_c = .470\\)) is a more accurate estimate of the true score population correlation \\(\\rho_{TU} = .500\\), than the observed score correlation (\\(r_{xy}=.351\\))."
  },
  {
    "objectID": "unreliability.html#correction-for-bias-in-standardized-mean-differences-d",
    "href": "unreliability.html#correction-for-bias-in-standardized-mean-differences-d",
    "title": "5  Unreliability",
    "section": "5.6 Correction for Bias in Standardized Mean Differences (d)",
    "text": "5.6 Correction for Bias in Standardized Mean Differences (d)\n\n5.6.1 Defining the Estimand\nPrior to correcting for measurement error let us define our estimand. Our estimand here is the difference in the means of group \\(A\\) and \\(B\\) with respect to the true scores of our dependent variable. We can define the observed scores of the independent and dependent variables \\(x\\) and \\(y\\) as,\n\\[\ny_A = U_A + e_A\n\\]\n\\[\ny_B = U_B + e_B.\n\\]\nWhere \\(U_A\\) and \\(U_B\\) are the true scores for group \\(A\\) and group \\(B\\), respectively. The true score standardized mean difference can thus be be denoted by, \\(\\delta_{U}\\), and can be defined as,\n\\[\n\\delta_{U} = \\frac{\\overline{U}_A - \\overline{U}_B}{\\sigma_{U_P}}.\n\\]\nWhere \\(\\overline{U}\\) is the mean of true scores for the respective group. In a given study, we only have access to the observed scores of the dependent variables, \\(y\\), therefore the observed score study standardized mean difference is \\(d_{y}\\). The relationship between the population true score standardized mean difference (\\(\\delta_U\\)) can be related to the observed study standardized mean difference with the following formulation:\n\\[\nd_y = a\\delta_U+\\varepsilon.\n\\]\nWhere \\(a\\) is the artifactual bias induced by measurement error and \\(\\varepsilon\\) denotes sampling error. To obtain an unbiased estimate of \\(\\delta_{U}\\), we can correct the observed standardized mean difference by dividing by \\(a\\),\n\\[\nd_c = \\frac{d_y}{a}.\n\\]\n\n\n5.6.2 Artifact Correction for Unreliability\nWe can calculate the standardized mean difference of the observed scores by dividing the mean difference in observed scores (\\(\\bar{y}_A-\\bar{y}_B\\)) by the pooled standard deviation (\\(\\sigma_p\\)). It is important to note that the mean of true scores and the mean of observed scores will be identical due to the fact that measurement error only affects variance in scores rather than the means. Therefore, we can express the observed standardized mean difference as,\n\\[\nd_y = \\frac{\\bar{y}_A-\\bar{y}_B}{\\sigma_{y_P}} = \\frac{\\overline{U}_A-\\overline{U}_B}{\\sigma_{y_P}}.\n\\]\nThe pooled standard deviation is a weighted average of the observed score standard deviations,\n\\[\n\\sigma_{y_P}=\\sqrt{\\frac{(n_A+1)\\sigma^2_{y_A}+(n_B+1)\\sigma^2_{y_B}}{n_A+n_B-2}}.\n\\]\nTo express \\(\\sigma_{y_P}\\) in terms of the true score standard deviations, we can replace the observed score standard deviations with the attenuated true score standard deviation in Equation 5.7,\n\\[\n\\sigma_{y_P} = \\sqrt{\\frac{(n_A+1)\\left(\\frac{\\sigma^2_{U_A}}{r_{yy'_A}}\\right)+(n_B+1)\\left(\\frac{\\sigma^2_{U_B}}{r_{yy'_B}}\\right)}{n_A + n_B - 2}}.\n\\]\nAlternatively, we can pool the reliability and the true score standard deviations separately so that we can obtain a simplified version of the above equation,\n\\[\n\\sigma_{U_P} = \\sqrt{\\frac{(n_A+1)\\sigma_{U_A}^2+(n_B+1)\\sigma_{U_B}^2}{n_A + n_B - 2}}\n\\]\n\\[\nr_{yy'_P} = \\sqrt{\\frac{(n_A+1) r_{yy'_A}^2+(n_B+1)r_{yy'_B}^2}{n_A + n_B - 2}}.\n\\]\nThen we can express \\(\\sigma_{y_P}\\) similarly to Equation 5.7,\n\\[\n\\sigma_{y_P} = \\frac{\\sigma_{U_P}}{\\sqrt{r_{yy'_P}}}\n\\]\nNow we can put it all together and see how the observed score standardized mean difference (\\(\\delta_y\\)) is biased relative to the true score standardized mean difference (\\(\\delta_U\\)),\n\\[\\begin{align}\n\\delta_y &= \\frac{\\bar{y}_A-\\bar{y}_B}{\\sigma_{y_P}}\n\\\\[1em] &= \\frac{\\overline{U}_A-\\overline{U}_B}{\\sigma_{y_P}}\n\\\\[1em] &= \\frac{\\overline{U}_A-\\overline{U}_B}{\\frac{\\sigma_{U_P}}{\\sqrt{r_{yy'_P}}}}\n\\\\[1em] &= \\frac{\\overline{U}_A-\\overline{U}_B}{\\sigma_{U_P}}\\sqrt{r_{yy'_P}}\n\\\\[1em] &= \\delta_U\\sqrt{r_{yy'_P}}.\n\\end{align}\\]\nThis attenuation bias is very similar to the one we saw in the correlation, with the only difference being that the pooled reliability is used here instead of the total sample reliability. Within a study, the observed study standardized mean difference (\\(d_y\\)) is a function of the observed population standardized mean (\\(\\delta_y\\)), artifact bias (\\(a\\)), and sampling error (\\(\\varepsilon\\)),\n\\[\nd_y = \\delta_y + \\varepsilon_o.\n\\]\nreplacing the observed population standardized mean difference, \\(\\delta_y\\), with \\(\\delta_U\\sqrt{r_{yy'_P}}\\), gives us,\n\\[\nd_y = \\delta_U\\sqrt{r_{yy'_P}} + \\varepsilon.\n\\] Therefore to obtain the corrected study standardized mean difference (\\(d_c\\)) we can divide \\(d_y\\) by the attenuation factor,\n\\[\nd_c = \\frac{d_y}{\\sqrt{r_{yy'_P}}}.\n\\]\nWhere the sampling variance of the corrected SMD must also be similarly adjusted,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o}}{r_{yy'_P}}.\n\\]\nAlthough the attenuation factor is quite simple, in more complex formulations (e.g., bivariate direct range restriction), it will be easier to apply a simplified correction for the sampling variance using the corrected correlation coefficient:\n\\[\n\\sigma^2_{\\varepsilon_c} = \\sigma^2_{\\varepsilon_o}\\left(\\frac{d_c}{d_y}\\right)^2.\n\\] It is important to point out that this correction can only be done if when estimates of the within-group reliability are available. It is common that studies will only report the full sample reliability. If there are differences between groups on the variable, the total sample reliability will over-estimate the within-group reliability. When the total sample reliability is all that is available, to correct \\(d_y\\), we must first convert it to a point-biserial correlation coefficient (\\(r_o\\)) using the observed proportion of subjects in either group \\(A\\) or \\(B\\) (\\(p\\,\\); it does not matter which one is chosen, as long as it is consistent throughout).\n\\[\nr_o = \\frac{d_y}{\\sqrt{\\frac{1}{p(1-p)}+d_y^2}}.\n\\]\nThen correct \\(r_o\\) for the total sample reliability,\n\\[\nr_c = \\frac{r_o}{\\sqrt{r_{yy'}}}\n\\]\nThen we can convert \\(r_c\\) back into \\(d_c\\),\n\\[\nd_c = \\frac{r_c}{\\sqrt{p(1-p)(1-r_c^2)}}\n\\]\nThe same process can be done for the sampling variance as well, but instead we can put it all into one equation,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac {\\sigma^2_{\\varepsilon_o}\\left(\\frac{r_c}{r_o}\\right)^2} {\\left(1+d_y^2p[1-p]\\right)^3\\left(1-r_c^2\\right)^3}\n\\]\n\n\n5.6.3 Correcting Standardized Mean Differences in R\nWe can simulate data that contains measurement error by using the simulate_d_sample function in the psychmeta package. Below we will simulate observed scores (y_score) and true scores (U_score).\n\n# load packages\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\nset.seed(123)\n\n# define parameters\nMeans = c()\nryyA &lt;- .75\nryyB &lt;- .70\nnA &lt;- 100\nnB &lt;- 100\nn &lt;- nA + nB\n\n# simulate data\ndata&lt;- simulate_d_sample(n_vec = c(nA, nB), \n                         rho_mat_list = list(reshape_vec2mat(1),reshape_vec2mat(1)),\n                         mu_mat = rbind(c(.5,0),\n                                        c(0,0)), \n                         sigma_mat = rbind(c(1,1),\n                                           c(1,1)),\n                         rel_mat = rbind(c(ryyA,1),\n                                         c(ryyB,1)), \n                         sr_vec = c(1,1),\n                         group_names = c(\"A\", \"B\"))\n\n# obtain observed scores\ny_score &lt;- data$data$observed$y1\ngroup &lt;- data$data$observed$group\n\n# obtain true scores\nU_score &lt;- data$data$true$y1\n\nThen we can compute observed score standardized mean difference (dy).\n\n# compute observed score means and standard deviations\nMean_A &lt;- mean(y_score[group=='A'])\nMean_B &lt;- mean(y_score[group=='B'])\nSD_A &lt;- sd(y_score[group=='A'])\nSD_B &lt;- sd(y_score[group=='B'])\n\n# compute pooled standard deviation\nSD_P &lt;- sqrt( ((nA-1)*SD_A^2 + (nB-1)*SD_B^2) / (nA+nB-2) )\n\n# compute standardized mean difference\ndy &lt;- (Mean_A - Mean_B) / SD_P\n\n# compute sampling variance of observed score correlation\nvar_e_o &lt;- n/(nA*nB) + dy^2 / (2*n)\n\n# print results\nprint(paste0('dy = ',round(dy,3),'  var_e_o = ',round(var_e_o,4)))\n\n[1] \"dy = 0.273  var_e_o = 0.0202\"\n\n\nLet us now compare the observed score standardized mean difference with the true score standardized mean difference (dU).\n\n# compute true score means and standard deviations\nMean_A &lt;- mean(U_score[group=='A'])\nMean_B &lt;- mean(U_score[group=='B'])\nSD_A &lt;- sd(U_score[group=='A'])\nSD_B &lt;- sd(U_score[group=='B'])\n\n# compute pooled standard deviation\nSD_P &lt;- sqrt( ((nA-1)*SD_A^2 + (nB-1)*SD_B^2) / (nA+nB-2) )\n\n# compute standardized mean difference\ndU &lt;- (Mean_A - Mean_B) / SD_P\n\n# compute sampling variance of the true score SMD\nvar_e &lt;- n/(nA*nB) + dy^2 / (2*n)\n\n# print results\nprint(paste0('dU = ',round(dU,3),'  var_e = ',round(var_e,4)))\n\n[1] \"dU = 0.509  var_e = 0.0202\"\n\n\nThe observed score standardized mean difference is substantially lower than the true score SMD (.286 vs .509). In order to correct the observed score correlation for attenuation, we can calculate it by hand. Lets correct the observed standardized mean difference for measurement error variance using the equations in Section 5.6.2.\n\n# calculate the pooled reliability\nryy_P &lt;- sqrt(((nA-1)*ryyA^2 + (nB-1)*ryyB^2) / (nA+nB-2))\n\n# correct correlation coefficient\ndc &lt;- dy / sqrt(ryy_P)\n\n# correct sampling variance\nvar_e_c &lt;- var_e_o / sqrt(ryyA)\n\n# print results\nprint(paste0('rc = ',round(dc,3),'  var_e_c = ',round(var_e_c,4)))\n\n[1] \"rc = 0.32  var_e_c = 0.0233\"\n\n\nNow lets correct the correlation with the correct_r() function. The correct_d() function only takes in the total sample reliability, therefore we can extract the total sample reliability from the simulated dataset and then use the resulting reliability coefficient in the ryy argument.\n\n# total sample reliability\nryy = data$overall_results$observed$parallel_ryyi_total[1]\n\n# correct correlation\ncorrect_d(d = dy,\n          ryy = ryy,\n          n1 = n)\n\nd Values Corrected for Measurement Error:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.322 -0.00878    0.667 200         142\n\n\nAs we can see, the corrected correlation (\\(d_c = .32\\)) is a more accurate estimate of the true score population SMD \\(\\delta_U = .500\\), than the observed score correlation (\\(r_{xy}=.273\\))."
  },
  {
    "objectID": "unreliability.html#sec-lim-information",
    "href": "unreliability.html#sec-lim-information",
    "title": "5  Unreliability",
    "section": "5.7 Estimating Reliability with Limited Information",
    "text": "5.7 Estimating Reliability with Limited Information\nReliability estimates should preferably be calculated from within the study’s sample, however there are a couple of ways to estimate reliability when this information is not provided. A common way to obtain an estimate of the reliability is to look in meta-analyses or a test manuals. If the number of items in a study differs from the test manual, you can approximate the reliability of the study’s test, with a re-arrangement of the Spearman-Brown prophecy formula,\n\\[\nr_{xx'_{study}} \\approx \\frac{1}{\\frac{\\kappa_{\\text{ref}}}{\\kappa_{\\text{study}}} \\left(\\frac{1}{r_{xx'_{study}}} - 1\\right) + 1}.\n\\]\nWhere \\(\\kappa_{\\text{ref}}\\) and \\(\\kappa_{\\text{study}}\\) denote the number of items in the reference test and the test used in the study, respectively.\n\n\n\n\nBorsboom, Denny, and Gideon J Mellenbergh. 2002. “True Scores, Latent Variables, and Constructs: A Comment on Schmidt and Hunter.”\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap Van Heerden. 2004. “The Concept of Validity.” Psychological Review 111 (4): 1061–71. https://doi.org/10.1037/0033-295X.111.4.1061.\n\n\nCronbach, Lee J., and Paul E. Meehl. 1955. “Construct Validity in Psychological Tests.” Psychological Bulletin 52 (4): 281–302. https://doi.org/10.1037/h0040957.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75 (1949): 450–51. https://doi.org/10.1038/075450a0.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nKelley, Truman Lee. 1927. Interpretation of Educational Measurements. World Book Company.\n\n\nMendoza, Jorge L., and Michael Mumford. 1987. “Corrections for Attenuation and Range Restriction on the Predictor.” Journal of Educational Statistics 12 (3): 282–93. https://doi.org/10.3102/10769986012003282.\n\n\n“Psych: Procedures for Personality and Psychological Research.” 2017. https://CRAN.R-project.org/package=psych.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” International Journal of Epidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nViswesvaran, Chockalingam, Deniz S. Ones, Frank L. Schmidt, Huy Le, and In-Sue Oh. 2014. “Measurement Error Obfuscates Scientific Knowledge: Path to Cumulative Knowledge Requires Corrections for Unreliability and Psychometric Meta-Analyses.” Industrial and Organizational Psychology 7 (4): 507–18. https://doi.org/10.1017/S1754942600006799.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "misclassification.html#introduction",
    "href": "misclassification.html#introduction",
    "title": "6  Group Misclassification",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nGroup misclassification describes a situation where true group membership (e.g., people with a disorder) does not perfectly match the observed group membership (e.g., people diagnosed with a disorder). Group misclassification can be considered a type of measurement error where instead of accounting for errors in continuous variables (i.e., unreliability), group misclassification accounts for errors in categorical variables."
  },
  {
    "objectID": "misclassification.html#defining-group-misclassification",
    "href": "misclassification.html#defining-group-misclassification",
    "title": "6  Group Misclassification",
    "section": "6.2 Defining Group Misclassification",
    "text": "6.2 Defining Group Misclassification\nMisclassification can be defined as any deviations between true group membership and observed group membership. Let us imagine two arbitrary groups, group \\(A\\) and group \\(B\\). In order to identify members of group \\(A\\) and group \\(B\\), we have to use some measurement instrument. We can also assume this measurement instrument produces imperfect group classifications, that is, people who are actually in group \\(A\\) are sometimes assigned group \\(B\\) and vice versa. We can visualize the performance of the classification procedure with a contingency table between actual group membership (\\(G\\)) and observed group membership (\\(g\\)):\n\n\n\n\n\n\n\n\n\n\\(G=A\\)\n\\(G=B\\)\n\n\n\n\n\\(g=A\\)\n\\(AA\\)\n\\(BA\\)\n\n\n\\(g=B\\)\n\\(AB\\)\n\\(BB\\)\n\n\n\n\n\n\n\nWe can see from the contingency table that subjects who were correctly classified, would be labeled in the cell block \\(AA\\) or \\(BB\\) and those who were misclassified would belong to cells \\(BA\\) and \\(AB\\). Therefore we can define the proportion of individuals that are accurately classified as \\(p_{\\text{acc}} = P(AA) + P(BB)\\) whereas the proportion of people misclassified can be defined as \\(p_{\\text{mis}} = P(AB)+ P(BA)\\). A high-quality classifier would would minimize \\(p_{\\text{mis}}\\) and maximize \\(p_{\\text{acc}}\\). Note that the proportion of people misclassified is inversely proportional to the proportion of people accurately classified such that, \\(p_{\\text{mis}} = 1-p_{\\text{acc}}\\)."
  },
  {
    "objectID": "misclassification.html#classification-reliability",
    "href": "misclassification.html#classification-reliability",
    "title": "6  Group Misclassification",
    "section": "6.3 Classification Reliability",
    "text": "6.3 Classification Reliability\nSimilar to quantifying reliability in continuous variables by calculating the correlation in parallel sets of observed scores, the same can be done in categorical variables. Instead of a contingency table between observed (\\(g\\)) and true (\\(G\\)) group membership, we will instead create a contingency table of two measurements producing two sets of observed group assignments (\\(g\\) and \\(g'\\)). Measurements often will take the form of inter-rater assessments, for example, two clinician’s diagnosis of Major Depressive Disorder (MDD) in the same sample of patients.\n\n\n\n\n\n\n\n\n\n\\(g=A\\)\n\\(g=B\\)\n\n\n\n\n\\(g'=A\\)\n\\(AA\\)\n\\(BA\\)\n\n\n\\(g'=B\\)\n\\(AB\\)\n\\(BB\\)\n\n\n\n\n\n\n\nTo obtain the reliability of the group assignments, we can calculate the correlation coefficient between \\(g\\) and \\(g'\\). Since both variables are categorical, a Pearson correlation coefficient would not be an appropriate correlation estimator, instead, we must compute the phi coefficient. The phi coefficient is often referred to as Matthew’s correlation coefficient and is most frequently used as an index of performance of a binary classifier in machine learning. For the sake of consistency, the phi coefficient will be denoted with the letter \\(r\\), and thus the reliability (i.e., the correlation between \\(g\\) and \\(g'\\)) is denoted with \\(r_{gg'}\\).\nThere are a few ways we can calculate the phi coefficient. The first way is to calculate phi directly from the contingency table,\n\\[\nr_{gg'} = \\frac{n_{AA}n_{BB}-n_{AB}n_{BA}}{\\sqrt{(n_{AA}+n_{BA})(n_{AB}+n_{BB})(n_{AA}+n_{AB})(n_{BA}+n_{BB})}}.\n\\]\nWhere \\(n_{AA}\\), \\(n_{BB}\\), \\(n_{AB}\\), and \\(n_{BA}\\) are the number of subjects within their respective cells of the contingency table. If the values of the contingency table are not available, we can calculate the phi coefficient from the \\(\\chi^2\\)-statistic,\n\\[\nr_{gg'} = \\sqrt{\\frac{\\chi^2}{n}}.\n\\]\nWhere \\(n\\) is the total sample size. If the \\(\\chi^2\\)-statistic is unavailable, we can approximate the phi coefficient from the accuracy (\\(p_{\\text{acc}}\\)) or the proportion of people misclassified (\\(p_{\\text{mis}}\\)),\n\\[\nr_{gg'} = (2p_{\\text{acc}}-1)^2 = (1-2p_{\\text{mis}})^2.\n\\]\nThis approximation assumes that the group sizes are approximately equal and the misclassification rates are approximately equal between groups. Otherwise, \\(r_{gg'}\\) will be overestimated (Wiernik and Dahlke 2020).\nIn the chapter 5, we discussed the relationship between reliability and the correlation between observed and true scores. The classification reliability will also be related similarly to the correlation between observed group membership and true group membership (\\(r_{gG}\\)) such that,\n\\[\nr_{gG}=\\sqrt{r_{gg'}}.\n\\]"
  },
  {
    "objectID": "misclassification.html#calculating-classification-reliability-in-r",
    "href": "misclassification.html#calculating-classification-reliability-in-r",
    "title": "6  Group Misclassification",
    "section": "6.4 Calculating Classification Reliability in R",
    "text": "6.4 Calculating Classification Reliability in R\nTo calculate classification reliability we will first need data. We can simulate 100 subjects with a group value for three variables: a true group membership and two sets of assigned (observed) group membership. We will set the misclassification rate to 10%.\n\n# set seed\nset.seed(17)\n\n# 10% misclassification rate\np_mis &lt;- .10\n\n# sample size of 100\nnA &lt;- 50\nnB &lt;- 50\nn = nA + nB\n\n# create a vector of true group values\ntrue_A &lt;- rep('A',nA)\ntrue_B &lt;- rep('B',nB)\ntrue_group &lt;- c(true_A,true_B)\n\n# initialize vectors of observed group membership from true group membership\nobs_1_A &lt;- true_A\nobs_1_B &lt;- true_B\nobs_2_A &lt;- true_A\nobs_2_B &lt;- true_B\n\n# add misclassified values to observed group membership\nobs_1_A[sample(1:nA,nA*p_mis)] &lt;- 'B'\nobs_1_B[sample(1:nB,nB*p_mis)] &lt;- 'A'\nobs_2_A[sample(1:nA,nA*p_mis)] &lt;- 'B'\nobs_2_B[sample(1:nB,nB*p_mis)] &lt;- 'A'\nobs_1_group &lt;- c(obs_1_A,obs_1_B)\nobs_2_group &lt;- c(obs_2_A,obs_2_B)\n\nThen we can generate a contingency table of the two sets of observed group assignments.\n\n# create contingency table of the two observed group memberships\ncon_table &lt;- table(data.frame(obs_1=obs_1_group,\n                              obs_2=obs_2_group))\nprint(con_table)\n\n     obs_2\nobs_1  A  B\n    A 40 10\n    B 10 40\n\n\nNow we can calculate the reliability of the group assignments by extracting the phi coefficient from the contingency table. We can compute it by hand or by using the psych package by William Revelle (2017).\n\n## Strategy 1: Using the {psych} package\n# load in psych package (make sure it is installed first: install.packages('psych'))\nlibrary(psych)\nrgg = phi(con_table,digits = 3)\n\n## Strategy 2: calculate from contingency table values\nnumerator &lt;- con_table['A','A']*con_table['B','B'] - con_table['A','B']*con_table['B','A']\ndenominator &lt;- sqrt(con_table['A','A']+con_table['A','B']) *\n               sqrt(con_table['B','A']+con_table['B','B']) * \n               sqrt(con_table['A','A']+con_table['B','A']) * \n               sqrt(con_table['A','B']+con_table['B','B'])\n\nrgg &lt;- numerator / denominator\n\n## Strategy 3: calculate from chi-square test\nchi2 &lt;- as.numeric(chisq.test(con_table)$statistic)\nrgg &lt;- sqrt(chi2/n)\n\n## Strategy 4: calculate from proportion of people misclassified\nrgg &lt;- (1-2*p_mis)^2\nprint(rgg)\n\n[1] 0.64"
  },
  {
    "objectID": "misclassification.html#correcting-for-group-misclassification-in-standardized-mean-difference",
    "href": "misclassification.html#correcting-for-group-misclassification-in-standardized-mean-difference",
    "title": "6  Group Misclassification",
    "section": "6.5 Correcting for Group Misclassification in Standardized Mean Difference",
    "text": "6.5 Correcting for Group Misclassification in Standardized Mean Difference\n\n6.5.1 Defining our Estimand\nOur quantity of interest is the true population standardized mean difference, \\(\\delta\\), between members of group \\(A\\) and group \\(B\\) on the scores of the dependent variable, \\(y\\). However, the observed sample standardized mean difference (\\(d_o\\)) is estimating the difference between individuals who are assigned group to \\(A\\) and group \\(B\\). Error in the assignment of groups (i.e., group misclassification) will bias the observed correlation by a factor we will label as \\(a\\). The relationship between the observed study standardized mean difference and the true population standardized mean difference\n\\[\nd_o = a\\delta + \\varepsilon.\n\\]\nWhere \\(\\varepsilon\\) denotes the sampling error. Therefore an unbiased (corrected) estimate of the true population correlation would be:\n\\[\nr_c = \\frac{r_o}{a}.\n\\]\n\n\n6.5.2 Artifact Correction for Standardized Mean Difference\nThe standardized mean differences will become biased when subject’s assigned groups differ from their actual group. This is largely due to the fact that the means of each group are driven closer to one another. Let us suppose that, on average, group \\(A\\) and group \\(B\\) score differently on some outcome, \\(y\\). The true mean of \\(y\\) for groups \\(A\\) and \\(B\\) can be denoted as \\(\\bar{y}^\\text{true}_{A}\\) and \\(\\bar{y}^\\text{true}_{B}\\), respectively. Nonetheless, when some subjects are erroneously assigned to the wrong group, the observed mean within each group will reflect a weighted average of the respective means. This is due to the fact that the misclassified individuals are being drawn from a population with a different mean. To calculate the mean of the observed groups we must incorporate the true mean of the correctly classified subjects and the misclassified subjects,\n\\[\n\\bar{y}^\\text{obs}_A = \\left(\\frac{n_{AA}}{n_{AA}+n_{BA}}\\right)\\bar{y}^\\text{true}_A + \\left(\\frac{n_{BA}}{n_{AA}+n_{BA}}\\right)\\bar{y}^\\text{true}_B\n\\]\n\\[\n\\bar{y}^\\text{obs}_A = \\left(\\frac{n_{BB}}{n_{BB}+n_{AB}}\\right)\\bar{y}^\\text{true}_B + \\left(\\frac{n_{AB}}{n_{BB}+n_{AB}}\\right)\\bar{y}^\\text{true}_A.\n\\]\nFrom the above equations, it becomes evident that as the number of misclassified individuals increases (\\(n_{AB}\\) and \\(n_{BA}\\)), the observed means of each group gradually converge towards each other. As the means converge, the standardized mean difference will correspondingly shift toward zero. To illustrate this phenomenon, the figure below shows the distributions for groups \\(A\\) and \\(B\\) without any misclassification. In this case, there is no attenuation of the standardized mean difference.\n\n\n\n\n\nDistributions of scores without misclassification. True mean difference and observed mean differ only due to sampling error.\n\n\n\n\nIf some individual’s are assigned to the incorrect group, then we will see attenuation in the standardized mean difference as the means converge. Now lets display a figure showing what happens when the group misclassification rate is 10%. A group misclassification rate of 10% is equivalent to a classification reliability of \\(r_{gg'}=.60\\).\n\n\n\n\n\nDistributions of scores with a 10% misclassification rate. Observed standardized mean differences are biased toward the null (i.e., SMD = 0). Note that a few members of group \\(A\\) (red squares) are within observed group \\(B\\) and vice versa (indicative of misclassification).\n\n\n\n\nThe bias in the standardized mean difference can be expressed as a function of the classification reliability (\\(r_{gg'}\\)). To illuminate this bias, we must first convert the true SMD to a point-biserial correlation coefficient (\\(\\rho\\)) using the proportion of individuals in group \\(A\\) (\\(p_A\\)) and group \\(B\\) (\\(1-p\\)),\n\\[\n\\rho = \\frac{\\delta}{\\sqrt{\\frac{1}{p(1-p)}-\\delta^2}}.\n\\]\nThen attenuation of the correlation is similar to the attenuation of correlation coefficients in the section on unreliability (\\(r = \\rho\\sqrt{r_{xx'}}\\)). However in this case, we also need to convert the point-biserial correlation to the observed standardized mean difference:\n\\[\n\\delta_o =\\frac{ \\rho \\sqrt{r_{gg'}} }{\\sqrt{p (p-1)\\left(1- r_{gg'} \\rho^2\\right) }}.\n\\]\nIt is important to note that for many of the biasing effects and corrections, converting the standardized mean difference to a point-biserial correlation is often a necessary step. However once the corrected point-biserial correlation is obtained, the correlation can then be converted back into a standardized mean difference like we see in the last equation. To correct for bias induced by misclassification we first need to convert the observed standardized mean difference to a point-biserial correlation coefficient by using the observed proportion of the sample that has been assigned to either group \\(A\\) or group \\(B\\) (\\(p\\)). The group proportion \\(p\\) in the following equations will only show up in the term \\(p(1-p)\\) so it will not matter which group is used. Converting \\(d\\) to \\(r\\):\n\\[\nr_o = \\frac{d_o}{\\sqrt{\\frac{1}{p(1-p)}-d_o^2}}.\n\\]\nWe can then correct the point-biserial correlation for group misclassification with the square root of the classification reliability:\n\\[\nr_c = \\frac{r_o}{\\sqrt{r_{gg'}}}.\n\\]\nIf we also wanted to correct for measurement error in the dependent variable \\(y\\), we can use the correction formula used in the chapter 4\n\\[\nr_c = \\frac{r_o}{\\sqrt{r_{gg'}}\\sqrt{r_{yy'}}}.\n\\]\nNow we can convert the corrected point-biserial correlation into a corrected standardized mean difference (\\(d_c\\)). When converting back to a standardized mean difference, we need to use the true group proportions, \\(p^*\\). Although if we are to assume equal misclassification rates between groups, then the observed proportion can be used \\(p\\):\n\\[\n\\hat{\\delta} = \\frac{r_c}{\\sqrt{p^*\\left(1-p^*\\right)\\left(1-r_c^2\\right)}}.\n\\]\nInstead of doing the converting, correcting, and back-converting for the sampling variance, we can correct the sampling variance in a single step,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac {\\sigma^2_{\\varepsilon_o}\\left(\\frac{r_c}{r_o}\\right)^2} {\\left(1+d_o^2p[1-p]\\right)^2\\left(d_o^2+\\frac{1}{p(1-p)}\\right)p^*(1-p^*)(1-r_c^2)^3}.\n\\]\nThis can be simplified if we assume that misclassification rates are equal between groups,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac {\\sigma^2_{\\varepsilon_o}\\left(\\frac{r_c}{r_o}\\right)^2} {\\left(1+d_o^2p[1-p]\\right)^3(1-r_c^2)^3}.\n\\]"
  },
  {
    "objectID": "misclassification.html#correcting-for-misclassification-in-r",
    "href": "misclassification.html#correcting-for-misclassification-in-r",
    "title": "6  Group Misclassification",
    "section": "6.6 Correcting for Misclassification in R",
    "text": "6.6 Correcting for Misclassification in R\nWe can correct for group misclassification in R by hand or by using the psychmeta package (Dahlke and Wiernik 2019). For our correction, say we got an observed standardized mean difference of \\(d = 0.50\\) and we calculated the classification reliability to be \\(r_{gg'} = .80\\). Let us also say that the observed and the true proportion of individuals in one of the groups is \\(p=p^*=.40\\), therefore the other group would be \\(1-p=1-p^*=.60\\).\n\nd = .50\nrgg = .70\nnA = 40\nnB = 60\n\n\nUsing the psychmeta package\nThe psychmeta package has a function, correct_d, that is dedicated to correcting standardized mean differences multiple types of artifacts including group misclassification.\n\n# step 1: install and load in psychmeta\n# install.packages{'psychmeta'}\nlibrary(psychmeta)\n\n# step 2: calculate proportion of group membership\np = nA / (nA + nB)\n# p = nB / (nA + nB) # alternative calculation\n\n# step 3: correct d for group misclassification\ncorrect_d(d = d,\n          rGg = sqrt(rgg), # square root of rgg = rGg\n          correction = \"meas\",\n          pi = p,\n          pa = p,\n          n1 = nA+nB,\n          correct_bias = FALSE)\n\nd Values Corrected for Measurement Error:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.618    0.118     1.18 100        66.6\n\n\nThe output provides the corrected standardized mean difference (value), the upper and lower 95% confidence intervals (CI_LL_95 and CI_UL_95), the sample size (n), and the effective sample size (n_effective).\n\n\nCorrecting by hand\nTo calculate the corrected standardized mean difference, we can use the equations in ?sec-corrections.\n\n## Calculate point estimate\n# step 1: convert d to r\nr = d / sqrt( 1/(p*(1-p)) + d^2)\n\n# step 2: correct r\nrho = r / sqrt(rgg)\n\n# step 3: convert r to d\ndelta = rho / sqrt( p*(1-p)*(1-rho^2) )\n\n## Calculate sampling variance\n# step 1: compute sampling variance for r\nv_d = (nA+nB)/(nA*nB) + d^2 / (2*(nA+nB))\n\n# step 2: adjust sampling variance for correction\nv_delta = (v_d * (rho/r)^2) / ((1 + d^2)^3 * p*(1-p) * (1-rho^2)^3)\n\n# print corrected values\npaste0('corrected: d = ',round(delta,3),',  var = ', round(v_delta,3))\n\n[1] \"corrected: d = 0.605,  var = 0.168\"\n\n\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\n“Psych: Procedures for Personality and Psychological Research.” 2017. https://CRAN.R-project.org/package=psych.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "scale_coarseness.html#introduction",
    "href": "scale_coarseness.html#introduction",
    "title": "8  Scale Coarseness",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nScale coarseness describes a situation where a variable that is naturally continuous (e.g., happiness) is binned into discrete values (e.g., happiness measured on a scale of 1-10). This situation is quite common in the social and psychological sciences where Likert items or dichotomous yes/no responses are aggregated to form a coarse total score for a naturally continuous construct. When coarseness is present, measurement error is introduced into the observed scores and those scores lose information. Unlike dichotomization, coarseness is an artifact that occurs due to the design of the study rather than during the analysis phase (Aguinis, Pierce, and Culpepper 2009). Particularly, dichotomization occurs after scores are obtained (e.g., splitting a group into high scorers and low scorers), whereas coarseness occurs as an artifact of the measurement instrument itself. The primary issue with coarseness is that it limits the set of possible values a score can be which introduces error when the variable is naturally continuous. Lets visualize how this occurs by sampling 500 data points from a normal distribution and binning the scores into 5 equal-interval scale points. Now the figure below shows the relationship between the coarse scores and the true underlying continuous scores.\n\n\n\n\n\nNotice that the correlation between coarse and continuous scores is not perfect, indicating that the coarse scores do not perfectly capture the underlying continuous scores."
  },
  {
    "objectID": "scale_coarseness.html#correcting-for-coarseness-in-correlations",
    "href": "scale_coarseness.html#correcting-for-coarseness-in-correlations",
    "title": "8  Scale Coarseness",
    "section": "8.2 Correcting for Coarseness in Correlations",
    "text": "8.2 Correcting for Coarseness in Correlations\n\n8.2.1 Defining our Estimand\nOur quantity of interest is the population correlation, \\(\\rho\\), between continuous independent variable, \\(x\\), and continuous dependent variable, \\(y\\). We can model the relationship between the observed sample correlation on coarse scores and the true population correlation,\n\\[\nr_o = a\\rho+\\varepsilon.\n\\]\nWhere \\(a\\) is our coarseness biasing factor and \\(\\varepsilon\\) is our sampling error term. Ultimately, we can obtain an unbiased estimate of the true (continuous) population correlation by correcting the observed correlation as follows,\n\\[\nr_c = \\frac{r_o}{a}.\n\\]\n\n\n8.2.2 Artifact Correction for Coarseness\nTo correct the correlation between coarse scores for \\(x\\) and \\(y\\), we need to know the correlation between coarse scores and their underlying continuous scores. These correlations between coarse scores and their underlying continuous scores do not have a specific closed form equation as of now (Aguinis, Pierce, and Culpepper 2009). To find this we need to make a couple of assumptions:\n\nThe shape of the underlying distribution (i.e., normal or uniform).\nThe intervals between scale-points are equal.\n\nPeters and Voorhis (1940) constructed a table based on these assumptions that is also reported more recently by Aguinis, Pierce, and Culpepper (2009). The table below shows the correction factor for a given number of scale points and underlying distribution shape (either normal or uniform). The correction factor is equal to the correlation between coarse scores and the underlying continuous normal (or uniform) distribution. For the normal distribution correction, its been shown that even in cases of extreme skew, these correction factors perform well Wylie (1976).\n\n\n\n\n\n\n\n\nScale Points\nCorrection Factor (normal)\nCorrection Factor (uniform)\n\n\n\n\n2\n.816\n.866\n\n\n3\n.859\n.943\n\n\n4\n.916\n.968\n\n\n5\n.943\n.980\n\n\n6\n.960\n.986\n\n\n7\n.970\n.990\n\n\n8\n.977\n.992\n\n\n9\n.982\n.994\n\n\n10\n.985\n.995\n\n\n11\n.988\n.996\n\n\n12\n.990\n.997\n\n\n13\n.991\n.997\n\n\n14\n.992\n.997\n\n\n15\n.994\n.998\n\n\n\nThese correction factors can be applied similarly to the correction formula for reliability (see chapter 5),\n\\[\nr_c = \\frac{r_o}{a_x a_y}.\n\\]\nWhere \\(a_x\\) and \\(a_y\\) are the appropriate correction factors from the table for \\(x\\) and \\(y\\). We must also adjust the sampling variance as well,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o}}{a^2_x a^2_y}.\n\\]\n\n\n8.2.3 Correcting Correlations in R\nTo correct scale coarseness in R, we can use the table in Section 8.2.2. Lets start by simulating a coarse data (5 scale-points for x and 7 scale-points for y) set of 500 individuals with a true population correlation of \\(\\rho = .50\\).\n\n# set seed\nset.seed(343)\n\n# load packages\n# install.packages('MASS')\nlibrary(MASS)\n\n# simulate data\nn &lt;- 500\nrho &lt;- .50\ndata &lt;- mvrnorm(n = n,\n                mu = c(0,0),\n                Sigma = data.frame(x=c(1,rho),y=c(rho,1)),\n                empirical = TRUE)\n\n# obtain simulated continuous data\nx &lt;- data[,1]\ny &lt;- data[,2]\n\n# get coarse data\nx_coarse &lt;- as.numeric(cut(x,breaks=5)) \ny_coarse &lt;- as.numeric(cut(y,breaks=7)) \n\nNow we can calculate the observed standardized mean difference and apply the correction factor.\n\n# calculate observed d value\nro &lt;- cor(x_coarse,y_coarse)\n\n# correct observed do using table\nax &lt;- .943\nay &lt;- .970\n\n# correct observed d value\nrc &lt;- ro/(ax*ay)\n\n# print results\nrbind(paste0('Observed: ro = ', round(ro,3)),\n      paste0('Corrected: rc = ', round(rc,3)))\n\n     [,1]                  \n[1,] \"Observed: ro = 0.457\"\n[2,] \"Corrected: rc = 0.5\""
  },
  {
    "objectID": "scale_coarseness.html#correcting-for-coarseness-in-d-values",
    "href": "scale_coarseness.html#correcting-for-coarseness-in-d-values",
    "title": "8  Scale Coarseness",
    "section": "8.3 Correcting for Coarseness in d values",
    "text": "8.3 Correcting for Coarseness in d values\n\n8.3.1 Defining our Estimand\nOur quantity of interest is the population standardized mean difference, \\(\\delta\\), between groups \\(A\\) and \\(B\\) on variable, \\(y\\). We can model the relationship between the observed sample standardized mean difference on coarse scores and the true population standardized mean difference,\n\\[\nd_o = a\\delta+\\varepsilon.\n\\]\nWhere \\(a\\) is our coarseness biasing factor and \\(\\varepsilon\\) is our sampling error term. Ultimately, we can obtain an unbiased estimate of the true (continuous) population standardized mean difference by correcting the observed standardized mean difference as follows,\n\\[\nd_c = \\frac{d_o}{a}.\n\\]\n\n\n8.3.2 Artifact Correction for Coarseness\nTo correct a standardized mean difference for coarseness in the outcome variable, \\(y\\), we can use the correction factors from the table in Section 8.2.2,\n\\[\nd_c = \\frac{d_o}{a}.\n\\]\nWhere \\(a\\) is the appropriate correction factor from the table. We must also adjust the sampling variance as well,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sigma^2_{\\varepsilon_o}}{a^2}.\n\\]\n\n\n8.3.3 Correcting Standardized Mean Differences in R\nTo correct scale coarseness in R, we can use the table in Section 8.2.2. Lets start by simulating a coarse data (5 scale-points) set of 1000 individuals with a true population standardized mean difference of \\(\\delta = .50\\).\n\n# set seed\nset.seed(343)\n\n# simulate data\nnA &lt;- nB &lt;- 1000\nyA &lt;- rnorm(nA,.5,1)\nyB &lt;- rnorm(nB,0,1)\n\n# concatenate data\ngroup &lt;- c(rep('A',nA),rep('B',nB))\ny &lt;- c(yA,yB)\n\n# get coarse data\ny_coarse &lt;- as.numeric(cut(y,breaks=5)) \n\nNow we can calculate the observed standardized mean difference and apply the correction factor.\n\n# calculate observed d value\ndo &lt;- ( mean(y_coarse[group=='A']) - mean(y_coarse[group=='B']) ) / sqrt((var(y_coarse) + var(y_coarse))/2)\n\n# correct observed do using table\nay &lt;- .943\n\n# correct observed d value\ndc &lt;- do/ay\n\n# print results\nrbind(paste0('Observed: do = ', round(do,3)),\n      paste0('Corrected: dc = ', round(dc,3)))\n\n     [,1]                   \n[1,] \"Observed: do = 0.499\" \n[2,] \"Corrected: dc = 0.529\"\n\n\n\n\n\n\nAguinis, Herman, Charles A Pierce, and Steven A Culpepper. 2009. “Scale Coarseness as a Methodological Artifact,” September.\n\n\nPeters, Charles C., and Walter R. Van Voorhis. 1940. “Further Methods of Correlation.” In, 362–403. New York, NY, US: McGraw-Hill Book Company. https://doi.org/10.1037/13596-013.\n\n\nWylie, Peter B. 1976. “Effects of Coarse Grouping and Skewed Marginal Distributions on the Pearson Product Moment Correlation Coefficient.” Educational and Psychological Measurement 36 (1): 1–7. https://doi.org/10.1177/001316447603600101."
  },
  {
    "objectID": "direct_range_restriction.html#introduction",
    "href": "direct_range_restriction.html#introduction",
    "title": "9  Direct Selection",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nDirect selection occurs when subjects are explicitly selected based on some eligibility criterion on the variables of interest (rather than a third variable). Range restriction is a form of selection bias that describes a situation where there is less variation in our sample then there is in the population. Whereas range enhancement indicates that there is more variation in a sample then there is in the population. Direct range restriction/enhancement biases the variances and effect size estimates."
  },
  {
    "objectID": "direct_range_restriction.html#an-applied-example-of-direct-range-restriction",
    "href": "direct_range_restriction.html#an-applied-example-of-direct-range-restriction",
    "title": "9  Direct Selection",
    "section": "9.2 An Applied Example of Direct Range Restriction",
    "text": "9.2 An Applied Example of Direct Range Restriction\nImagine a tech company that wants to assess the correlation between years of experience and programming proficiency for their software engineers. They have two primary divisions: Division A and Division B. Division A primarily hires entry-level software engineers, with less than 3 years of experience. Division B, on the other hand, hires experienced software engineers with more than 3 years of experience. The company decides to conduct a study to assess the correlation between years of experience and programming proficiency. However, they only collect data from Division A due to logistical reasons, assuming that the relationship found there would be represent the entire company. In this scenario, direct range restriction occurs because the sample used for the study (Division A) represents a narrow range of years of experience (0-3 years) compared to the broader range present in the entire company (0+ years). Consequently, the standard deviation will be smaller in the sample then it would if we had sampled from the entire company. As we will see in later sections of this chapter, the observed correlation between years of experience and programming proficiency would be attenuated, underestimating the true correlation."
  },
  {
    "objectID": "direct_range_restriction.html#indexing-range-restriction-with-the-u-ratio",
    "href": "direct_range_restriction.html#indexing-range-restriction-with-the-u-ratio",
    "title": "9  Direct Selection",
    "section": "9.3 Indexing Range Restriction with the u-ratio",
    "text": "9.3 Indexing Range Restriction with the u-ratio\nThe distribution of scores in the unrestricted pool of individuals will exhibit a greater (or lesser) degree of variability compared to the sample that has been selected into the study. Therefore the standard deviation of scores in the unrestricted population (\\(\\sigma_x\\)) will differ from that of the selected (restricted/enhanced) sample (\\(\\sigma_{x_{S}}\\)). To index the difference between the two standard deviations, we can calculate the \\(u\\)-ratio Wiernik and Dahlke (2020). The \\(u\\)-ratio is the ratio between the standard deviations of the selected sample to the unrestricted sample such that,\n\\[\nu_x = \\frac{\\sigma_{x_S}}{\\sigma_x}\n\\]\nThe \\(u\\)-ratio in cases of range restriction will exist in the interval (0–1). Conversely, when the \\(u\\)-ratio is greater than 1 it is indicative of range enhancement. The unrestricted standard deviation is often quite difficult to acquire since we do not usually have access to the unrestricted group. However, the unrestricted standard deviation can be estimated from some reference study that has been conducted on the unrestricted group. This often comes in the form of standardization samples or norm samples (obtained from test manuals) if the unrestricted group is the general population. For example, the distribution full-scale IQ scores derived from the Wechsler Adult Intelligence Test has a standard deviation of 15 in the US population (Wechsler 2008). We can use this estimate as the standard deviation for the unrestricted population. Lets say we select a sample from members of Mensa, a high IQ society, who are specifically selected on the basis high IQ scores. If the standard deviation of Mensa members is 5, then the \\(u\\)-ratio would be,\n\\[\nu_x =  \\frac{\\sigma_{x_S}}{\\sigma_x} = \\frac{5}{15}= .33\\, .\n\\]\nHowever it is not always the case that an estimate of the unrestricted standard deviation is readily available. Therefore if the reliability coefficient from the unrestricted and selected sample can be used to estimate the \\(u\\)-ratio,\n\\[\nu_x = \\sqrt{\\frac{1-r_{xx'}}{1-r_{xx'_S}}}.\n\\]\nWhere \\(r_{xx'_S}\\) and \\(r_{xx'}\\) are the reliability estimates within the selected and unrestricted groups respectively."
  },
  {
    "objectID": "direct_range_restriction.html#correcting-correlations-for-direct-range-restriction",
    "href": "direct_range_restriction.html#correcting-correlations-for-direct-range-restriction",
    "title": "9  Direct Selection",
    "section": "9.4 Correcting Correlations for Direct Range Restriction",
    "text": "9.4 Correcting Correlations for Direct Range Restriction\n\n9.4.1 Defining our Estimand\nFor our study we want to estimate the population correlation of the unrestricted scores of the independent (\\(x\\)) and dependent variable (\\(y\\)). We can denote this correlation as \\(\\rho_{xy_S}\\). The restricted population correlation can be denoted as \\(\\rho_{xy_S}\\). Within a study that suffers from range restriction, the study correlation (\\(r_{xy_S}\\)) will be biased relative to our estimand, \\(\\rho_{xy}\\). This bias can be denoted by \\(a\\) such that,\n\\[\nr_{xy} = a \\rho_{xy_S} + \\varepsilon.\n\\]\nTherefore an unbiased estimate of the unrestricted population correlation would be\n\\[\nr_c = \\frac{ r_{xy_S} }{ a}.\n\\]\n\n\n9.4.2 Artifact Correction for Correlations\n\nThe Univariate Case\nRange restriction (or enhancement) in either the independent or dependent variable will induce bias into the correlation coefficient. Let us consider a case where just the independent variable is restricted (or enhanced) such that \\(u_x\\neq 1\\), but the dependent variable is not restricted (directly). It is important to note, that if there is direct selection one of the two variables, then there will be indirect selection in the other variable too if the two are correlated. This would suggest that if \\(u_x\\neq 1\\) and \\(\\rho_{xy}\\neq 0\\) then \\(u_y\\neq 1\\). Lets visualize the correlation between independent (\\(x\\)) and dependent (\\(y\\)) variables under this range restriction by only selecting individuals above some cut off. The scores of individuals that have been selected will show less variance than the entire pool of individuals. Specifically, the scenario below shows a \\(u\\)-ratio of about 0.69 in the independent variable. We see in the figure that the correlation in the restricted scores (\\(\\rho_{xy_S}\\)) is attenuated relative to the unrestricted (true) correlation (\\(\\rho_{xy}\\)).\n\n\n\n\n\nWe can also visualize what happens to the correlation when the range is enhanced. Enhancement can be accomplished by selecting individuals at the ends of the distribution (Taylor and Griess 1976). In the visualization below, we see an opposite effect on the correlation, that is, an over-estimate of the unrestricted correlation rather than an attenuation like we see under range restriction. The scenario below has a \\(u\\)-ratio of about 1.26 in the independent variable.\n\n\n\n\n\nIt starts to become apparent that if \\(u_x&gt;1\\) (i.e., \\(\\sigma_x&gt;\\sigma_{x_S}\\)) the observed correlation over-estimates the true, unrestricted correlation and under-estimates the unrestricted correlation when \\(u_x&lt;1\\) (i.e., \\(\\sigma_x&lt;\\sigma_{x_S}\\), Sackett and Yang 2000).\nA bias correction formula for univariate direct range restriction was first developed by Pearson (1903) and provided more recently by Hunter and Schmidt (1990). To correct for the systematic bias in correlations, we can use the \\(u\\)-ratio of the independent variable such that,\n\\[\nr_c = \\frac{r_{xy_S}}{u_x\\left(1+r_{xy_S}^2\\left[\\frac{1}{u^2_x}-1\\right]\\right)}\n\\tag{9.1}\\]\nWhere the sampling variance of the corrected correlation is\n\\[\n\\sigma^2_{\\varepsilon_c} = \\sigma^2_{\\varepsilon_o}\\left(\\frac{r_c}{r_{xy_S}}\\right)^2.\n\\tag{9.2}\\]\n\n\n9.4.2.1 The Bivariate Case\nBivariate direct range restriction/enhancement occurs when the variability in both independent and dependent variables within the selected sample is less than or greater than the variability in the unrestricted population. Let us consider a case where just the independent variable is restricted (or enhanced) such that \\(u_x\\neq 1\\) and \\(u_y \\neq 1\\). Like we showed for the univariate case, let’s visualize the correlation between independent (\\(x\\)) and dependent (\\(y\\)) variables under range restriction by only selecting individuals above some cut off point for both \\(x\\) and \\(y\\). The scores of individuals that have been selected will show less variance than the entire pool of individuals. Specifically, the scenario below shows a \\(u\\)-ratio of about 0.70 in the independent variable and dependent variables. We see in the figure that the correlation in the restricted sample (\\(\\rho_{xy_S}\\)) is attenuated relative to the unrestricted (true) correlation (\\(\\rho_{xy}\\)).\n\n\n\n\n\nLikewise let’s visualize what happens to the correlation when the range is enhanced. Enhancement in both variables can be accomplished by selecting individuals at the ends of the distribution of \\(x\\) and \\(y\\). In the visualization below, we observe an over-estimation of observed correlation relative to the unrestricted correlation. The scenario below has a \\(u\\)-ratio of about 1.32 in both the independent variable and dependent variable.\n\n\n\n\n\nA bias correction formula for bivariate range restriction is much more complicated than the univariate formulation. This is due to the fact that there is inter-dependence between the correlation, the \\(u\\)-ratio of \\(x\\), and the \\(u\\)-ratio of \\(y\\). For instance, if \\(x\\) and \\(y\\) are positively correlated and if there is direct range restriction in \\(x\\) this will also restrict the variability in y even if there is no range restriction in \\(y\\). To break down the correction formula into simpler parts, let us first define a factor we will denote with \\(\\psi\\),\n\\[\n\\psi = \\frac{u_x u_y\\left(r_{xy_S}^2-1\\right)}{2r_{xy_S}}\n\\tag{9.3}\\] This factor contains all the parameters needed to correct the correlation coefficient under direct selection (\\(r_{xy_S}\\)). Then we can plug it into the formula\n\\[\nr_c = \\psi + \\text{sign}\\left[r_{xy_S}\\right]\\sqrt{\\psi^2+1}\n\\tag{9.4}\\]\nWhere the sampling variance of the corrected correlation is,\n\\[\n\\sigma^2_{\\varepsilon_c} = \\sigma^2_{\\varepsilon_o}\\left(\\frac{r_c}{r_{xy_S}}\\right)^2.\n\\tag{9.5}\\]\n\n\n\n9.4.3 Correcting Correlations in R\nTo correct correlations for range restriction we can start by simulating data from the the mvrnorm function in the MASS package. Lets first simulate 200 data points.\n\n# load packages\n# install.packages('MASS')\nlibrary(MASS)\n\n# set seed\nset.seed(343)\n\n# define parameters \nrho &lt;- .50\nn &lt;- 200\n\n# sample data from a bivariate normal distribution\ndata &lt;- mvrnorm(n = n,\n                mu = c(0,0),\n                Sigma = data.frame(x = c(1,rho),\n                                  y = c(rho,1)),\n                empirical = TRUE)\n\n# obtain unrestricted scores\nx &lt;- data[,1]\ny &lt;- data[,2]\n\n\nUnivariate Direct Range Restriction\nWe can start with univariate direct range restriction by selecting only on the independent variable. We will select only the values above the mean.\n\n# obtain scores when x &gt; Mean(x)\nselected &lt;- x &gt; mean(x)\nxS &lt;- x[selected]\nyS &lt;- y[selected]\n\n# calculate correlation between unrestricted and restricted scores\nrxy &lt;- cor(x,y) # unrestricted\nrxyS &lt;- cor(xS,yS) # restricted\n\n# print results\nrbind(paste0('unrestricted: rxy = ',round(rxy,2)),\n      paste0('restricted: rxyS = ',round(rxyS,2))\n      )\n\n     [,1]                     \n[1,] \"unrestricted: rxy = 0.5\"\n[2,] \"restricted: rxyS = 0.32\"\n\n\nAs expected, we observe an attenuation of the correlation under range restriction. Now lets calculate the \\(u\\)-ratios for both variables. Remember that even though we only selected on \\(x\\), we should expect the variability in \\(y\\) in the restricted sample to also be smaller than the unrestricted sample when \\(x\\) and \\(y\\) are correlated.\n\n# calculate u-ratios\nux &lt;- sd(xS)/sd(x)\nuy &lt;- sd(yS)/sd(y)\n\n# print results\nrbind(paste0('ux = ',round(ux,2)),\n      paste0('uy = ',round(uy,2))\n      )\n\n     [,1]       \n[1,] \"ux = 0.59\"\n[2,] \"uy = 0.86\"\n\n\nAs anticipated, not only is \\(u_x\\) below 1 indicating range restriction, but also \\(u_y\\) is slightly below 1 since \\(x\\) and \\(y\\) covary. Now we can apply the correction for univariate direct range restriction by hand from Equation 10.1 and Equation 10.1.\n\n# correct the restricted correlation\nrc &lt;- rxyS / (ux * sqrt(1 + rxyS^2 * (1/ux^2-1)) )\n\n# acquire sample size from \nn &lt;- length(xS)\n\n# calculate the observed correlation sampling variance\nvar_e_o &lt;- (1-rxyS^2)^2 / (n-1)\n\n# correct sampling variance\nvar_e_c &lt;- var_e_o * (rc/rxyS)^2\n\n# print results\nrbind(paste0('corrected cor: r = ',round(rc,2)),\n      paste0('corrected var: var_e = ',round(var_e_c,3))\n      )\n\n     [,1]                         \n[1,] \"corrected cor: r = 0.49\"    \n[2,] \"corrected var: var_e = 0.02\"\n\n\nThe correction formula produced a very close estimate of the true population correlation (\\(r_c = .49\\) vs \\(\\rho_{xy}=.50\\)). Lets also correct the correlation using the correct_r function in the psychmeta package, psychmeta (Dahlke and Wiernik 2019).\n\n# load packages\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\n# correct the restricted correlation for univariate direct range restriction\ncorrect_r(rxyi = rxyS,\n          correction = 'uvdrr_x',  # uvdrr_x = univariate direct range restriction in x\n          ux = ux,\n          n = n)\n\nCorrelations Corrected for Measurement Error and Univariate Direct Range Restriction:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95  n n_effective\n1 0.492    0.209    0.685 97        40.8\n\n\nWe can see that the correction made by the correct_r function provides identical results to the one done by hand.\n\n\n9.4.3.1 Bivariate Direct Range Restriction\nFor bivariate direct range restriction we can select values above the mean in both independent and dependent variables.\n\n# obtain scores when x &gt; Mean(x) and y &gt; Mean(y)\nselected &lt;- x &gt; mean(x) & y &gt; mean(y)\nxS &lt;- x[selected]\nyS &lt;- y[selected]\n\n# calculate correlation between unrestricted and restricted scores\nrxy &lt;- cor(x,y) # unrestricted\nrxyS &lt;- cor(xS,yS) # restricted\n\n# print results\nrbind(paste0('unresticted: rxy = ',round(rxy,2)),\n      paste0('restricted: rxyS = ',round(rxyS,2))\n      )\n\n     [,1]                     \n[1,] \"unresticted: rxy = 0.5\" \n[2,] \"restricted: rxyS = 0.29\"\n\n\nNotice that there is even more attenuation in the selected correlation coefficient than there was in the univariate case. Now we can correct for bivariate range restriction by hand using Equation 9.3, Equation 9.4, Equation 9.5.\n\n# calculate the factor, psi\npsi &lt;- ux*uy*(rxyS^2-1) / (2*rxyS)\n\n# calculate the corrected correlation using psi\nrc &lt;- psi + sign(rxyS)*sqrt(psi^2 + 1)\n\n# acquire sample size from \nn &lt;- length(xS)\n\n# calculate the observed correlation sampling variance\nvar_e_o &lt;- (1-rxyS^2)^2 / (n-1)\n\n# correct sampling variance\nvar_e_c &lt;- var_e_o * (rc/rxyS)^2\n\n# print results\nrbind(paste0('corrected cor: r = ',round(rc,2)),\n      paste0('corrected var: var_e = ',round(var_e_c,3))\n      )\n\n     [,1]                          \n[1,] \"corrected cor: r = 0.48\"     \n[2,] \"corrected var: var_e = 0.036\"\n\n\nAgain, we see that the corrected correlation closely resembles the unrestricted correlation (\\(r_c=.48\\) vs \\(\\rho_{xy}=.50\\)). lets\n\n# load packages\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\n# correct the restricted correlation for univariate direct range restriction\ncorrect_r(rxyi = rxyS,\n          correction = 'bvdrr',  # bvdrr = bivariate direct range restriction\n          ux = ux,\n          uy = uy,\n          n = n)\n\nCorrelations Corrected for Measurement Error and Bivariate Direct Range Restriction:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95  n n_effective\n1  0.48   0.0943    0.689 64        17.3\n\n\nWe can see that the correction exactly reflects the correction done by hand.\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nPearson, Karl. 1903. “I. Mathematical Contributions to the Theory of Evolution. XI. On the Influence of Natural Selection on the Variability and Correlation of Organs.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 200 (321-330): 1–66. https://doi.org/10.1098/rsta.1903.0001.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nTaylor, Erwin K., and Thomas Griess. 1976. “The Missing Middle in Validation Research.” Personnel Psychology 29 (1): 5–11. https://doi.org/10.1111/j.1744-6570.1976.tb00397.x.\n\n\nWechsler, David. 2008. Wechsler Adult Intelligence Scale–Fourth Edition. 4th ed. https://doi.org/10.1037/t15169-000.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "indirect_range_restriction.html#introduction",
    "href": "indirect_range_restriction.html#introduction",
    "title": "10  Indirect Selection",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nIndirect range restriction/enhancement occurs when selection of sample participants is based on a variable that is correlated with the variables of interest. If the selector Whereas range enhancement indicates that there is more variation in a sample then there is in the population. Direct range restriction/enhancement (as opposed to indirect range restriction) is when selection into the sample is based on the variable(s) of interest (i.e., the independent and/or dependent variable). This selection into the sample will either restrict or enhance the variation in the variable, thus causing"
  },
  {
    "objectID": "indirect_range_restriction.html#an-applied-example-of-indirect-range-restriction",
    "href": "indirect_range_restriction.html#an-applied-example-of-indirect-range-restriction",
    "title": "10  Indirect Selection",
    "section": "10.2 An Applied Example of Indirect Range Restriction",
    "text": "10.2 An Applied Example of Indirect Range Restriction\nImagine a research team is conducting a study on academic motivation among college students using a survey that includes various questions related to academic engagement, goal orientation, and effort investment. The researchers administer the survey to a large sample of students across different universities. However, during the data cleaning process, the researchers identify a subset of respondents who exhibited signs of inattentiveness and carelessness in their responses. These signs include straight-lining questions (e.g., consistently selecting the same response option without reading the questions) or responding randomly without considering the content of the questions. Recognizing that inattentive or careless responding can distort the measurement of academic motivation, the researchers decide to exclude these individuals from the analysis. The rationale is to ensure that the data collected represents genuine responses and validly measures academic motivation. The unintended consequence of this decision is indirect range restriction. By removing inattentive and careless responders, who likely also have lower academic motivation and engagement, from the dataset, the observed range of academic motivation scores is reduced. The excluded individuals, who may have had lower academic motivation scores, are not accounted for in the analysis, resulting in an underestimation of the variability of academic motivation relative to the population."
  },
  {
    "objectID": "indirect_range_restriction.html#indexing-range-restriction-with-the-u-ratio",
    "href": "indirect_range_restriction.html#indexing-range-restriction-with-the-u-ratio",
    "title": "10  Indirect Selection",
    "section": "10.3 Indexing Range Restriction with the u-ratio",
    "text": "10.3 Indexing Range Restriction with the u-ratio\nThe distribution of scores in the unrestricted pool of individuals will exhibit a greater (or lesser) degree of variability compared to the sample that has been selected into the study. Therefore the standard deviation of scores in the unrestricted population (\\(\\sigma_x\\)) will differ from that of the selected (restricted/enhanced) sample (\\(\\sigma_{x_{S}}\\)). To index the difference between the two standard deviations, we can calculate the \\(u\\)-ratio Wiernik and Dahlke (2020). The \\(u\\)-ratio is the ratio between the standard deviations of the selected sample to the unrestricted sample such that,\n\\[\nu_x = \\frac{\\sigma_{x_S}}{\\sigma_x}\n\\]\nThe \\(u\\)-ratio in cases of range restriction will exist in the interval (0–1). Conversely, when the \\(u\\)-ratio is greater than 1 it is indicative of range enhancement. The unrestricted standard deviation is often quite difficult to acquire since we do not usually have access to the unrestricted group. However, the unrestricted standard deviation can be estimated from some reference study that has been conducted on the unrestricted group. This often comes in the form of standardization samples or norm samples (obtained from test manuals) if the unrestricted group is the general population. For example, the distribution full-scale IQ scores derived from the Wechsler Adult Intelligence Test has a standard deviation of 15 in the US population (Wechsler 2008). We can use this estimate as the standard deviation for the unrestricted population. Lets say we select a sample from members of Harvard students, who tend to have higher IQs than the general population (this is due to the fact that selection criterion, such as GPA and SAT scores are positively correlated with IQ). If the standard deviation of IQ in Harvard students is 10, then the \\(u\\)-ratio would be,\n\\[\nu =  \\frac{\\sigma_{x_S}}{\\sigma_x} = \\frac{10}{15}= .67\n\\]\nHowever it is not always the case that an estimate of the unrestricted standard deviation is readily available. Therefore if the reliability coefficient from the unrestricted and selected sample can be used to estimate the \\(u\\)-ratio,\n\\[\nu_x = \\sqrt{\\frac{1-r_{xx'}}{1-r_{xx'_S}}}\n\\]\nWhere \\(r_{xx'_S}\\) and \\(r_{xx'}\\) are the reliability estimates within the selected and unrestricted groups respectively. In the context of indirect range restriction, the selection does not occur directly on \\(x\\) (or \\(y\\)), instead it occurs on a selector variable, \\(z\\). Therefore we can see how the \\(u\\)-ratio of the selector variable (\\(u_z\\)) relates to the \\(u\\)-ratio of \\(x\\),\n\\[\nu_x = \\sqrt{\\rho_{xz}^2u_z^2 -\\rho_{xz}^2 + 1 }\n\\]\nThe formulation above is also dependent on the correlation between the selector, \\(z\\), and \\(x\\). If the correlation between \\(z\\) and \\(x\\) is \\(\\rho_{xz}=0\\), then you will notice that \\(u_x=u_z\\). Notice that a correlation of \\(\\rho_{xz}=1\\) would also simplify to a direct range restriction problem. A correlation of \\(\\rho_{xz}=0\\), would effectively have no selection effect (i.e., restriction nor enhancement) since the equation would simplify to \\(u_x=1\\)."
  },
  {
    "objectID": "indirect_range_restriction.html#correcting-correlations-for-direct-range-restriction",
    "href": "indirect_range_restriction.html#correcting-correlations-for-direct-range-restriction",
    "title": "10  Indirect Selection",
    "section": "10.4 Correcting Correlations for Direct Range Restriction",
    "text": "10.4 Correcting Correlations for Direct Range Restriction\n\n10.4.1 Defining our Estimand\nFor our study we want to estimate the population correlation of the unrestricted scores of the independent (\\(x\\)) and dependent variable (\\(y\\)). We can denote this correlation as \\(\\rho_{xy}\\). The population correlation within the selected sample can be denoted as \\(\\rho_{xy_S}\\). Within a study sample that suffers from indirect selection (and sampling error), the study correlation (\\(r_{xy_S}\\)) will be biased relative to our estimand, \\(\\rho_{xy}\\). This bias can be denoted by \\(a\\) such that,\n\\[\nr_{xy} = a \\rho_{xy_S} + \\varepsilon  \n\\]\nTherefore an unbiased estimate of the unrestricted population correlation would be\n\\[\nr_c = \\frac{ r_{xy_S} }{ a}.\n\\] Note that we may also want to correct for measurement on top of range restriction. In this case, using the true score model for x (\\(x=T+e_x\\)) and y (\\(y = U+e_y\\)), we may want to estimate the unrestricted correlation between true scores (\\(\\rho_{TU}\\)). The figure below shows the relationship between three variables: the selector (\\(z\\)), the independent variable (\\(x\\)), and the dependent variable (\\(y\\)). The correlation between \\(x\\) and \\(y\\) (\\(\\rho_{xy}\\)) is our estimand (i.e., the quantity we want to estimate), however as we will show, under selection on \\(z\\), the correlations between the selector and the independent/dependent variable can severely bias the observed correlation.\n\n\n\n10.4.2 Artifact Correction for Correlations\n\nThe Univariate Case\nRange restriction (or enhancement) in either the independent or dependent variable will induce bias into the correlation coefficient. Let us consider a case where we select individuals based on meeting some criterion of some third variable, \\(z\\). The extent to which selection on \\(z\\) induces restriction (or enhancement) on \\(x\\) (or \\(y\\)) depends on the correlation between \\(z\\) and \\(x\\). We can simulate an example where individuals are selected into the sample if they are above the mean of \\(z\\). Lets see how the selection process affects the distribution of \\(x\\) when we vary the correlation between \\(x\\) and \\(z\\) (\\(\\rho_{xz}\\)).\n\n\n\n\n\nNotice that the distribution of rejected and accepted participants are more similar within the plot on the left where there is a relatively low correlation between the selector variable and the independent variable. More importantly, when the correlation is lower we see wider distributions (larger standard deviation, \\(\\sigma_{x_S}\\)) than in the selected sample (and rejected), however, when the correlation is higher the standard deviation shrinks accordingly. Not only is the correlation between \\(x\\) and \\(z\\) important in the resulting variance of \\(x\\), but so is where the cut-point, that is the threshold where individuals are selected above (or below). For example, the standard deviation of \\(x\\) will be smaller when individuals are selected above the 90th percentile of \\(z\\) than when individuals are selected above the median of \\(z\\) (i.e., the 50th percentile). We can visualize this phenomenon as well by setting a low cut-point for selection (\\(z&gt;-1.0\\)) and a high cut-point for selection (\\(z&gt;0.5\\)). Notice in the figure below that the standard deviation is lower when the cut-point is higher on \\(z\\).\n\n\n\n\n\nNow lets consider a study where we want to calculate the unrestricted correlation between an independent variable, \\(x\\), and a dependent variable, \\(y\\). However, the sample is selected based on meeting some criterion on a selector, \\(z\\). Lets look at a diagram looking at the relationship between variables. We can visualize the correlation between independent (\\(x\\)) and dependent (\\(y\\)) variables under range restriction by only selecting individuals above some cut off of our selector variable, \\(z\\). The scores of individuals that have been selected will show less variance than the entire pool of individuals. Specifically, the scenario below shows a \\(u\\)-ratio of about 0.69 in the independent variable. We see in the figure that the correlation in the restricted scores (\\(\\rho_{xy_S}\\)) is attenuated relative to the unrestricted (true) correlation (\\(\\rho_{xy}\\)).\n\n\n\n\n\nWe can also visualize what happens to the correlation when the range is enhanced. Enhancement can be accomplished by selecting individuals at the ends of the distribution (Taylor and Griess 1976). In the visualization below, we see an opposite effect on the correlation, that is, an over-estimate of the unrestricted correlation rather than an attenuation like we see under range restriction. The scenario below has a \\(u\\)-ratio of about 1.44 in the independent variable.\n\n\n\n\n\nIt starts to become apparent that if \\(u_x&gt;1\\) (i.e., \\(\\sigma_x&gt;\\sigma_{x_S}\\)) the observed correlation over-estimates the true, unrestricted correlation and under-estimates the unrestricted correlation when \\(u_x&lt;1\\) (i.e., \\(\\sigma_x&lt;\\sigma_{x_S}\\), Sackett and Yang 2000). A bias correction formula for univariate direct range restriction was first developed by Pearson (1903) and provided more recently by Hunter and Schmidt (1990). To correct for the systematic bias in correlations, we can use the \\(u\\)-ratio of the independent variable such that,\n\\[\nr_c = \\frac{r_{xy_S}}{\\sqrt{r_{xy_S}^2 + u_{x}^2 (1- r_{xy_S}^2)  }}\n\\tag{10.1}\\]\nThis correction formula is only meant for observed scores only. If one wants to correct for range restriction and measurement error, we need to adjust the \\(u\\)-ratio for measurement error, as well as adjust the reliability coefficients for range restriction/enhancement. We can incorporate these adjustments into a single correction formula using the equation in table 3 of Wiernik and Dahlke (2020), \\[\nr_c=\\frac{r_{xy_S}}{\\sqrt{r_{xy_S}^2 + \\frac{u_{x}^2 r_{xx'_S}\\left(r_{xx'_S}r_{yy'_S} - r_{xy_S}^2\\right) }{1 - u_{x}^2 \\left(1-r_{xx'_S}\\right)} }}\n\\] Where \\(r_{xx'_S}\\) and \\(r_{yy'_S}\\) are the reliabilities within the selected sample. If the reliability coefficients come from the unrestricted population (e.g., from a norm/standardization sample), we can estimate the restricted reliability using the following formulas:\n\\[\nr_{xx'_S} = 1-\\frac{1-r_{xx'}}{u_x^2}\n\\tag{10.2}\\] \\[\nr_{yy'_S} = 1-\\frac{1-r_{yy'}}{u_y^2}\n\\tag{10.3}\\]\nNow once the the correlation is corrected the observed sampling variance (\\(\\sigma_{\\varepsilon_o}\\)) must also be adjusted as well. To do this, we can simply use the corrected correlation and the observed correlation to adjust the sampling variance:\n\\[\n\\sigma^2_{\\varepsilon_c} = \\sigma^2_{\\varepsilon_o}\\left(\\frac{r_c}{r_{xy_S}}\\right)^2.\n\\tag{10.4}\\]\n\n\nThe Bivariate Case\nBivariate direct range restriction/enhancement occurs when the selection variable has independent relationships with both the independent and dependent variable. Like we did for the univariate case, let’s visualize the correlation between independent (\\(x\\)) and dependent (\\(y\\)) variables under range restriction by only selecting individuals above some cut off point in our selector variable, \\(z\\). We will fix the correlations between the selector and independent variable (\\(\\rho_{xz}\\)), as well as the selector and dependent variable (\\(\\rho_{yz}\\)) to be .80. The \\(x\\) and \\(y\\) scores of individuals that have been selected above some threshold of \\(z\\) will show less variance than the entire pool of individuals. Specifically, the scenario below shows a \\(u\\)-ratio of about ~0.82 in the independent variable and dependent variables. We see in the figure below that the correlation in the restricted sample (\\(\\rho_{xy_S}=.24\\)) is attenuated relative to the unrestricted (true) correlation (\\(\\rho_{xy}=.50\\)).\n\n\n\n\n\nLikewise let’s visualize what happens to the correlation when the range is enhanced. Enhancement in both variables can be accomplished by selecting individuals at the ends of the distribution of \\(z\\). In the visualization below, we observe an over-estimation of observed correlation (\\(\\rho_{xy_S}=.75\\)) relative to the unrestricted correlation (\\(\\rho_{xy}=.50\\)). The scenario below has a \\(u\\)-ratio of about ~1.38 in both the independent variable and dependent variable.\n\n\n\n\n\nNote that bivariate restriction or bivariate enhancement will increase the amount of bias in the correlation coefficients similar to that of measurement error in both variables rather than just one (see chapter 5). A bias correction formula for bivariate range restriction is much more complicated than the univariate formulation. Particularly we need to have a basic idea of the selection mechanism at play (Dahlke and Wiernik 2019). The correction formula requires the direction of the correlation between the selector variable, \\(z\\), and the independent (\\(\\rho_{xz}\\)) and dependent variable (\\(\\rho_{yz}\\)). For example, if we run a study on a college admissions test and college academic performance (indexed by grade-point average), we can be reasonably certain that the selection (i.e., admissions decisions) are positively correlated with both academic performance and SAT scores. Other situations this may not be as straight-forward however laying out the correlational structure of the selection mechanism is vital step of the correction procedure. To break down the correction formula into simpler parts, let us first define a factor we will denote with \\(\\lambda\\). This factor is what takes into account the direction of the correlation of \\(\\rho_{xz}\\) (if positive, we can set \\(\\rho_{yz}=1\\), if negative, \\(\\rho_{yz}=-1\\), if zero, \\(\\rho_{yz}=0\\)) and \\(\\rho_{yz}\\) (repeat the same procedure as \\(\\rho_{xz}\\)).\n\\[\\begin{align}\n\\lambda =& \\text{ sign}\\left(\\rho_{xz}\\rho_{yz} [1-u_x][1-u_y]\\right)\\times \\\\\n&\\frac{\\text{sign}\n\\left(1-u_x\\right)\\min\\left(u_x,\\frac{1}{u_x}\\right) +\n\\left(1-u_x\\right)\\min\\left(u_y,\\frac{1}{u_y}\\right)\n}{\n\\min\\left(u_x,\\frac{1}{u_x}\\right)+\\min\\left(u_y,\\frac{1}{u_y}\\right)\n}\n\\end{align}\\]\nThe output of \\(\\lambda\\) will be either -1, 0, or 1. We can then plug this factor into the full correction equation that provides us with an unbiased estimate of the correlation in the unrestricted population,\n\\[\nr_c = r_{xy_S}u_xu_y+\\lambda\\sqrt{|1-u_x^2||1-u_y^2|}\n\\]\nSimilar to the univariate formula, we can also incorporate measurement error into the correction. Measurement error will bias the observed correlation on top of the bias induced by range restriction/enhancement. Therefore we can incorporate the reliabilities estimated within the restricted sample (\\(_{xx'_S}\\) and \\(r_{yy'_S}\\)), into our correction formula:\n\\[\nr_c = \\frac{r_{xy_S}u_xu_y+\\lambda\\sqrt{|1-u_x^2||1-u_y^2|}}{\\sqrt{1-u_x^2(1-r_{xx'_S})}\\sqrt{1-u_y^2(1-r_{yy'_S})}}\n\\]\nIf the reliability estimates come from an unrestricted sample, we can get estimates of the reliability coefficients in the selected sample using Equation 10.2 and Equation 10.3. We then can correct the observed sampling variance (\\(\\sigma^2_{\\varepsilon_o}\\)),\n\\[\n\\sigma^2_{\\varepsilon_c} = \\sigma^2_{\\varepsilon_o}\\left(\\frac{r_c}{r_{xy_S}}\\right)^2.\n\\]\n\n\n\n10.4.3 Correcting Correlations in R\n\nUnivariate Indirect Range Restriction\nTo correct correlations for range restriction we can start by simulating data from the the mvrnorm function in the MASS package. Lets first simulate 1000 data points. Then we will select values above the mean of the selector variable, \\(z\\).\n\n# load packages\n# install.packages('MASS')\nlibrary(MASS)\n\n# set seed\nset.seed(1)\n\n# define parameters\nrho_xy &lt;- .5\nn &lt;- 1000\n\n# simulate data\ndata &lt;- mvrnorm(n=n,\n                mu=c(0,0),\n                Sigma = reshape_vec2mat(c(rho_xy)),\n                empirical=TRUE)\nx &lt;- data[,1]\ny &lt;- data[,2]\nz &lt;- x + rnorm(n,0,.5)\nselected &lt;- z &gt; 0\n\nWe can start with univariate indirect range restriction by selecting only on the independent variable. We will select only the values above the mean.\n\n# calculate correlation between unrestricted and restricted scores\nrxy &lt;- cor(x,y) # unrestricted\nrxyS &lt;- cor(x[selected],y[selected]) # restricted\n\n# print results\nrbind(paste0('unrestricted: rxy = ',round(rxy,2)),\n      paste0('restricted: rxyS = ',round(rxyS,2))\n      )\n\n     [,1]                     \n[1,] \"unrestricted: rxy = 0.5\"\n[2,] \"restricted: rxyS = 0.39\"\n\n\nAs expected, we observe an attenuation of the correlation under range restriction. Now lets calculate the \\(u\\)-ratios for both variables. We should expect the variability not only in \\(x\\), but also \\(y\\) in the restricted sample to be smaller than the unrestricted sample. Since \\(x\\) and \\(y\\) are positively correlated, restriction \\(x\\) will restrict\n\n# calculate u-ratios\nux &lt;- sd(x[selected])/sd(x)\nuy &lt;- sd(y[selected])/sd(y)\n\n# print results\nrbind(paste0('ux = ',round(ux,2)),\n      paste0('uy = ',round(uy,2))\n      )\n\n     [,1]       \n[1,] \"ux = 0.69\"\n[2,] \"uy = 0.93\"\n\n\nNow we can apply the correction for univariate direct range restriction by hand from Equation 10.1 and Equation 10.1.\n\n# correct the restricted correlation\nrc &lt;- rxyS / ( ux*sqrt((1/ux^2-1)*rxyS^2+1 ))\n\n# acquire sample size from \nn &lt;- length(x[selected])\n\n# calculate the observed correlation sampling variance\nvar_e_o &lt;- (1-rxyS^2)^2 / (n-1)\n\n# correct sampling variance\nvar_e_c &lt;- var_e_o * (rc/rxyS)^2\n\n# print results\nrbind(paste0('corrected cor: r = ',round(rc,3)),\n      paste0('corrected var: var_e = ',round(var_e_c,3))\n      )\n\n     [,1]                          \n[1,] \"corrected cor: r = 0.52\"     \n[2,] \"corrected var: var_e = 0.003\"\n\n\nThe correction formula produced a very close estimate of the true population correlation (\\(r_c = .50\\) vs \\(\\rho_{xy}=.50\\)). Lets also correct the correlation using the correct_r function in the psychmeta package, psychmeta (Dahlke and Wiernik 2019).\n\n# load packages\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\n# correct the restricted correlation for univariate direct range restriction\ncorrect_r(rxyi = rxyS,\n          correction = 'bvirr',  # uvdrr_x = univariate direct range restriction in x\n          ux = ux,\n          uy = uy,\n          n = n)\n\nCorrelations Corrected for Measurement Error and Bivariate Indirect Range Restriction:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.523    0.474    0.568 517         220\n\n\nWe can see that the correction made by the correct_r function provides identical results to the one done by hand.\n\n\nBivariate Indirect Range Restriction\nTo correct correlations for range restriction we can start by simulating data from the the mvrnorm function in the MASS package. Lets first simulate 1000 data points. Then we will select values above the mean of the selector variable, \\(z\\).\n\n# load packages\n# install.packages('MASS')\nlibrary(MASS)\n\n# set seed\nset.seed(1)\n\n# define parameters\nrho_xz &lt;- .8\nrho_yz &lt;- .8\nrho_xy &lt;- .5\nn &lt;- 1000\n\n# simulate data\ndata &lt;- mvrnorm(n=n,\n                mu=c(0,0,0),\n                Sigma = reshape_vec2mat(c(.5,.8,.8)),\n                empirical=TRUE)\nx &lt;- data[,1]\ny &lt;- data[,2]\nz &lt;- data[,3]\nselected &lt;- z &gt; 0\n\nWe can see how the correlations are attenuated under bivariate indirect range restriction. We will select only the values above the mean.\n\n# calculate correlation between unrestricted and restricted scores\nrxy &lt;- cor(x,y) # unrestricted\nrxyS &lt;- cor(x[selected],y[selected]) # restricted\n\n# print results\nrbind(paste0('unrestricted: rxy = ',round(rxy,2)),\n      paste0('restricted: rxyS = ',round(rxyS,2))\n      )\n\n     [,1]                     \n[1,] \"unrestricted: rxy = 0.5\"\n[2,] \"restricted: rxyS = 0.18\"\n\n\nAs expected, we observe an attenuation of the correlation under range restriction. Now lets calculate the \\(u\\)-ratios for both variables. We should expect the variability in \\(x\\) and \\(y\\) in the restricted sample to be smaller than the unrestricted sample.\n\n# calculate u-ratios\nux &lt;- sd(x[selected])/sd(x)\nuy &lt;- sd(y[selected])/sd(y)\n\n# print results\nrbind(paste0('ux = ',round(ux,2)),\n      paste0('uy = ',round(uy,2))\n      )\n\n     [,1]       \n[1,] \"ux = 0.8\" \n[2,] \"uy = 0.78\"\n\n\nNow we can apply the correction for univariate direct range restriction by hand from Equation 10.1 and Equation 10.1.\n\n# calculate lambda\nrho_xz &lt;- 1 # assume a positive correlation for rho_xz\nrho_yz &lt;- 1 # assume a positive correlation for rho_yz\nlambda &lt;- sign(rho_xz*rho_yz*(1-ux)*(1-uy)) * (sign(1-ux)*min(c(ux,1/ux))+sign(1-uy)*min(c(uy,1/uy)))/(min(c(ux,1/ux))+min(c(uy,1/uy)))\n\n# correct the restricted correlation\nrc &lt;- rxyS*ux*uy + lambda*sqrt(abs(1-ux^2)*abs(1-uy^2))\n\n# acquire sample size from \nn &lt;- length(x[selected])\n\n# calculate the observed correlation sampling variance\nvar_e_o &lt;- (1-rxyS^2)^2 / (n-1)\n\n# correct sampling variance\nvar_e_c &lt;- var_e_o * (rc/rxyS)^2\n\n# print results\nrbind(paste0('corrected cor: r = ',round(rc,3)),\n      paste0('corrected var: var_e = ',round(var_e_c,3))\n      )\n\n     [,1]                          \n[1,] \"corrected cor: r = 0.493\"    \n[2,] \"corrected var: var_e = 0.014\"\n\n\nThe correction formula produced a very close estimate of the true population correlation (\\(r_c = .50\\) vs \\(\\rho_{xy}=.50\\)). Lets also correct the correlation using the correct_r function in the psychmeta package, psychmeta (Dahlke and Wiernik 2019).\n\n# load packages\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\n# correct the restricted correlation for univariate direct range restriction\ncorrect_r(rxyi = rxyS,\n          correction = 'bvirr',  # uvdrr_x = univariate direct range restriction in x\n          ux = ux,\n          uy = uy,\n          n = n)\n\nCorrelations Corrected for Measurement Error and Bivariate Indirect Range Restriction:\n---------------------------------------------------------------------------------------\n  value CI_LL_95 CI_UL_95   n n_effective\n1 0.493     0.44    0.545 500         465\n\n\nWe can see that the correction made by the correct_r function provides identical results to the one done by hand.\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nPearson, Karl. 1903. “I. Mathematical Contributions to the Theory of Evolution. XI. On the Influence of Natural Selection on the Variability and Correlation of Organs.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 200 (321-330): 1–66. https://doi.org/10.1098/rsta.1903.0001.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nTaylor, Erwin K., and Thomas Griess. 1976. “The Missing Middle in Validation Research.” Personnel Psychology 29 (1): 5–11. https://doi.org/10.1111/j.1744-6570.1976.tb00397.x.\n\n\nWechsler, David. 2008. Wechsler Adult Intelligence Scale–Fourth Edition. 4th ed. https://doi.org/10.1037/t15169-000.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "metaanalysis_intro.html#introduction",
    "href": "metaanalysis_intro.html#introduction",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nMeta-analysis is an analytic tool to synthesize quantitative evidence from multiple studies. By systematically combining and analyzing the results of multiple studies, meta-analysis provides a comprehensive overview, unveiling patterns, trends, and insights that individual studies might not be able to capture. Combining research findings also has the added benefit of increasing the precision of our results (i.e., greater statistical power). In this section we will cover the method described by (Hunter and Schmidt 1990) since it is readily compatible with artifact corrections (see next chapter). For the random-effects model however, we use an integrated approach that incorporates methods from Hunter and Schmidt (1990) and Hedges and Vevea (1998) that was first introduced by Morris et al. (2014). However it is important to note that there are other common methods to conduct meta-analyses that have their strengths and weaknesses (Hedges and Olkin 2014; Callender and Osburn 1980; Johnson, Mullen, and Salas 1995)."
  },
  {
    "objectID": "metaanalysis_intro.html#common-effect-model",
    "href": "metaanalysis_intro.html#common-effect-model",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.2 Common-Effect Model",
    "text": "11.2 Common-Effect Model\nA common effect model is the simplest form of meta-analysis. It assumes that all the variation in observed effect sizes is attributable to sampling error. In other words, all the observed effect sizes are estimates of the same population effect size. Note that there is a distinction between fixed-effects models and a common effect model (Viechtbauer, n.d.; Laird and Mosteller 1990). The common effect model assumes that the true effect size is identical for each study while the fixed effects model does not assume this. Instead, the fixed effects model can be interpreted as the weighted average of true effects. Computationally, they are the same and provide the same parameter estimates, yet the interpretation differs.\n\n\n\nThe diagram above depicts a common effect meta-analysis of five studies. The study effect sizes are homogenous and all estimate a single true population effect size.\n\n\n\n11.2.1 The General Case\nThe common effect model can be modeled such that population effect size \\(\\vartheta\\) is held constant each sample (study) effect sizes (\\(\\theta_i\\)), such that,\n\\[\n\\theta_i  = \\vartheta + \\varepsilon_i\n\\tag{11.1}\\]\nWhere \\(\\varepsilon_i\\) indicates sampling error and the subscript \\(i\\) denotes each study. Similar to the true score theory model that we discussed in chapter 4, the variance components of each term can similarly be written out as,\n\\[\n\\sigma^2_\\theta = \\sigma^2_\\vartheta + \\sigma^2_\\varepsilon\n\\]\nHowever in our fixed effects model, the population effect size is constant across studies and will not vary, simplifying the formula to,\n\\[\n\\sigma^2_\\theta = \\sigma^2_\\varepsilon\n\\tag{11.2}\\]\nTherefore the only source of variation in the observed effect sizes, is sampling error. Since sampling error varies from study to study, we can take the average sampling variance across studies to estimate \\(\\sigma^2_\\varepsilon\\):\n\\[\n\\sigma^2_\\varepsilon=\\frac{1}{k}\\sum^k_{i=1}\\sigma^2_{\\varepsilon_i}\n\\]\nUltimately, our goal is to obtain a precise estimate of the population effect size. To obtain an estimate of the population effect size, \\(\\vartheta\\), we can calculate the average observed effect size, \\(\\bar{\\theta}_i\\) from \\(k\\) studies. However, in practice, effect sizes from different studies have varying levels of precision (i.e., varying sample size). A simple average will not account for the differences between studies in their precision. Instead, we can calculate a weighted average where the weights each study can be calculated by the inverse variance (i.e., precision) of each study such that,\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_i}}\n\\]\nThen we can calculate a weighted average,\n\\[\n\\hat{\\vartheta} =\\frac{\\sum^k_{i=1}w_i\\theta_i}{\\sum^k_{i=1}w_i}\n\\]\nThis weighted average will be an unbiased estimate of the population effect size. However, even though this mean effect size is more precise compared to single-study estimates, it is not exempt from error itself. In the fixed-effects model, we can obtain the standard error of our estimate of the population effect size using,\n\\[\nSE_{\\hat{\\vartheta}} = \\sqrt{\\frac{\\sigma^2_\\varepsilon}{k}}\n\\] The standard error can be used to comput the 95% confidence intervals of the meta-analytic point estimate:\n\\[\n\\vartheta_{\\text{Lower}} = \\hat{\\vartheta}- 1.96 \\cdot SE_{\\hat{\\vartheta}}\n\\] \\[\n\\vartheta_{\\text{Upper}} = \\hat{\\vartheta}+ 1.96\\cdot SE_{\\hat{\\vartheta}}\n\\]\n\n\n11.2.2 Fixed Effects Meta-Analysis of Correlations\nTo apply the general case in the previous section to correlation coefficients, lets define our model similarly to Equation 11.1,\n\\[\nr_i=\\rho+\\varepsilon_i.\n\\]\nWhere \\(r_i\\) is our sample (study) correlation and \\(\\rho\\) is the population correlation. We can breakdown the variance components the same way as we did in the general case,\n\\[\n\\sigma^2_r = \\sigma^2_\\rho + \\sigma^2_\\varepsilon\n\\]\nFor each sample correlation, the large sample formulation for sampling variance is,\n\\[\n\\sigma^2_{\\varepsilon_i} = \\frac{\\left(1-\\rho^2\\right)^2}{n_i}\n\\tag{11.3}\\]\nNote that formulation includes the population correlation, which is unknown. Also notice that, since the population correlation is fixed, the inverse sampling variance would be proportional to the sample size (\\(1/\\sigma^2_{\\varepsilon_i} \\propto n_i\\)). For this reason, we can use the sample size as our weights. We can estimate the population correlation, \\(\\rho\\), by taking the \\(n\\)-weighted average,\n\\[\n\\hat{\\rho} = \\frac{\\sum^k_{i=1} n_i r_i}{\\sum^k_{i=1}n_i}\n\\]\nWe can use this estimate of the population correlation in the equation in Equation 11.3 to estimate the sampling variance for each study.\n\\[\n\\sigma^2_{\\varepsilon_i} = \\frac{\\left(1-\\hat{\\rho}^2\\right)^2}{n_i}\n\\] We also can acquire the standard error of our population correlation estimate (\\(\\hat{\\rho}\\)). To do so, we must first calculate the weighted average of the sampling variance from each study,\n\\[\n\\sigma_\\varepsilon^2 = \\frac{\\sum^k_{i=1} n_i \\sigma^2_{\\varepsilon_i}}{\\sum^k_{i=1}n_i}\n\\]\nthen we can calculate the standard error of the population correlation from this by dividing by the number of studies, \\(k\\),\n\\[\nSE_{\\hat{\\rho}} = \\sqrt{\\frac{\\sigma^2_{\\varepsilon}}{k}}\n\\]\nThe standard error can be used to comput the 95% confidence intervals of the meta-analytic point estimate:\n\\[\n\\rho_{\\text{Lower}} = \\hat{\\rho}- 1.96 \\cdot SE_{\\hat{\\rho}}\n\\] \\[\n\\rho_{\\text{Upper}} = \\hat{\\rho}+ 1.96\\cdot SE_{\\hat{\\rho}}\n\\]\n\n\n11.2.3 Fixed Effects Meta-Analysis of Standardized Mean Differences\nSimilar to Equation 11.1, we can model sample standardized mean differences similarly,\n\\[\nd_i = \\delta + \\varepsilon_i\n\\]\nThe most staightforward method for meta-analyzing standardized mean differences (i.e., \\(d\\) values) is to first convert all the sample \\(d\\) values to point-biserial correlations by using,\n\\[\nr_i = \\frac{d_i}{\\sqrt{\\frac{1}{p_i(1-p_i)}+d_i^2}}\n\\]\nWhere \\(p_i\\) is the observed proportion of group membership in either group \\(A\\) or group \\(B\\). The sampling variance of the study standardized mean difference is defined as\n\\[\n\\sigma^2_{\\varepsilon_di} = \\frac{n_A+n_B}{n_A n_B} + \\frac{d_i^2}{2(n_A+n_B)}\n\\]\nWhich can then be converted to the standard error of the point-biserial correlation,\n\\[\n\\sigma^2_{\\varepsilon_ri} =\\frac{\\sigma_{\\varepsilon_di}^2}{\\left(d_i^2p_i[1-p_i]+1\\right)\\left(\\frac{1}{p_i(1-p_i)}+d_i^2\\right)}\n\\]\nNote that the subscripts, \\(r\\) and \\(d\\) denote the sampling variances for correlations and \\(d\\) values respectively. Once the \\(d\\) values and sampling variances are converted to point-biserial correlations, the meta-analysis can then be conducted by using the methods from Section 11.2.2. Once the meta-analysis is completed, the estimate of the population correlation and it’s standard error can be converted back to a \\(d\\) value using the weighted average proportion of individuals in group \\(A\\) or \\(B\\) (\\(\\bar{p}\\)),\n\\[\n\\hat{\\delta} = \\frac{\\hat{\\rho}}{\\sqrt{\\bar{p}(1-\\bar{p})(1-\\hat{\\rho}^2)}}\n\\]\n\\[\nSE_{\\hat{\\delta}} = \\sqrt{\\frac{\\sigma_\\varepsilon^2}{k\\,\\bar{p}(1-\\bar{p})(1-\\hat{\\rho}^2)^3}}\n\\]\nWhere \\(k\\) is the number of studies. The standard error can be used to comput the 95% confidence intervals of the meta-analytic point estimate:\n\\[\n\\delta_{\\text{Lower}} = \\hat{\\delta}- 1.96 \\cdot SE_{\\hat{\\delta}}\n\\] \\[\n\\delta_{\\text{Upper}} = \\hat{\\delta}+ 1.96\\cdot SE_{\\hat{\\delta}}\n\\]\n\n\n11.2.4 Example from Roth et al. (2015)\nLets use a meta-analytic data set investigating the correlation of school grades and intelligence test scores from Roth (2015). This data set has correlation coefficients from \\(k=240\\) studies (total sample size: \\(n=105,151\\)) and is available within the developmental version of the psychmeta package (Dahlke and Wiernik 2019). Note that there is substantial heterogeneity in effect sizes here, far more than what could be accounted for by sampling error alone, but for the sake of this example we will assume that there is no heterogeneity. Lets conduct a common effect meta-analysis using the equations from the previous section:\n\n# Load in packages (we need the development version of psychmeta)\n# install.packages(\"devtools\")\n# devtools::install_github(\"psychmeta/psychmeta\")\nlibrary(psychmeta)\n\n# obtain data for correlations (r) amd sample size (n)\nr &lt;- data_r_roth_2015$rxyi\nn &lt;- data_r_roth_2015$n\nk &lt;- length(r)\n\n# calculate the sample size weighted average of r\nr_bar = sum(r*n) / sum(n)\n\n# calculate the sampling variance for each study\nvar_ei &lt;- (1-r_bar^2)^2 / n\n\n# estimate the mean population correlation\nrho_hat &lt;- sum(r*n) / sum(n)\n\n# calculate the variance in study correlations (r)\nvar_r &lt;- sum(n*(r-rho_hat)^2) / sum(n)\n\n# calculate average sampling variance\nvar_e &lt;- sum(n*var_ei) / sum(n)\n\n# calculate the variance in true population correlations\nvar_rho &lt;- var_r - var_e\n\n# calculate standard error of rho estimate\nSE_rho = sqrt(var_r/k)\n\n# compute confidence interval\nCI_lower = rho_hat - qnorm(.975)*SE_rho\nCI_upper = rho_hat + qnorm(.975)*SE_rho\n\n# print results\ndata.frame(rho_hat,\n           SE = SE_rho,\n           CI_lower,\n           CI_upper)\n\n    rho_hat         SE  CI_lower  CI_upper\n1 0.4413862 0.01188354 0.4180949 0.4646775\n\n\nDue to the massive sample size and the assumption that there is no variation in population correlations (i.e., fixed effects), the standard error is quite small. We can also use the metafor package (Viechtbauer 2010) to conduct a fixed effects meta-analysis without having to write each equation by hand.\n\n# install.packages(\"metafor\")\nlibrary(metafor)\n\n# fixed effects model\nmdl &lt;- rma(data = data_r_roth_2015,\n           yi = rxyi,\n           vi = var_ei,\n           method = 'EE')\n\n# print results\ndata.frame(rho_hat = mdl$b[1],\n           SE = mdl$se[1],\n           CI_lower = mdl$ci.lb[1],\n           CI_upper = mdl$ci.ub[1])\n\n    rho_hat          SE  CI_lower  CI_upper\n1 0.4413862 0.002483049 0.4365195 0.4462529"
  },
  {
    "objectID": "metaanalysis_intro.html#random-effects-model",
    "href": "metaanalysis_intro.html#random-effects-model",
    "title": "11  Introduction to Meta-Analysis Methods",
    "section": "11.3 Random Effects Model",
    "text": "11.3 Random Effects Model\nThe random-effects model refers to a model that allows for the population effect size to vary from study to study. Random-effects differs from the fixed effects model in an important way: it does not assume that all observed effect sizes come from a single (fixed) population effect size (Borenstein et al. 2010). This variation in population effect sizes is called heterogeneity. In the traditional Hunter and Schmidt (1990) the weights utilized in the random effects meta-analysis are identical to the common effect version (sample size weights). In other conventional meta-analysis methods (Hedges and Vevea 1998), random-effect weights include a random effect component containing the variation in population effect sizes (this has the effect of making study weights more similar to each other with more variation in population effects). A modern approach introduced by Morris et al. (2014) and later tested by Brannick et al. (2019), combined these two approaches. The simulation study by Brannick et al. (2019), concluded that weights incorporating random effect components improved the Hunter and Schmidt (1990) estimates. This section will thus use Morris’s method.\n\n\n\nThe diagram above depicts a random-effects meta-analysis of five studies. The study effect sizes are heterogeneous as population effect sizes vary.\n\n\n\n11.3.1 The General Case\nThe model from Equation 11.1 can be changed slightly to encompass variation of the population effect size from study to study:\n\\[\n\\theta_i = \\vartheta_i + \\varepsilon_i.\n\\]\nIn the common effect model, we assumed that all the variation in study effect sizes is accounted for by variation in sampling error (\\(\\sigma^2_\\theta = \\sigma^2_\\varepsilon\\); see Equation 11.2). However in the random-effects model the variance in population effect sizes (\\(\\sigma^2_\\vartheta\\)) is allowed to be greater than zero. The variance components can be written out as\n\\[\n\\sigma^2_\\theta=\\sigma^2_\\vartheta + \\sigma^2_\\varepsilon.\n\\tag{11.4}\\]\nThe variance of population effects, \\(\\sigma^2_\\vartheta\\), can be calculated by first calculating \\(\\sigma^2_\\theta\\) and \\(\\sigma^2_\\varepsilon\\). Since the variation in study effect sizes is no longer solely accounted for by sampling error, this would suggest that \\(\\sigma^2_\\theta \\neq \\sigma^2_\\varepsilon\\), therefore we must calculate them separately. First we need to calculate study weights using the inverse of the sampling variance and a the variance in population effect sizes (i.e., the random effect component) from each study,\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\sigma_\\vartheta^2}\n\\]\nIn order to estimate random effects component, \\(\\sigma_\\vartheta^2\\) (i.e., the variance in population effect sizes), we can calculate it by subtracting the average sampling variance (\\(\\sigma^2_\\varepsilon\\)) from the the observed variance in effect sizes (\\(\\sigma^2_\\theta\\)). The problem however is that in order to calculate the variance components, we need estimates of the population effect size and the weights, and in order to calculate the population effect size and the weights, we need the variance components. So instead, we will use sample size weights and the sample size weighted mean effect size (\\(\\bar{\\theta}\\)) as an estimate of the population correlation to estimate the weights:\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\sigma_\\vartheta^2} = \\frac{1}{\\sigma^2_{\\varepsilon_i}+(\\sigma^2_{\\theta}-\\sigma^2_{\\varepsilon})} \\approx \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\left(\\frac{\\sum^k_{i=1}n_i(\\theta_i - \\bar{\\theta})^2}{\\sum^k_{i=1}n_i}-\\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_i}}{\\sum^k_{i=1}n_i}\\right)}\n\\]\nNow with these weights, we can calculate a more precise estimate of the mean population effect size,\n\\[\n\\hat{\\bar{\\vartheta}} = \\frac{\\sum^k_{i=1}w_i\\theta_i}{\\sum^k_{i=1}w_i}\n\\]\nWith these weights and the estimate of the population effect size, we can now estimate each of the three variance components from Equation 11.4:\n\nvariance in study effect sizes \\[\n\\sigma^2_{\\theta}=\\frac{\\sum^k_{i=1}w_i(\\theta_i - \\hat{\\bar{\\vartheta}})}{\\sum^k_{i=1}w_i}\n\\]\nsampling error variance (mean) \\[\n\\sigma^2_{\\varepsilon} = \\frac{\\sum^k_{i=1}w_i\\sigma^2_{\\varepsilon_i}}{\\sum^k_{i=1}w_i}\n\\]\nvariance in true effect sizes \\[\n\\sigma^2_{\\vartheta} = \\sigma^2_{\\theta} - \\sigma^2_{\\varepsilon}\n\\]\n\nIn other conventions, \\(\\sigma^2_\\vartheta\\) is denoted as \\(\\tau^2\\) (Borenstein et al. 2010; DerSimonian and Kacker 2007; Hedges and Vevea 1998), but conceptually these are identical. Taking the root of \\(\\sigma^2_\\vartheta\\), \\(\\sigma_\\vartheta\\) is the standard deviation of population effect sizes which can be a useful measure of heterogeneity. Furthermore, we can use \\(\\sigma_\\vartheta\\) to calculate credibility (prediction) intervals which allows us to draw inferences about the range of plausible population effect sizes. For example, the 90% credibility interval can be calculated with the following equations:\n\\[\n\\vartheta_\\text{Upper} = \\hat{\\bar{\\vartheta}} + 1.645\\sigma_\\vartheta\n\\]\n\\[\n\\vartheta_\\text{Lower} = \\hat{\\bar{\\vartheta}} - 1.645\\sigma_\\vartheta\n\\]\nWe can also calculate the standard error of the mean of population effect sizes (\\(SE_{\\hat{\\bar{\\vartheta}}}\\)) by dividing the sampling error variance component by the number of studies, \\(k\\),\n\\[\nSE_{\\hat{\\bar{\\vartheta}}} = \\sqrt{\\frac{\\sigma^2_\\theta}{k}}\n\\]\nWhich can then be used to calculate 95% confidence intervals:\n\\[\n\\bar{\\vartheta}_\\text{Upper} = \\hat{\\bar{\\vartheta}} + 1.96\\cdot SE_{\\hat{\\bar{\\vartheta}}}\n\\]\n\\[\n\\bar{\\vartheta}_\\text{Lower} = \\hat{\\bar{\\vartheta}} - 1.96\\cdot SE_{\\hat{\\bar{\\vartheta}}}\n\\]\nThe confidence interval and credibility interval have fundamentally different interpretations that are often misinterpreted in published work (Whitener 1990). When we are interpreting a single realized interval (i.e., our estimate-in-hand), the 90% credibility interval can be interpreted as the region in which 90% of population effect sizes exist, however, a 95% confidence interval describes the interval in which there is a 95% probability of containing the true mean of population effect sizes. It is important to note that the confidence interval interpretation here is only valid in the case of a single realized interval (Vos and Holbert 2022), if there is more than one computed intervals the same population of studies, then the interpretation does not hold (this would be an exceedingly rare scenario in a meta-analysis).\n\n\n11.3.2 Random Effects Meta-Analysis of Correlations\nLets now specifically apply the random effects model to pearson correlation coefficients. Let us again start by defining the meta-analytic model allowing the population correlation to vary for each study,\n\\[\nr_i = \\rho_i + \\varepsilon_i\n\\]\nWhere it’s corresponding variance components are defined similarly as,\n\\[\n\\sigma^2_r = \\sigma^2_\\rho + \\sigma^2_\\varepsilon\n\\]\nLike in the general case, we must calculate the study weights using the method by Morris et al. (2014) and further described in Brannick et al. (2019). The weights are a function of the study-level sampling variance (\\(\\sigma^2_{\\varepsilon_i}\\)) and the variance in population correlations (\\(\\sigma^2_{\\rho}\\)).\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\sigma^2_{\\rho}}\n\\] However as described in the last section, to estimate the variance in population effect sizes (\\(\\sigma^2_{\\rho}\\)), we need estimates of the mean of true population effect sizes and the weights, but to get both those parameters, we need the weights. In order to get around this dilemma we can instead replace the weights with \\(n_i\\) and the mean of population correlations with the \\(n\\)-weighted average correlation (\\(\\bar{r}\\)). Lets first define the sampling variance for a pearson correlation:\n\\[\n\\sigma^2_{\\varepsilon_i} = \\frac{\\left(1-\\rho^2\\right)^2}{n_i}\n\\] Therefore we can approximate the weights with,\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\sigma_\\vartheta^2} = \\frac{1}{\\sigma^2_{\\varepsilon_i}+(\\sigma^2_{\\theta}-\\sigma^2_{\\varepsilon})} \\approx \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\left(\\frac{\\sum^k_{i=1}n_i(r_i - \\bar{r})^2}{\\sum^k_{i=1}n_i}-\\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_i}}{\\sum^k_{i=1}n_i}\\right)}\n\\]\nWith the weights we can estimate a precise estimate of the mean of population correlations (\\(\\bar{\\rho}\\))\n\\[\n\\hat{\\bar{\\rho}} = \\frac{\\sum^k_{i=1}w_ir_i}{\\sum^k_{i=1}w_i}\n\\]\nWhere the variance components can be calculated as:\n\nVariance in study correlations: \\[\n\\sigma^2_{r}=\\frac{\\sum^k_{i=1}w_i(r_i - \\hat{\\bar{\\rho}})}{\\sum^k_{i=1}w_i}.\n\\]\nSampling error variance (mean): \\[\n\\sigma^2_{\\varepsilon} = \\frac{\\sum^k_{i=1}w_i\\sigma^2_{\\varepsilon_i}}{\\sum^k_{i=1}w_i}.\n\\]\nVariance in population correlations: \\[\n\\sigma^2_\\rho = \\sigma^2_r - \\sigma^2_{\\varepsilon}.\n\\]\n\nNow lets use these variance components to calculate the 90% credibility (prediction) interval and the 95% confidence interval. The 90% credibility interval can be calculated with the following equations:\n\\[\n\\rho_\\text{Upper} = \\hat{\\bar{\\rho}} + 1.645\\sigma_\\rho\n\\]\n\\[\n\\rho_\\text{Lower} = \\hat{\\bar{\\rho}} - 1.645\\sigma_\\rho\n\\]\nWe can also calculate the standard error of the mean of population effect sizes (\\(SE_{\\hat{\\bar{\\rho}}}\\)) by dividing the sampling error variance component by the number of studies, \\(k\\),\n\\[\nSE_{\\hat{\\bar{\\rho}}} = \\sqrt{\\frac{\\sigma^2_r}{k}}\n\\]\nWhich can then be used to calculate 95% confidence intervals:\n\\[\n\\bar{\\rho}_\\text{Upper} = \\hat{\\bar{\\rho}} + 1.96\\cdot SE_{\\hat{\\bar{\\rho}}}\n\\]\n\\[\n\\bar{\\rho}_\\text{Lower} = \\hat{\\bar{\\rho}} - 1.96\\cdot SE_{\\hat{\\bar{\\rho}}}\n\\]\n\n\n11.3.3 Random Effects Meta-Analysis of Standardized Mean Differences\nWe can model sample standardized mean differences similarly to that of correlations,\n\\[\nd_i = \\delta_i + \\varepsilon_i\n\\]\nLike we did in the common effect model, instead of meta-analyzing the \\(d\\) values, we can instead convert all the sample \\(d\\) values to point-biserial correlations by using,\n\\[\nr_i = \\frac{d_i}{\\sqrt{\\frac{1}{p_i(1-p_i)}+d_i^2}}\n\\]\nWhere \\(p_i\\) is the observed proportion of group membership in either group \\(A\\) or group \\(B\\). The sampling variance of the study standardized mean difference can be defined as\n\\[\n\\sigma^2_{\\varepsilon_id} = \\frac{n_A+n_B}{n_A n_B} + \\frac{\\delta_i^2}{2(n_A+n_B)}\n\\] Where the population standardized mean difference, \\(\\delta_i\\) can be approximated with the sample size weighted mean \\(d\\) value (\\(\\bar{d}\\)).\n\\[\n\\sigma^2_{\\varepsilon_id} = \\frac{n_A+n_B}{n_A n_B} + \\frac{\\bar{d}^2}{2(n_A+n_B)}\n\\]\nWhich can then be converted to the standard error of the point-biserial correlation,\n\\[\n\\sigma^2_{\\varepsilon_ir} =\\frac{\\sigma_{\\varepsilon_id}^2}{\\left(d_i^2p_i[1-p_i]+1\\right)\\left(\\frac{1}{p_i(1-p_i)}+d_i^2\\right)}\n\\]\nThe subscripts, \\(r\\) and \\(d\\) denote the sampling variances for correlations and \\(d\\) values respectively. Once the \\(d\\) values and sampling variances are converted to point-biserial correlations, the meta-analysis can then be conducted by using the methods from Section 11.2.2. Once the meta-analysis is completed, the estimate of the population correlation and it’s standard error can be converted back to a \\(d\\) value,\n\\[\n\\hat{\\bar{\\delta}} = \\frac{\\hat{\\bar{\\rho}}}{\\sqrt{\\bar{p}(1-\\bar{p})(1-\\hat{\\bar{\\rho}}^2)}}\n\\]\n\\[\nSE_{\\hat{\\bar{\\delta}}}= \\sqrt{\\frac{\\sigma_r^2}{k\\, \\bar{p}\\left(1-\\bar{p}\\right)\\left(1-\\hat{\\bar{\\rho}}^2\\right)^3}}\n\\]\nWhere \\(k\\) is the number of studies. Likewise we can also convert the variance of the population correlations to the standard deviation of population standardized mean differences.\n\\[\n\\sigma_\\delta = \\sqrt{\\frac{\\sigma_\\rho^2}{\\bar{p}(1-\\bar{p})(1-\\hat{\\rho}^2)^3}}\n\\]\n\n\n11.3.4 Random Effects Meta-Analysis in R\nLets conduct a random effects meta-analysis using the equations from the previous section and the data set we used earlier (Roth 2015). For this dataset, it is more appropriate to use a random-effects model due to the large amount of heterogeneity we observe in the correlations.\n\n# Load in packages (we need the development version of psychmeta)\n# install.packages(\"devtools\")\n# devtools::install_github(\"psychmeta/psychmeta\")\nlibrary(psychmeta)\n\n# obtain data for correlations (r) amd sample size (n)\nr &lt;- data_r_roth_2015$rxyi\nn &lt;- data_r_roth_2015$n\nk &lt;- length(r)\n\n# calculate the sample size weighted average of r\nr_bar = sum(r*n) / sum(n)\n\n# calculate the sampling variance for each study\nvar_ei &lt;- (1-r_bar^2)^2 / n\n\n# calculate weights\nw &lt;- 1 / (var_ei + ( (sum(n*(r-r_bar)^2)/sum(n)) - (sum(n*var_ei)/sum(n)) ) )\n\n# estimate the mean population correlation\nmean_rho_hat &lt;- sum(r*w) / sum(w)\n\n# calculate the variance in study correlations (r)\nvar_r &lt;- sum(w*(r-mean_rho_hat)^2) / sum(w)\n\n# calculate average sampling variance\nvar_e &lt;- sum(w*var_ei) / sum(w)\n\n# calculate the variance in population correlations\nvar_rho &lt;- var_r - var_e\n\n# calculate standard error of rho estimate\nSE_rho = sqrt(var_r/k)\n\n# compute 95% confidence interval\nCI_lower = mean_rho_hat - qnorm(.975)*SE_rho\nCI_upper = mean_rho_hat + qnorm(.975)*SE_rho\n\n# compute 90% credibility interval\nCR_lower = mdl$b[1] - qnorm(.95)*sqrt(var_rho)\nCR_upper = mdl$b[1] + qnorm(.95)*sqrt(var_rho)\n\n# print results\ndata.frame(mean_rho_hat,\n           SE = SE_rho,\n           CI_lower,\n           CI_upper,\n           SD_rho = sqrt(var_rho),\n           CR_lower,\n           CR_upper)\n\n  mean_rho_hat         SE  CI_lower  CI_upper    SD_rho  CR_lower  CR_upper\n1    0.4265249 0.01131785 0.4043424 0.4487075 0.1601417 0.1779766 0.7047958\n\n\nNotice that the standard error of the mean correlation is larger than the common effects model. The reason for this disparity, is that the random effects model has two sources of variance, sampling error and variance in true correlations. We can also use the metafor package (Viechtbauer 2010) to conduct a random effects meta-analysis. The method used in the previous sections is not available in metafor so slight deviations with the resulting statistics may occur.\n\n# install.packages(\"metafor\")\nlibrary(metafor)\n\n# fixed effects model\nmdl &lt;- rma(data = data_r_roth_2015,\n           yi = rxyi,\n           vi = var_ei,\n           method = 'HS')\n\n# print results\ndata.frame(mean_rho_hat = mdl$b[1],\n           SE = mdl$se[1],\n           CI_LO = mdl$ci.lb[1],\n           CI_HI = mdl$ci.ub[1],\n           SD_rho = sqrt(mdl$tau2),\n           CR_LO = mdl$b[1] - qnorm(.95)*sqrt(mdl$tau2),\n           CR_HI = mdl$b[1] + qnorm(.95)*sqrt(mdl$tau2))\n\n  mean_rho_hat         SE     CI_LO     CI_HI    SD_rho     CR_LO     CR_HI\n1    0.4265249 0.01250165 0.4020222 0.4510277 0.1800354 0.1303931 0.7226568\n\n\n\n\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and Random-Effects Models for Meta-Analysis.” Research Synthesis Methods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nBrannick, Michael T., Sean M. Potter, Bryan Benitez, and Scott B. Morris. 2019. “Bias and Precision of Alternate Estimators in Meta-Analysis: Benefits of Blending Schmidt-Hunter and Hedges Approaches.” Organizational Research Methods 22 (2): 490–514. https://doi.org/10.1177/1094428117741966.\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test of a New Model for Validity Generalization.” Journal of Applied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects Model for Meta-Analysis of Clinical Trials: An Update.” NIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nHedges, Larry V., and Ingram Olkin. 2014. Statistical Methods for Meta-Analysis. Academic press. https://books.google.com/books?hl=en&lr=&id=7GviBQAAQBAJ&oi=fnd&pg=PP1&dq=info:e6P1zfh2T6QJ:scholar.google.com&ots=Dx-YqN6_9B&sig=-39HgbYdWPp_BwSTzA9cRODs2Q0.\n\n\nHedges, Larry V., and Jack L. Vevea. 1998. “Fixed- and Random-Effects Models in Meta-Analysis.” Psychological Methods 3 (4): 486–504. https://doi.org/10.1037/1082-989X.3.4.486.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nJohnson, Blair T., Brian Mullen, and Eduardo Salas. 1995. “Comparison of Three Major Meta-Analytic Approaches.” Journal of Applied Psychology 80 (1): 94–106. https://doi.org/10.1037/0021-9010.80.1.94.\n\n\nLaird, Nan M., and Frederick Mosteller. 1990. “Some Statistical Methods for Combining Experimental Results.” International Journal of Technology Assessment in Health Care 6 (1): 5–30. https://doi.org/10.1017/S0266462300008916.\n\n\nMorris, Scott, Rebecca Daisley, Megan Wheeler, and Peggy Boyer. 2014. “A Meta-Analysis of the Relationship Between Individual Assessments and Job Performance.” The Journal of Applied Psychology 100 (May). https://doi.org/10.1037/a0036938.\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A Meta-Analysis.”\n\n\nViechtbauer, Wolfgang. 2010. “Conducting meta-analyses in R with the metafor package.” Journal of Statistical Software 36 (3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\n———. n.d. “Fixed-Effects and Random-Effects Models in Meta-Analysis.” https://wviechtb.github.io/metafor/index.html.\n\n\nVos, Paul, and Don Holbert. 2022. “Frequentist Statistical Inference Without Repeated Sampling.” Synthese 200 (2): 89. https://doi.org/10.1007/s11229-022-03560-x.\n\n\nWhitener, Ellen M. 1990. “Confusion of Confidence Intervals and Credibility Intervals in Meta-Analysis.” Journal of Applied Psychology 75 (3): 315–21. https://doi.org/10.1037/0021-9010.75.3.315."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#introduction",
    "href": "artifact_correction_metaanalysis.html#introduction",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nArtifact correction meta-analysis, also referred to as psychometric meta-analysis, is a form of meta-analysis where effect sizes are systematically corrected for sources of bias. These sources of bias have been discussed in previous chapters 4-10. Methodology for conducting artifact correction style meta-analyses were originally pioneered by Frank Schmidt and John Hunter (Hunter and Schmidt 1990; Schmidt and Hunter 1977) and then reviewed more recently by Brenton Wiernik and Jeffrey Dahlke (Wiernik and Dahlke 2020). There has also been powerful R packages developed to aide in the application of artifact correction meta-analyses that we have used in previous chapters (Dahlke and Wiernik 2019). You will notice that in this section, we do not discuss standardized mean differences. This is due to the fact that the artifact correction model is designed for pearson correlations, in order to use this method for standardized mean differences, convert to pearson correlations using the methods described in chapter 11, and then use the correction methods used below. Once you apply the corrections to the converted correlations they can then be converted back to a standardized mean difference."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#bare-bones-vs-artifact-correction-meta-analysis",
    "href": "artifact_correction_metaanalysis.html#bare-bones-vs-artifact-correction-meta-analysis",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.2 Bare Bones vs Artifact Correction Meta-Analysis",
    "text": "12.2 Bare Bones vs Artifact Correction Meta-Analysis\nThis is because even if the estimates are biased relative to our estimand (i.e., the thing we are trying to estimate), the observed value still has its own population value. Chapter 11 focused on bare-bones meta-analysis, that is, meta-analyses that do not correct for biases in effect size estimates. This section will be dedicated to the artifact correction style of meta-analysis that does aim to correct for such artifactual biases. The choice between these two types of meta-analyses depends on the research question, the available data, and the assumptions researchers are willing to make. If the goal is to investigate the state of the quantitative evidence while avoiding additional assumptions about the data, then a bare-bones meta-analysis might be the way to go. On the other hand, if the goal is to obtain a more accurate estimate of the true effect size by accounting for biases induced by statistical artifacts, an artifact correction meta-analysis is preferable.\n\nBare-Bones Meta-Analysis: In a bare-bones meta-analysis, the focus is on aggregating effect sizes from various studies without explicitly correcting for potential biases in these effect size estimates.\nArtifact Correction Meta-Analysis: In contrast, an artifact correction meta-analysis takes into account and attempts to correct for biases that may be present in the effect size estimates from individual studies. This involves addressing potential sources of bias, such as measurement error or selection effects, through statistical techniques or adjustments. By doing so, the meta-analysis aims to provide a more accurate and unbiased estimate of the true effect size. Although it is important to note that this method will require additional assumptions about the nature of the data.\n\nNote that the bare-bones model does not assume that there is no bias, rather, the bare-bones model is estimating something else entirely, that is, the observed population effect size."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#individual-artifact-correction-model",
    "href": "artifact_correction_metaanalysis.html#individual-artifact-correction-model",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.3 Individual Artifact Correction Model",
    "text": "12.3 Individual Artifact Correction Model\nThe individual artifact correction model corrects each effect size individually prior to conducting the meta-analysis. This method is ideal if we have high-quality artifact estimates for most/all studies in the meta-analysis. If there is a substantial amount of missingness in the artifact values, then the artifact distribution model may be a better choice.\n\n12.3.1 The General Case\nLet us recall the random effects model in chapter 11, where \\(\\theta_i=\\vartheta_i+\\varepsilon_i\\). This model would be considered a bare-bones meta-analytic model; we can re-write it slightly to denote that these are observed values: \\(\\theta_{o_i}=\\vartheta_{o_i}+\\varepsilon_{o_i}\\). Ultimately, observed values tend to be biased relative to true values due to many artifactual factors, some that we can account for and some we can not. If we decide that corrections to observed effect sizes are necessary to answer our research question, then we can construct an artifact correction model. In the artifact correction framework, we can incorporate a compound artifact biasing factor, \\(A\\), to the bare-bones formula such that,\n\\[\n\\theta_{o_i} = A_i\\vartheta_i + \\varepsilon_{o_i}\n\\tag{12.1}\\]\nSo now instead of the model being in terms of the observed population value (\\(\\vartheta_{o_i}\\)), it is now in terms of the true population value (\\(\\vartheta_{i}\\)). The compound biasing factor, \\(A_i\\) is a product of multiple independent artifact values (e.g., unreliability and range restriction),\n\\[\nA_i = a_{1i} a_{2i} a_{3i} ...\n\\]\nThis compound artifact formula assumes that the values are independent of one another, which is not always the case, see chapter 11 on independence of artifacts. Therefore adjustments must be made to ensure independence. eq-art-mdl can be re-arranged to obtain unbiased estimates of the true population effect size:\n\\[\n\\frac{\\theta_{o_i}}{A_i} = \\vartheta_i + \\frac{\\varepsilon_{o_i}}{A_i}\n\\tag{12.2}\\]\nThis division of \\(A_i\\) will provide us with our corrected effect size estimates that we can denote with the subscript, \\(c\\),\n\\[\n\\theta_{c_i} = \\frac{\\theta_{o_i}}{A_i}\n\\]\nand the corresponding error term must also be corrected\n\\[\n\\varepsilon_{c_i} = \\frac{\\varepsilon_{o_i}}{A_i}.\n\\tag{12.3}\\]\nTherefore Equation 12.2 can be reformulated as,\n\\[\n\\theta_{c_i} = \\vartheta_i + \\varepsilon_{c_i}\n\\tag{12.4}\\]\nThese corrections cause changes in the point estimate and the error variance of the study effect sizes. Like we saw in chapter 11, we can breakdown the variance components of the model,\n\\[\n\\sigma^2_{\\theta_c} = \\sigma^2_{\\vartheta} + \\sigma^2_{\\varepsilon_c}\n\\tag{12.5}\\]\nTo obtain these variance components, we can start by correcting the observed sampling variances from each study. We can calculate the corrected sampling variance (\\(\\sigma^2_{\\varepsilon_c}\\)) by first correcting each study-level sampling variance estimate,\n\\[\n\\sigma^2_{\\varepsilon_ci} = \\frac{\\sigma^2_{\\varepsilon_oi}}{A^2_i}\n\\] This may also be done by using the corrected effect size and the observed effect size to correct the sampling variance:\n\\[\n\\sigma^2_{\\varepsilon_ci} = \\sigma^2_{\\varepsilon_oi} \\left(\\frac{\\theta_{c_i}}{\\theta_{o_i}}\\right)^2\n\\]\nThe next step is to obtain the random effects weights of the study, we can do this with the inverse corrected variance for each study, \\(w_i=1/(\\sigma^2_{\\varepsilon_ci}+\\sigma^2_\\rho)\\). From here we can calculate our estimate of the mean of true population correlations,\n\\[\n\\hat{\\bar{\\vartheta}}=\\frac{\\sum^k_{i=1}n_i\\theta_{c_i}}{\\sum^k_{i=1}n_i}\n\\]\nRemember that because this is a random effects model, \\(\\hat{\\bar{\\vartheta}}\\) is not an estimate of the true population effect size, instead it is an estimate of the mean of a distribution of true population effect sizes. Now that we have an estimate of the mean and the corrected sampling variances, the variance components from Equation 12.5 can be easily calculated as follows:\n\\[\n\\sigma^2_{\\theta_c} = \\frac{\\sum^k_{i=1}n_i(\\theta_{c_i} - \\hat{\\bar{\\vartheta}})^2}{\\sum^k_{i=1}n_i}\n\\]\n\\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_c i}}{\\sum^k_{i=1}n_i}\n\\]\n\\[\n\\sigma^2_{\\vartheta}  = \\sigma^2_{\\theta_c} - \\sigma^2_{\\varepsilon_c}\n\\]\nThe standard deviation of true effects is a useful measure of heterogeneity and is simply the square root of the variance of true population effect sizes (\\(\\sigma_{\\vartheta}\\)). From the standard deviation in true effects, we can also calculate a credibility (prediction) interval that shows the range of plausible values for which a true effect size is likely to fall,\n\\[\n\\vartheta_{\\text{Upper}} = \\hat{\\bar{\\vartheta}} + 1.645\\sigma_\\vartheta\n\\]\n\\[\n\\vartheta_{\\text{Lower}} = \\hat{\\bar{\\vartheta}} - 1.645\\sigma_\\vartheta\\, .\n\\]\nNote that this is not to be confused with confidence intervals which denotes the range of plausible values that the mean of true effects can take on. This differentiation is akin to understanding the disparity between the standard error of the mean and the standard deviation in the context of a normal distribution. We can also see how the corrections reduced the heterogeneity in the effect size estimates by comparing variance in true effect sizes (\\(\\sigma^2_{\\vartheta_o}\\)) to the variance in observed effect sizes (\\(\\sigma^2_{\\vartheta_o}\\); this can be calculated by conducting a bare-bones random effects meta-analysis described in chapter 11). The percent reduction in heterogeneity can be computed by taking the ratio of the two, \\(\\sigma^2_{\\vartheta}/\\sigma^2_{\\vartheta_o}\\). Hunter and Schmidt (1990) suggest that if 75% of the heterogeneity is accounted for by artifact corrections, then we can assume that the remaining heterogeneity is attributable to remaining artifacts that have not been addressed in the current meta-analysis. Although it is important to point out that this is simply a rule of thumb rather than a mathematical property.\n\n\n12.3.2 Individual Corrections in Correlations\nFor correlation coefficients we can define the model similarly to Equation 12.1, with the only difference being that we will use the notation for pearson correlations,\n\\[\nr_{o_i} = A_i\\rho_i + \\varepsilon_{o_i}\n\\]\nThe artifact correction formulation of this, corresponding to Equation 12.4, would be\n\\[\nr_{c_i} = \\rho_i + \\varepsilon_{c_i}\n\\]\nThe corresponding variance components would then be,\n\\[\n\\sigma^2_{r_c} = \\sigma^2_\\rho + \\sigma^2_{\\varepsilon_c}\n\\]\nIn order to compute the variance components as well as the mean true population correlation, we first need to calculate the study weights. We will follow a similar procedure for calculating random effects weights in chapter 11. Lets define the corrected random effects weights as,\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_ci}+\\sigma^2_\\rho}.\n\\] However the variance components, \\(\\sigma^2_{\\varepsilon_ci}\\) and \\(\\sigma^2_\\rho\\), require the weights themselves to actually estimate them, so instead we can approximate the variance components using the sample size as the weights such that,\n\\[\nw_i =\\frac{1}{\\sigma^2_{\\varepsilon_ci}+\\sigma_\\rho^2}= \\frac{1}{\\sigma^2_{\\varepsilon_ci}+(\\sigma^2_{r_c}-\\sigma^2_{\\varepsilon})} \\approx \\frac{1}{\\sigma^2_{\\varepsilon_ci}+\\left(\\frac{\\sum^k_{i=1}n_i(r_{c_i} - \\bar{r}_c)^2}{\\sum^k_{i=1}n_i}-\\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_ci}}{\\sum^k_{i=1}n_i}\\right)}\n\\]\nWhere \\(\\bar{r}_c\\) is the sample size weighted average corrected correlation. These weights can then be used to obtain a more precise estimate of the true population correlation,\n\\[\n\\hat{\\bar{\\rho}}=\\frac{\\sum_{i=1}^k w_i r_{c_i}}{\\sum_{i=1}^k w_i}\n\\] Now we can compute each of the three variance components:\n\nVariance in corrected correlations:\n\n\\[\n\\sigma^2_{r_c}=\\frac{\\sum^k_{i=1}w_i(r_{c_i} - \\hat{\\bar{\\rho}})}{\\sum^k_{i=1}w_i}.\n\\] 2) Sampling error variance: \\[\n\\sigma^2_{\\varepsilon_c} = \\frac{\\sum^k_{i=1}w_i\\sigma^2_{\\varepsilon_ci}}{\\sum^k_{i=1}w_i}.\n\\] 3) Variance in population correlations: \\[\n\\sigma^2_\\rho = \\sigma^2_r - \\sigma^2_{\\varepsilon}.\n\\] Now lets use these variance components to calculate the 90% credibility (prediction) interval and the 95% confidence interval. The 90% credibility interval can be calculated with the following equations:\n\\[\n\\rho_\\text{Upper} = \\hat{\\bar{\\rho}} + 1.645\\sigma_\\rho\n\\]\n\\[\n\\rho_\\text{Lower} = \\hat{\\bar{\\rho}} - 1.645\\sigma_\\rho\n\\]\nWe can also calculate the standard error of the mean of true population effect sizes (\\(SE_{\\hat{\\bar{\\rho}}}\\)) by dividing the sampling error variance component by the number of studies, \\(k\\),\n\\[\nSE_{\\hat{\\bar{\\rho}}} = \\sqrt{\\frac{\\sigma^2_{r_c}}{k}}\n\\]\nWhich can then be used to calculate 95% confidence intervals:\n\\[\n\\bar{\\rho}_\\text{Upper} = \\hat{\\bar{\\rho}} + 1.96\\cdot SE_{\\hat{\\bar{\\rho}}}\n\\]\n\\[\n\\bar{\\rho}_\\text{Lower} = \\hat{\\bar{\\rho}} - 1.96\\cdot SE_{\\hat{\\bar{\\rho}}}\n\\]\n\n\n12.3.3 Applied Example in R\nLets conduct an individual correction meta-analysis in r using the data set by Roth (2015). This data set consists of correlations between school grades and intelligence test scores. It also contains information on the reliability of the intelligence test scores and the extent of range restriction in test scores. We can conduct a meta-analysis correcting for univariate indirect range restriction and measurement error in test scores. The compound artifact biasing factor for the correlation would be: \\[\nA_i=\\sqrt{r_{o_i}^2 + \\frac{u_{x_i}^2 r_{xx'_i}(r_{xx'_i} - r_{o_i}^2) }{1 - u_{x_i}^2 (1-r_{xx'_i})} }\n\\] Sticking with our theme of doing everything in base R first, lets use the equations from the previous section to conduct the meta-analysis.\n\n# Load in packages (we need the development version of psychmeta)\n# install.packages(\"devtools\")\n# devtools::install_github(\"psychmeta/psychmeta\")\nlibrary(psychmeta)\n\n# obtain artifact values\nrxx &lt;- data_r_roth_2015$rxxi\nux &lt;- data_r_roth_2015$ux\nro &lt;- data_r_roth_2015$rxyi\nn &lt;- data_r_roth_2015$n\nk &lt;- length(ro)\n\n# fill in missing artifact values with mean\nrxx[is.na(rxx)] &lt;- mean(rxx,na.rm=TRUE)\nux[is.na(ux)] &lt;- mean(ux,na.rm=TRUE)\n\n# calculate compound artifact biasing factor for univariate direct range restriction with measurement error\nA &lt;- sqrt(ro^2 + (ux^2*rxx*(rxx - ro^2)) / (1 - ux^2*(1-rxx)))\n\n# calculate the sample size weighted average of r\nro_bar &lt;- sum(ro*n) / sum(n)\n\n# calculate the observed sampling variance for each study\nvar_eoi &lt;- (1-ro_bar^2)^2 / (n-1)\n\n# correct sampling variance\nvar_eci &lt;- var_eoi / A^2\n\n# calculate corrected correlations\nrc &lt;- ro / A\n\n# calculate weights\nw &lt;- 1/var_eci\n\n# calculate population effect size estimate\nmean_rho_hat &lt;- sum(rc*w) / sum(w)\n\n# calculate the variance in corrected correlations (rc)\nvar_rc &lt;- sum(w*(rc - mean_rho_hat)^2) / sum(w)\n\n# calculate average corrected sampling variance\nvar_ec &lt;- sum(var_eci*w) / sum(w)\n\n# calculate the variance in true population correlations\nvar_rho &lt;- var_rc - var_ec\n\n# calculate standard error of rho estimate\nSE_rho = sqrt(var_rc/k)\n\n# print results\ndata.frame(k = k,\n           n = sum(n),\n           mean_rho_hat,\n           SE = SE_rho,\n           SD_rho = sqrt(var_rho))\n\n    k      n mean_rho_hat         SE    SD_rho\n1 240 105151    0.5398838 0.01339916 0.2022865\n\n\nThe estimated mean correlation of .540 is precisely what is precisely what the original paper reported (Roth 2015). Lets conduct the meta-analysis using the the psychmeta package (Dahlke and Wiernik 2019). The function ma_r_ic is designed to conduct an individual correction meta-analysis on correlation coefficients.\n\n# install.packages('psychmeta')\nlibrary(psychmeta)\n\n# conduct individual correction meta-analysis\nmdl_ic &lt;- ma_r_ic(rxyi = ro, n = n,\n            correction_method = \"uvirr\",\n            rxx = rxx,\n            ux = ux,\n            ux_observed = TRUE,\n            rxx_restricted = TRUE)\n\nsummary_stats &lt;- data.frame(k = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$k,\n                            n = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$N,\n                            mean_rho = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$mean_rho,\n                            SE = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$se_r_c,\n                            SD_rho = mdl_ic$meta_tables$`analysis_id: 1`$individual_correction$true_score$sd_rho)\nsummary_stats \n\n    k      n  mean_rho        SE    SD_rho\n1 240 105151 0.5404134 0.0134356 0.2036946\n\n\nWe can also obtain credibility intervals by using the credibility function in the psychmeta package. The interval defaults to 80% intervals, however we can change that to 90% by inputting .90 into the cred_level argument.\n\ncredibility(mean = summary_stats$mean_rho_hat,\n            sd = summary_stats$SD_rho,\n            cred_method = \"norm\",\n            cred_level = .90)\n\n     CR_LL_90 CR_UL_90\n\n\nLets compare these results to the bare-bones model. In psychmeta the bare-bones model can be conduced using ma_r_bb. However, the ma_r_ic function also reports the bare-bones results as well. Therefore we can just extract the necessary statistics from the model.\n\ndata.frame(\n  k = mdl_ic$meta_tables$`analysis_id: 1`$barebones$k,\n  n = mdl_ic$meta_tables$`analysis_id: 1`$barebones$N,\n  mean_rho_obs = mdl_ic$meta_tables$`analysis_id: 1`$barebones$mean_r,\n  SE = mdl_ic$meta_tables$`analysis_id: 1`$barebones$se_r,\n  SD_rho_obs = mdl_ic$meta_tables$`analysis_id: 1`$barebones$sd_r)\n\n    k      n mean_rho_obs         SE SD_rho_obs\n1 240 105151    0.4418789 0.01191933  0.1846534\n\n\nWe can see that the estimate of the population correlation is largely attenuated in the observed values. This is due to the fact tests of intelligence are not perfectly reliable and the scores were restricted in their range."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#artifact-distribution-model",
    "href": "artifact_correction_metaanalysis.html#artifact-distribution-model",
    "title": "12  Artifact Correction Meta-Analysis",
    "section": "12.4 Artifact Distribution Model",
    "text": "12.4 Artifact Distribution Model\nWhen we observe a lot of missingness in artifact values (e.g., studies not reporting reliability), we may choose to use an artifact distribution model. The artifact distribution model conducts a meta-analysis on the observed effect sizes and artifact values separately, and then uses the aggregate artifact values to correct for the observed mean effect size. Since the artifact distribution method uses Taylor series approximations (Dahlke and Wiernik 2020) that are custom-tailored to estimate the sampling variance of corrected correlations, we will skip the general case to focus on its application to correlations.\n\n12.4.1 The Correlational Case\nThe model here can be broken down into two parts, the first part aggregates the observed effect sizes and the second part aggregates the artifact values. The artifact values we will focus on here are the reliability coefficients (see chapter 5 and 6), however other artifact values like \\(u\\)-ratios will follow similar procedures. We can start with the bare-bones meta-analysis model: \\(r_{o_i} = \\rho_{o_i} + \\varepsilon_{o_i}\\). We can estimate the observed population correlation (\\(\\vartheta_{o_i}\\)) by first calculating the weights (using the \\(n\\)-weighted mean correlation in the formula for sampling variance, \\(\\bar{r}\\)):\n\\[\n\\sigma^2_{\\varepsilon_oi} \\approx \\frac{(1-\\bar{r}^2)^2}{n_i-1}\n\\]\n\\[\nw_i = \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\sigma_\\vartheta^2} = \\frac{1}{\\sigma^2_{\\varepsilon_i}+(\\sigma^2_{\\theta}-\\sigma^2_{\\varepsilon})} \\approx \\frac{1}{\\sigma^2_{\\varepsilon_i}+\\left(\\frac{\\sum^k_{i=1}n_i(\\theta_i - \\bar{\\theta})^2}{\\sum^k_{i=1}n_i}-\\frac{\\sum^k_{i=1}n_i\\sigma^2_{\\varepsilon_oi}}{\\sum^k_{i=1}n_i}\\right)}\n\\]\nTaking the mean of the observed study correlations weighted by the inverse sampling variance,\n\\[\n\\hat{\\bar{\\rho}}_o=\\frac{\\sum^k_{i=1}w_i r_{o_i}}{\\sum^k_{i=1}w_i}\n\\]\nThen lets get the variance in observed population correlations, in order to do this we need the v\n\\[\n\\sigma^2_{\\rho_o}=\\sigma^2_{r_o} - \\sigma^2_{\\varepsilon_o} = \\frac{\\sum^k_{i=1}w_i (r_{o_i}-\\hat{\\bar{\\rho}}_o)^2}{\\sum^k_{i=1}w_i} - \\frac{\\sum^k_{i=1}w_i \\sigma^2_{\\varepsilon_oi}}{\\sum^k_{i=1}w_i}\n\\]\nWith the weights we can also take the weighted average of the artifact values (such as \\(u\\)-ratios or reliabilities) that are available. For our example here, we will correct only for measurement error, therefore the weighted means for reliability in \\(x\\) and \\(y\\) will be:\n\\[\n\\bar{r}_{xx'}=\\frac{\\sum^k_{i=1}w_i r_{xx'_i}}{\\sum^k_{i=1}w_i}\n\\]\n\\[\n\\bar{r}_{yy'}=\\frac{\\sum^k_{i=1}w_i r_{yy'_i}}{\\sum^k_{i=1}w_i}\n\\]\nNow recall from chapter 5 that the square root of the reliability is equal to the correlation between observed scores and true scores. We can denote the mean correlation as follows: \\(\\bar{r}_{xT}=\\sqrt{\\bar{r}_{xx'}}\\) and \\(\\bar{r}_{yU}=\\sqrt{\\bar{r}_{xx'}}\\). We then must also compute the average sampling variances of \\(r_{xT_i}\\) and \\(r_{yU_i}\\) between studies. These sampling variance of these correlations can be computed the same way as a pearson correlation:\n\\[\n\\sigma^2_{r_{xT}i} \\approx \\frac{(1-\\bar{r}_{xT}^2)^2}{n_i-1}\n\\]\n\\[\n\\sigma^2_{r_{yU}i} \\approx \\frac{(1-\\bar{r}_{yU}^2)^2}{n_i-1}\n\\]\nThen weighted average of these sampling variances is\n\\[\n\\sigma^2_{r_{xT}} = \\frac{\\sum^k_{i=1}w_i r_{xT_i}}{\\sum^k_{i=1}w_i}\n\\]\n\\[\n\\sigma^2_{r_{yU}} = \\frac{\\sum^k_{i=1}w_i r_{yU_i}}{\\sum^k_{i=1}w_i}\n\\] Now that we have the point-estimate of the population observed correlation, the variance of observed population correlations, the sampling variance of observed correlations, and the sampling variance of the square root of the reliability for \\(x\\) and \\(y\\), we can now attempt to correct the point-estimate and the variance of population correlations.\n\nCorrecting Using Summary Values\nFirst, we can start by correcting the overall point-estimate for the observed population correlation in order to remove bias due to measurement error. Recall from chapter 5 the correction formula:\n\\[\n\\hat{\\bar{\\rho}} = \\frac{\\hat{\\bar{\\rho}}_o}{\\bar{A}} = \\frac{\\hat{\\bar{\\rho}}_o}{\\bar{r}_{xT} \\bar{r}_{yU}} = \\frac{\\hat{\\bar{\\rho}}_o}{\\sqrt{\\bar{r}_{xx'}} \\sqrt{\\bar{r}_{yy'}}}\n\\]\nNote that the artifact biasing factor, \\(A\\), is the product of the two sources of attenuation. Correcting the variance in observed population correlations (\\(\\sigma^2_{\\rho_o}\\)), so that it is accurately estimating the variance of true population effect sizes (\\(\\sigma^2_{\\rho}\\)), we must use a Taylor series approximation. This formula can become fairly complex with more types of artifacts involved. The taylor series approximation is for estimating specifically the amount of sampling variance within the correction factor we apply to the observed correlation. The first step is lay out our attenuation formula (the equation where observed effect size is on the left side of the equation and the artifact values and true effect size is on the right hand side of the equation). In the case of correcting only for measurement error, the attenuation formula is relatively simple\n\\[\n\\hat{\\bar{\\rho}}_o = \\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\\cdot \\bar{r}_{yU}\n\\]\nFor the taylor series approximation, we want to first find the partial derivitive with respect to each artifact component:\n\\[\nB_{r_{xT}}=\\frac{\\partial}{\\partial r_{xT}} (\\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\\cdot \\bar{r}_{yU}) = \\hat{\\bar{\\rho}}\\cdot \\bar{r}_{yU}\n\\] \\[\nB_{r_{yU}}=\\frac{\\partial}{\\partial r_{yU}} (\\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\\cdot \\bar{r}_{yU}) = \\hat{\\bar{\\rho}}\\cdot \\bar{r}_{xT}\n\\]\nThe variance due to artifacts is then approximately,\n\\[\n\\sigma^2_A\\approx B^2_{r_{xT}} \\sigma^2_{r_{xT}} + B^2_{r_{yU}} \\sigma^2_{r_{yU}}\n\\]\nNow we can approximate the variance in true population correlations,\n\\[\n\\sigma_\\rho^2= \\frac{\\sigma^2_{\\rho_o} - \\sigma^2_A}{\\bar{A}^2}\n\\]\nWhere the artifact biasing factor is: \\(\\bar{A}=\\bar{r}_{xT}\\cdot \\bar{r}_{yU}\\). See the supplementary materials of Dahlke and Wiernik (2020) for detailed Taylor series approximation derivations for the immensely more complicated bivariate indirect range restriction plus measurement error correction.\n\n\n\n12.4.2 Applied Example in R\nLets conduct an artifact distribution correction meta-analysis in R, instead using data from the meta-analysis by McDaniel et al. (1994). This dataset contains correlations between employment interviews and job performance. This data set has a lot of missing values for reliability coefficients and \\(u\\)-ratios which might suggest that the artifact distribution approach is a better choice compared to the individual correction approach. We can conduct a meta-analysis correcting for univariate indirect range restriction and measurement error in both job performance and employment interviews. The attenuation formula will be important for calculating the Taylro series approximation can be defined as\n\\[\n\\bar{\\rho}_o=\\bar{\\rho}\\sqrt{\\bar{r}_{o_i}^2 + \\frac{\\bar{u}_{x_i}^2 \\bar{r}_{xx'_i}(\\bar{r}_{xx'_i}\\bar{r}_{yy'_i} - \\bar{r}_{o_i}^2) }{1 - \\bar{u}_{x_i}^2 (1-\\bar{r}_{xx'_i})} }\n\\] Instead of conducting a taylor series approximation by hand, we will simply use the psychmeta package to perform the artifact distribution meta-analysis. The function ma_r_ad is designed to conduct an artifact distribution meta-analysis on correlation coefficients. The function also reports the bare-bones model allowing us to compare the corrected estimates to the uncorrected.\n\n# Load in packages (we need the development version of psychmeta)\n# install.packages(\"devtools\")\n# devtools::install_github(\"psychmeta/psychmeta\")\nlibrary(psychmeta)\n\n# obtain artifact values\nrxx &lt;- data_r_roth_2015$rxxi\nux &lt;- data_r_roth_2015$ux\nro &lt;- data_r_roth_2015$rxyi\nn &lt;- data_r_roth_2015$n\nk &lt;- length(ro)\n\n# compute barebones meta-analysis\nma_obj &lt;- ma_r_bb(r = rxyi, \n                  n = n, \n                  correct_bias = FALSE, \n                  wt_type = \"REML\",\n                  data = data_r_mcdaniel_1994)\n\n# construct artifact distribution for x\nad_obj_x &lt;- create_ad(ad_type = \"tsa\", \n                      mean_rxxi = data_r_mcdaniel_1994$Mrxxi[1],\n                      var_rxxi = data_r_mcdaniel_1994$SDrxxi[1]^2,\n                      ux = data_r_mcdaniel_1994$ux,\n                      wt_ux = data_r_mcdaniel_1994$`ux frequency`)\n\n# construct artifact distribution for y\nad_obj_y &lt;- create_ad(ad_type = \"tsa\", \n                      rxxi = data_r_mcdaniel_1994$ryyi,\n                      wt_rxxi = data_r_mcdaniel_1994$`ryyi frequency`)\n\n# compute artifact-distribution meta-analysis, correcting for measurement error only\nmdl_ad &lt;- ma_r_ad(ma_obj = ma_obj, \n                  ad_obj_x = ad_obj_x, \n                  ad_obj_y = ad_obj_y, \n                  correction_method = \"meas\")\n\n\n# summary table of meta-analysis\nsummary_stats &lt;- data.frame(\n  type = c('Artifact Distribution', 'Bare-Bones'),\n  k = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$k,mdl_ad$meta_tables$`analysis_id: 1`$barebones$k),\n  n = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$N,mdl_ad$meta_tables$`analysis_id: 1`$barebones$N),\n  mean_rho = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$mean_rho,mdl_ad$meta_tables$`analysis_id: 1`$barebones$mean_r),\n  SE = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$se_r_c,mdl_ad$meta_tables$`analysis_id: 1`$barebones$se_r),\n  SD_rho = c(mdl_ad$meta_tables$`analysis_id: 1`$artifact_distribution$true_score$sd_rho,0))\n\nsummary_stats \n\n                   type   k     n  mean_rho         SE    SD_rho\n1 Artifact Distribution 160 25244 0.3201828 0.02108407 0.1985571\n2            Bare-Bones 160 25244 0.2205043 0.01452023 0.0000000\n\n\nWe can also obtain credibility intervals by using the credibility function in the psychmeta package. The interval defaults to 80% intervals, however we can change that to 90% by inputting .90 into the cred_level argument.\n\ncredibility(mean = summary_stats$mean_rho[1],\n            sd = summary_stats$SD_rho[1],\n            cred_method = \"norm\",\n            cred_level = .90)\n\n         CR_LL_90  CR_UL_90\n[1,] -0.006414571 0.6467802\n\n\nLets compare these results to the bare-bones model. In psychmeta the bare-bones model can be conduced using ma_r_bb. However, the ma_r_ic function also reports the bare-bones results as well. Therefore we can just extract the necessary statistics from the model.\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\n———. 2020. “Not Restricted to Selection Research: Accounting for Indirect Range Restriction in Organizational Research.” Organizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nMcDaniel, Michael A., Deborah L. Whetzel, Frank L. Schmidt, and Steven D. Maurer. 1994. “The Validity of Employment Interviews: A Comprehensive Review and Meta-Analysis.” Journal of Applied Psychology 79 (4): 599–616. https://doi.org/10.1037/0021-9010.79.4.599.\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A Meta-Analysis.”\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General Solution to the Problem of Validity Generalization.” Journal of Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "references_page.html",
    "href": "references_page.html",
    "title": "References",
    "section": "",
    "text": "Aguinis, Herman, Charles A Pierce, and Steven A Culpepper. 2009.\n“Scale Coarseness as a Methodological Artifact,” September.\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah\nR. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and\nRandom-Effects Models for Meta-Analysis.” Research Synthesis\nMethods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nBorsboom, Denny, and Gideon J Mellenbergh. 2002. “True Scores,\nLatent Variables, and Constructs: A Comment on Schmidt and\nHunter.”\n\n\nBorsboom, Denny, Gideon J. Mellenbergh, and Jaap Van Heerden. 2004.\n“The Concept of Validity.” Psychological Review\n111 (4): 1061–71. https://doi.org/10.1037/0033-295X.111.4.1061.\n\n\nBrannick, Michael T., Sean M. Potter, Bryan Benitez, and Scott B.\nMorris. 2019. “Bias and Precision of Alternate Estimators in\nMeta-Analysis: Benefits of Blending Schmidt-Hunter and Hedges\nApproaches.” Organizational Research Methods 22 (2):\n490–514. https://doi.org/10.1177/1094428117741966.\n\n\nBravais, A. 1844. Analyse mathématique sur les probabilités des\nerreurs de situation d’un point. Impr. Royale.\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test\nof a New Model for Validity Generalization.” Journal of\nApplied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. Academic Press.\n\n\n———. 2013. Statistical Power Analysis for the Behavioral\nSciences. Academic Press.\n\n\nCronbach, Lee J., and Paul E. Meehl. 1955. “Construct Validity in\nPsychological Tests.” Psychological Bulletin 52 (4):\n281–302. https://doi.org/10.1037/h0040957.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R\nPackage for Psychometric Meta-Analysis.” Applied\nPsychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933.\n\n\n———. 2020. “Not Restricted to Selection Research: Accounting for\nIndirect Range Restriction in Organizational Research.”\nOrganizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects\nModel for Meta-Analysis of Clinical Trials: An Update.”\nNIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75\n(1949): 450–51. https://doi.org/10.1038/075450a0.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator\nof Effect Size and Related Estimators.” Journal of\nEducational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\n———. 1989. “An Unbiased Correction for Sampling Error in Validity\nGeneralization Studies.” Journal of Applied Psychology\n74 (3): 469–77. https://doi.org/10.1037/0021-9010.74.3.469.\n\n\nHedges, Larry V., and Ingram Olkin. 2014. Statistical Methods for\nMeta-Analysis. Academic press. https://books.google.com/books?hl=en&lr=&id=7GviBQAAQBAJ&oi=fnd&pg=PP1&dq=info:e6P1zfh2T6QJ:scholar.google.com&ots=Dx-YqN6_9B&sig=-39HgbYdWPp_BwSTzA9cRODs2Q0.\n\n\nHedges, Larry V., and Jack L. Vevea. 1998. “Fixed- and\nRandom-Effects Models in Meta-Analysis.” Psychological\nMethods 3 (4): 486–504. https://doi.org/10.1037/1082-989X.3.4.486.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of\nmeta-analysis: correcting error and bias in research findings.\nNewbury Park: Sage Publications.\n\n\nHunter, John, and Frank Schmidt. 1990. “Dichotomization of\nContinuous Variables: The Implications for Meta-Analysis.”\nJournal of Applied Psychology 75 (June): 334–49. https://doi.org/10.1037/0021-9010.75.3.334.\n\n\nJohnson, Blair T., Brian Mullen, and Eduardo Salas. 1995.\n“Comparison of Three Major Meta-Analytic Approaches.”\nJournal of Applied Psychology 80 (1): 94–106. https://doi.org/10.1037/0021-9010.80.1.94.\n\n\nKelley, Truman Lee. 1927. Interpretation of Educational\nMeasurements. World Book Company.\n\n\nLaird, Nan M., and Frederick Mosteller. 1990. “Some Statistical\nMethods for Combining Experimental Results.” International\nJournal of Technology Assessment in Health Care 6 (1): 5–30. https://doi.org/10.1017/S0266462300008916.\n\n\nMcDaniel, Michael A., Deborah L. Whetzel, Frank L. Schmidt, and Steven\nD. Maurer. 1994. “The Validity of Employment Interviews: A\nComprehensive Review and Meta-Analysis.” Journal of Applied\nPsychology 79 (4): 599–616. https://doi.org/10.1037/0021-9010.79.4.599.\n\n\nMorris, Scott, Rebecca Daisley, Megan Wheeler, and Peggy Boyer. 2014.\n“A Meta-Analysis of the Relationship Between Individual\nAssessments and Job Performance.” The Journal of Applied\nPsychology 100 (May). https://doi.org/10.1037/a0036938.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of\nCertain Correlation Coefficients.” The Annals of Mathematical\nStatistics 29 (1): 201–11. https://www.jstor.org/stable/2237306.\n\n\nPearson, Karl. 1903. “I. Mathematical Contributions to the Theory\nof Evolution. XI. On the Influence of Natural Selection on\nthe Variability and Correlation of Organs.” Philosophical\nTransactions of the Royal Society of London. Series A, Containing Papers\nof a Mathematical or Physical Character 200 (321-330): 1–66. https://doi.org/10.1098/rsta.1903.0001.\n\n\n———. 1913. “On the Probable Error of a Coefficient of Correlation\nas Found from a Fourfold Table.” Biometrika 9 (1/2):\n22–33. https://doi.org/10.2307/2331798.\n\n\nPeters, Charles C., and Walter R. Van Voorhis. 1940. “Further\nMethods of Correlation.” In, 362–403. New York, NY, US:\nMcGraw-Hill Book Company. https://doi.org/10.1037/13596-013.\n\n\n“Psych: Procedures for Personality and Psychological\nResearch.” 2017. https://CRAN.R-project.org/package=psych.\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A\nMeta-Analysis.”\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range\nRestriction: An Expanded Typology.” Journal of Applied\nPsychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General\nSolution to the Problem of Validity Generalization.” Journal\nof Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association\nBetween Two Things.” International Journal of\nEpidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nTaylor, Erwin K., and Thomas Griess. 1976. “The Missing Middle in\nValidation Research.” Personnel Psychology 29 (1): 5–11.\nhttps://doi.org/10.1111/j.1744-6570.1976.tb00397.x.\n\n\nViechtbauer, Wolfgang. 2010. “Conducting meta-analyses in R with\nthe metafor package.” Journal of Statistical Software 36\n(3): 1–48. https://doi.org/10.18637/jss.v036.i03.\n\n\n———. n.d. “Fixed-Effects and Random-Effects Models in\nMeta-Analysis.” https://wviechtb.github.io/metafor/index.html.\n\n\nVos, Paul, and Don Holbert. 2022. “Frequentist Statistical\nInference Without Repeated Sampling.” Synthese 200 (2):\n89. https://doi.org/10.1007/s11229-022-03560-x.\n\n\nWechsler, David. 2008. Wechsler Adult Intelligence Scale–Fourth\nEdition. 4th ed. https://doi.org/10.1037/t15169-000.\n\n\nWhitener, Ellen M. 1990. “Confusion of Confidence Intervals and\nCredibility Intervals in Meta-Analysis.” Journal of Applied\nPsychology 75 (3): 315–21. https://doi.org/10.1037/0021-9010.75.3.315.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining\nUnbiased Results in Meta-Analysis: The Importance of Correcting for\nStatistical Artifacts.” Advances in Methods and Practices in\nPsychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611.\n\n\nWylie, Peter B. 1976. “Effects of Coarse Grouping and Skewed\nMarginal Distributions on the Pearson Product Moment Correlation\nCoefficient.” Educational and Psychological Measurement\n36 (1): 1–7. https://doi.org/10.1177/001316447603600101."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "",
    "text": "1 Greetings\nWelcome to the living open source textbook Correcting Effect Sizes for Statistical Artifacts. This textbook covers all the essential equations and code needed to correct for biases in our effect size estimates. It will also hopefully provide readers with a deeper understanding, appreciation, and intuition for these seemingly complex formulas. It also covers how to apply these corrections to meta-analysis."
  },
  {
    "objectID": "index.html#cite-this-work",
    "href": "index.html#cite-this-work",
    "title": "Artifact Corrections for Effect Sizes",
    "section": "Cite This Work",
    "text": "Cite This Work\n\nAPA\nJané, M. B. (2023). Correcting Effect Sizes for Statistical Artifacts: Implementation in R and Application to Meta-Analysis. (n.p.). https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/\n\n\nBibTeX\n@book{MatthewBJane2023,\n  title     = \"Correcting Effect Sizes for Statistical Artifacts: Implementation in R and Application to Meta-Analysis\",\n  author    = \"Jané, Matthew B.\",\n  year      = 2023,\n  publisher = \"(n.p.)\",\n  url      = {https://matthewbjane.quarto.pub/artifact-corrections-for-effect-sizes/}\n}"
  }
]