[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "A correlation describes the relationship between two continuous variables. The correlation coefficient was first introduced in\n\n\nIf we draw a sample of \\(n\\) observations from a population, we can estimate the population correlation between variables \\(x\\) and \\(y\\) using a reasonably unbiased estimator, \\(r\\),\n\\[\nr = \\frac{\n\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n}{\n\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n}\n\\] This formulation is commonly referred to as the Pearson correlation coefficient (pe?) . To see under the hood of this seemingly complex mathematical formulation. Since the correlation is simply the the standardized covariance between two variables, x and y, we can first define the covariance as the product of the squared errors between (\\(x_i - \\bar{x}\\)) and y (\\(y_i - \\bar{y}\\)),\n\\[\n\\text{cov}(x,y) =\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\\]\nThen we can find the variance for x and y by taking the average squared error from the mean for x and y,\n\\[\n\\text{var}(x) = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})\n\\]\n\\[\n\\text{var}(y) = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})\n\\]\nNow with each of these components, we can standardized the covariance by dividing by the standard deviations of x and y (i.e., square root of the variance). It can be now seen that the \\(\\frac{1}{n-1}\\) term cancels out in the numerator and denominator and thus will give us the original formula for the sample correlation coefficient.\n\\[\nr = \\frac{\\text{cov}(x,y)}{\\sqrt{\\text{var}(x)}\\sqrt{\\text{var}(y)}} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}}\n\\]\nSince \\(r\\) is the observed sample correlation, it is important to note that, in the absence of artifacts, \\(r\\) provides an unbiased estimate of the true population correlation \\(\\rho\\) (\\(r =\\hat{\\rho}\\)). Therefore, in conditions uncontaminated by artifacts, differences between the observed sample correlation and the true population correlation are attributable to sampling error (\\(\\varepsilon\\)) such that,\n\\[\n\\rho = \\hat{\\rho} + \\varepsilon, \\;\\;\\;\\; \\varepsilon \\sim \\mathcal{N}(0,se_{r}^2)\n\\] Where \\(se_r\\) is the standard error of the observed correlation. The standard error can be calculated from the sample size (\\(n\\)) and the observed correlation, \\[\nse_r =\\sqrt{\\frac{1 - r^2}{n-2}}\n\\]\n\n\n\n\n\n\n\n\n(Roth 2015)\n(Van Aarde, Meiring, and Wiernik 2017)\n(Hunter and Hunter, n.d.)\n\n\n\n\nHunter, John E, and Ronda F Hunter. n.d. “Validity and Utility of Alternative Predictors of Job Performance.”\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A Meta-Analysis.”\n\n\nVan Aarde, Ninette, Deon Meiring, and Brenton M. Wiernik. 2017. “The Validity of the Big Five Personality Traits for Job Performance: Meta-Analyses of South African Studies.” International Journal of Selection and Assessment 25 (3): 223–39. https://doi.org/10.1111/ijsa.12175."
  },
  {
    "objectID": "intro.html#effect-sizes",
    "href": "intro.html#effect-sizes",
    "title": "Introduction",
    "section": "",
    "text": "A correlation describes the relationship between two continuous variables. The correlation coefficient was first introduced in\n\n\nIf we draw a sample of \\(n\\) observations from a population, we can estimate the population correlation between variables \\(x\\) and \\(y\\) using a reasonably unbiased estimator, \\(r\\),\n\\[\nr = \\frac{\n\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n}{\n\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n}\n\\] This formulation is commonly referred to as the Pearson correlation coefficient (pe?) . To see under the hood of this seemingly complex mathematical formulation. Since the correlation is simply the the standardized covariance between two variables, x and y, we can first define the covariance as the product of the squared errors between (\\(x_i - \\bar{x}\\)) and y (\\(y_i - \\bar{y}\\)),\n\\[\n\\text{cov}(x,y) =\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\\]\nThen we can find the variance for x and y by taking the average squared error from the mean for x and y,\n\\[\n\\text{var}(x) = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})\n\\]\n\\[\n\\text{var}(y) = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})\n\\]\nNow with each of these components, we can standardized the covariance by dividing by the standard deviations of x and y (i.e., square root of the variance). It can be now seen that the \\(\\frac{1}{n-1}\\) term cancels out in the numerator and denominator and thus will give us the original formula for the sample correlation coefficient.\n\\[\nr = \\frac{\\text{cov}(x,y)}{\\sqrt{\\text{var}(x)}\\sqrt{\\text{var}(y)}} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}}\n\\]\nSince \\(r\\) is the observed sample correlation, it is important to note that, in the absence of artifacts, \\(r\\) provides an unbiased estimate of the true population correlation \\(\\rho\\) (\\(r =\\hat{\\rho}\\)). Therefore, in conditions uncontaminated by artifacts, differences between the observed sample correlation and the true population correlation are attributable to sampling error (\\(\\varepsilon\\)) such that,\n\\[\n\\rho = \\hat{\\rho} + \\varepsilon, \\;\\;\\;\\; \\varepsilon \\sim \\mathcal{N}(0,se_{r}^2)\n\\] Where \\(se_r\\) is the standard error of the observed correlation. The standard error can be calculated from the sample size (\\(n\\)) and the observed correlation, \\[\nse_r =\\sqrt{\\frac{1 - r^2}{n-2}}\n\\]"
  },
  {
    "objectID": "intro.html#bias-induced-by-statistical-artifacts",
    "href": "intro.html#bias-induced-by-statistical-artifacts",
    "title": "Introduction",
    "section": "",
    "text": "(Roth 2015)\n(Van Aarde, Meiring, and Wiernik 2017)\n(Hunter and Hunter, n.d.)\n\n\n\n\nHunter, John E, and Ronda F Hunter. n.d. “Validity and Utility of Alternative Predictors of Job Performance.”\n\n\nRoth, Bettina. 2015. “Intelligence and School Grades: A Meta-Analysis.”\n\n\nVan Aarde, Ninette, Deon Meiring, and Brenton M. Wiernik. 2017. “The Validity of the Big Five Personality Traits for Job Performance: Meta-Analyses of South African Studies.” International Journal of Selection and Assessment 25 (3): 223–39. https://doi.org/10.1111/ijsa.12175."
  },
  {
    "objectID": "indirect_range_restriction.html",
    "href": "indirect_range_restriction.html",
    "title": "Indirect Range Restriction/Enhancement",
    "section": "",
    "text": "Indirect Range Restriction/Enhancement\n(Dahlke and Wiernik 2020)\n(Hunter and Schmidt 1990)\n(Thorndike 1949)\n(Bobko, Roth, and Bobko 2001)\n(Sackett and Yang 2000)\n\n\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001. “Correcting the Effect Size of d for Range Restriction and Unreliability.” Organizational Research Methods - ORGAN RES METHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2020. “Not Restricted to Selection Research: Accounting for Indirect Range Restriction in Organizational Research.” Organizational Research Methods 23 (4): 717–49. https://doi.org/10.1177/1094428119859398.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nThorndike, Robert L. 1949. Personnel Selection; Test and Measurement Techniques. Personnel Selection; Test and Measurement Techniques. Oxford, England: Wiley."
  },
  {
    "objectID": "artificial_dichotomization.html",
    "href": "artificial_dichotomization.html",
    "title": "Artificial Dichotomization",
    "section": "",
    "text": "Artificial Dichotomization\n(Naggara et al. 2011)\n(Russell, Pinto, and Bobko 1991)\n(Digby 1983)\n(Maxwell and Delaney 1993)\n(Hunter and Schmidt 1990)\n(Vargha et al. 1996)\n(Royston, Altman, and Sauerbrei 2006)\n(Peters and Voorhis 1940)\n(Bonett and Price 2005)\n(Ulrich and Wirtz 2004)\n(Muthén and Hofacker 1988)\n(MacCallum et al. 2002)\n\n\n\n\nBonett, Douglas G., and Robert M. Price. 2005. “Inferential Methods for the Tetrachoric Correlation Coefficient.” Journal of Educational and Behavioral Statistics 30 (2): 213–25. https://www.jstor.org/stable/3701350.\n\n\nDigby, P. G. N. 1983. “Approximating the Tetrachoric Correlation Coefficient.” Biometrics 39 (3): 753–57. https://doi.org/10.2307/2531104.\n\n\nHunter, John, and Frank Schmidt. 1990. “Dichotomization of Continuous Variables: The Implications for Meta-Analysis.” Journal of Applied Psychology 75 (June): 334–49. https://doi.org/10.1037/0021-9010.75.3.334.\n\n\nMacCallum, Robert C., Shaobo Zhang, Kristopher J. Preacher, and Derek D. Rucker. 2002. “On the Practice of Dichotomization of Quantitative Variables.” Psychological Methods 7: 19–40. https://doi.org/10.1037/1082-989X.7.1.19.\n\n\nMaxwell, Scott, and Harold Delaney. 1993. “Bivariate Median Splits and Spurious Statistical Significance.” Psychological Bulletin 113 (January): 181–90. https://doi.org/10.1037/0033-2909.113.1.181.\n\n\nMuthén, Bengt, and Charles Hofacker. 1988. “Testing the Assumptions Underlying Tetrachoric Correlations.” Psychometrika 53 (4): 563–77. https://doi.org/10.1007/BF02294408.\n\n\nNaggara, O., J. Raymond, F. Guilbert, D. Roy, A. Weill, and D. G. Altman. 2011. “Analysis by Categorizing or Dichotomizing Continuous Variables Is Inadvisable: An Example from the Natural History of Unruptured Aneurysms.” American Journal of Neuroradiology 32 (3): 437–40. https://doi.org/10.3174/ajnr.A2425.\n\n\nPeters, Charles C., and Walter R. Van Voorhis. 1940. “Further Methods of Correlation.” In, 362–403. New York, NY, US: McGraw-Hill Book Company. https://doi.org/10.1037/13596-013.\n\n\nRoyston, Patrick, Douglas G. Altman, and Willi Sauerbrei. 2006. “Dichotomizing Continuous Predictors in Multiple Regression: A Bad Idea.” Statistics in Medicine 25 (1): 127–41. https://doi.org/10.1002/sim.2331.\n\n\nRussell, Craig J., Jeffrey K. Pinto, and Philip Bobko. 1991. “Appropriate Moderated Regression and Inappropriate Research Strategy: A Demonstration of Information Loss Due to Scale Coarseness” 15 (3): 257–66. https://doi.org/10.1177/014662169101500305.\n\n\nUlrich, Rolf, and Markus Wirtz. 2004. “On the Correlation of a Naturally and an Artificially Dichotomized Variable.” British Journal of Mathematical and Statistical Psychology 57 (2): 235–51. https://doi.org/10.1348/0007110042307203.\n\n\nVargha, András, Tamás Rudas, Harold D. Delaney, and Scott E. Maxwell. 1996. “Dichotomization, Partial Correlation, and Conditional Independence.” Journal of Educational and Behavioral Statistics 21 (3): 264–82. https://doi.org/10.2307/1165272."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Proposal and Outline",
    "section": "",
    "text": "Proposal and Outline\nResults across scientific studies vary drastically even when studying the same underlying phenomena. Sometimes these can be accounted for by some study-level characteristic (i.e., methodology, population, etc.) or it can be accounted for by variations in statistical artifacts such as measurement error or selection effects. Not only does the heterogeneity increase in the presence of statistical artifacts, but artifacts also induce systematic biases that can cause inaccurate results. Artifacts restrict our ability to draw meaningful inferences from scientific results, therefore it is important to apply corrections in order to obtain unbiased estimates.\nOverview. The goal of the general exam is to turn it into an online open source e-textbook similar to this one on meta-analysis in R by Mathias Harrer and colleagues. The e-text will be split into two parts: artifact corrections and their application to meta-analysis. Each type of artifact will have a section of its own that details the history, methodology, and its implementation in R and Python. Another section will be dedicated to applying artifact corrections to meta-analysis, which is the type of research where the corrections are most used. The meta-analysis section will also contain the four parts (history, methodology, assumptions, and implementation in R). See this webpage where I cataloged equations and code that will go into this general exam. I also have started writing the unreliability section so you can start get an idea of how this will look.\nHistory. The history part will provide an overview of the literature for that artifact correction. It will note where the correction first was introduced, the adjustments people have made since then, as well as studies where the correction has been applied (most likely in a meta-analysis). Also if applicable, provide examples of where these meta-analyses have utilized such corrections (e.g., Roth et al., 2015) Methodology. The methodological part will review the correction equations (for point estimates and standard errors) and how they were derived for each artifact correction. Artifact corrections will be applied to both correlation coefficients and standardized mean differences (repeated measures and independent samples). The methodological part for the meta-analysis section will discuss how heterogeneity, credibility/confidence intervals, and averages are calculated in the context of artifact corrections. It will also touch on competing approaches (Raju et al., 1983) to the traditional artifact correction approaches (Hunter & Schmidt, 2004).\nAssumptions. Each artifact correction contains assumptions that must be met in order to obtain unbiased estimate of the true population effect size. This part will discuss each of these assumptions, when each matter, and what simulation studies have found in regard to violation of assumptions. Assumptions at the meta-analytic level of analysis (e.g., independence between artifacts and moderators) will also be discussed.\nImplementation in R and Python. Artifact corrections are only useful if you can apply them. Since many of the corrections are quite complex, it is important that these can be implemented easily in an open-source software such as R and Python. Each correction will be supplemented with code using R and Python with some additional packages like psychmeta (Dahlke and Wiernik 2019). For statistical analyses and especially for meta-analysis, R has tremendous support in terms of useful packages and a large community that makes it a highly flexible and powerful language. Python is a general-purpose programming language that is a more popular than R. For an example of how this will be done see this section on estimating reliability in R and Python.\n\n\n\n\nDahlke, Jeffrey A., and Brenton M. Wiernik. 2019. “Psychmeta: An R Package for Psychometric Meta-Analysis.” Applied Psychological Measurement 43 (5): 415–16. https://doi.org/10.1177/0146621618795933."
  },
  {
    "objectID": "metaanalysis_intro.html",
    "href": "metaanalysis_intro.html",
    "title": "Introduction to Meta-Analysis Methods",
    "section": "",
    "text": "(Borenstein et al. 2010)\n(Hartung 1999)\n(Cooper, Hedges, and Valentine 2009)\n(DerSimonian and Kacker 2007)\n\n\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and Random-Effects Models for Meta-Analysis.” Research Synthesis Methods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nCooper, Harris M., Larry V. Hedges, and Jeff C. Valentine, eds. 2009. The Handbook of Research Synthesis and Meta-Analysis. 2nd ed. New York: Russell Sage Foundation.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects Model for Meta-Analysis of Clinical Trials: An Update.” NIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nHartung, Joachim. 1999. “An Alternative Method for Meta-Analysis.” Biometrical Journal 41 (8): 901–16. https://doi.org/10.1002/(SICI)1521-4036(199912)41:8&lt;901::AID-BIMJ901&gt;3.0.CO;2-W."
  },
  {
    "objectID": "metaanalysis_intro.html#random-effects-model",
    "href": "metaanalysis_intro.html#random-effects-model",
    "title": "Introduction to Meta-Analysis Methods",
    "section": "",
    "text": "(Borenstein et al. 2010)\n(Hartung 1999)\n(Cooper, Hedges, and Valentine 2009)\n(DerSimonian and Kacker 2007)\n\n\n\n\nBorenstein, Michael, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein. 2010. “A Basic Introduction to Fixed-Effect and Random-Effects Models for Meta-Analysis.” Research Synthesis Methods 1 (2): 97–111. https://doi.org/10.1002/jrsm.12.\n\n\nCooper, Harris M., Larry V. Hedges, and Jeff C. Valentine, eds. 2009. The Handbook of Research Synthesis and Meta-Analysis. 2nd ed. New York: Russell Sage Foundation.\n\n\nDerSimonian, Rebecca, and Raghu N. Kacker. 2007. “Random-Effects Model for Meta-Analysis of Clinical Trials: An Update.” NIST 28 (January): 105–14. https://www.nist.gov/publications/random-effects-model-meta-analysis-clinical-trials-update.\n\n\nHartung, Joachim. 1999. “An Alternative Method for Meta-Analysis.” Biometrical Journal 41 (8): 901–16. https://doi.org/10.1002/(SICI)1521-4036(199912)41:8&lt;901::AID-BIMJ901&gt;3.0.CO;2-W."
  },
  {
    "objectID": "unreliability.html",
    "href": "unreliability.html",
    "title": "Unreliability",
    "section": "",
    "text": "In general terms, measurement is the process of quantifying an attribute or characteristic of something. In scientific measurement, the measurand is the quantity of the attribute we intend to measure. In the psychological sciences, measurands usually take the form of constructs such as intelligence or anxiety. The goal of measurement is to produce quantities (i.e., scores) that accurately reflect the measurand. It is important to note that measures are not all created equal, some perform better than others. Ideally, measures should produce scores that are consistent and repeatable, this is referred to as the reliability of a measure. A high quality measure should produce highly reliable scores. This section will review what reliability is in theory, how to estimate reliability, and how to adjust effect sizes for measurement error.\n\n\n\nTrue score theory (or classical test theory) is a mathematical formalization of scores obtained from measurements. The true score model assumes that each individual has a true score, \\(t\\), that stays constant over measurements. Observed scores, \\(x_m\\), can vary between measurements, \\(m\\). This variation is due to measurement-specific error, \\(e_m\\).\n\\[\nx_{m} = t+e_{m}\n\\]\nThe true score can be defined as the expected value (i.e., the mean) of observed scores over an infinite number of repeated measurements such that, \\(t=\\mathbb{E}_{m\\rightarrow\\infty}[x_{m}]\\). Therefore, it is also assumed that the expectation of measurement-specific error is zero, \\(\\mathbb{E}_{m\\rightarrow\\infty}[e_{m}]=0\\). It follows from these assumptions that the covariance between errors and true scores is zero (\\(\\sigma_{et}=0\\)) and the covariance between error scores in parallel measurements is zero (\\(\\sigma_{e e'}=0\\)). The independence between true scores and errors provide convenient parsing of the variance in observed scores (\\(\\sigma^2_{x_m}\\)) into components of variance in true scores (\\(\\sigma_t^2\\)) and errors (\\(\\sigma_{e_m}^2\\)),\n\\[\n\\sigma_{x_m}^2 = \\sigma_t^2 + \\sigma_{e_m}^2\n\\] If \\(\\sigma_{e_m}^2 &gt; 0\\) then the measurement has imperfect reliability, that is, observed scores are not identical to true scores. In practice, this is almost always the case. Reliability can be defined as the square correlation between observed scores and true scores, \\(r_{xt}^2\\), or the correlation between observed scores in parallel measurements, \\(r_{xx'}=r_{xt}^2\\).\n\n\n\nStructural model illustrating the relationship between true scores, observed scores, and error scores. The pink circle labeled \\(t\\) indicates the true scores, the blue squares labeled with \\(x\\) and \\(x'\\) represent observed scores on parallel measurements, and the red \\(e\\) denotes error. Correlations between \\(t\\), \\(x\\), and \\(x'\\) are in terms of reliability (\\(r_{xx'}\\)). Note that \\(\\sqrt{r_{xx'}}=r_{xt}\\).\n\n\nGiven that errors do not co-vary between parallel measurements and true scores are held constant over measurements, it becomes apparent that the covariance between observed scores produced from parallel measurements must solely be attributable to true score variance, \\(\\sigma_{xx'}=\\sigma_t^2\\). The covariance in observed scores can be standardized to obtain the correlation coefficient between parallel measurements (i.e., the reliability), such that, \\(r_{xx'}=\\sigma_{xx'}/\\sigma_x\\sigma_{x'} = \\sigma_t /\\sigma^2_{x_m}\\). Therefore reliability can be expressed in a few forms different forms\n\\[\nr_{xx'} =r^2_{xt} = \\frac{\\sigma_t^2}{\\sigma_t^2+\\sigma_{e_m}^2} = \\frac{\\sigma_t^2}{\\sigma_{x_m}^2}\n\\]\nIn the literature, the correlation between observed and true scores, \\(r_{xt}\\), is often referred to as the “measure quality index” (Hunter and Schmidt 1990), however measure quality encompasses both reliability and validity. A measure can demonstrate high reliability even though the scores produced by the measure do not accurately reflect the measurand (the quantity that we are intending to measure). For example, if an individual were to step on a weight scale with shoes on, the weight presented on the scale would be highly reliable, namely, if the individual were to repeat this process, they would achieve highly similar results. Nevertheless, the observed weight is systematically biased upward by the weight of the shoes. Therefore if a measure is reliable it does not logically follow that the measure is necessarily valid.\n\n\n\nIn practice, reliability must be estimated through indirect methods, since true scores and errors are unknown. Their are many estimators that can be used however, we will go over three of the most common approaches: coefficient alpha, split-half, and test-retest reliability.\n\n\nMaybe the most conventionally reported reliability estimator in the psychological sciences is coefficient alpha, also referred to as Cronbach’s alpha or internal consistency. Alpha has the benefit of being computationally convenient, but it also brings along many assumptions that are often violated in practice (Haertel 2006; Sijtsma 2009). A common strategy in the social sciences is to take \\(k\\) noisy measurements of some construct (e.g., likert responses in a psychological scale) and combine them to create a more stable composite score (\\(x_C\\)) that better estimates the individlatrue score, \\[\nx_c = x_1 + x_2 +...+x_k= \\sum^k_{m=1}x_m\n\\]Coefficient alpha is represents the reliability of this composite. Coefficient alpha only requires three parameters to calculate, the number of measurements (\\(k\\)), the variances of each items ( \\(\\sigma^2_{x_m}\\)), and the variance of the composite score (\\(\\sigma^2_{x_c}\\)),\n\\[\nr_{x_c x_c',\\alpha} = \\frac{k}{k-1}\\left( 1 - \\frac{\\sum_{m=1}^k \\sigma^2_{x_m}}{\\sigma^2_{x_c}} \\right)\n\\]\nWith tighter assumptions [i.e., strictly parallel forms; Haertel (2006)], the formula for coefficient alpha can be simplified to just two parameters: the number of measurements and the average correlation between measured scores (\\(\\bar{r}_{x_i x_j}\\), where \\(i\\neq j\\)). This formula is known as Spearman-Brown’s prophecy,\n\\[\nr_{x_cx_c',\\text{SB}}= \\frac{k \\bar{r}_{x_i x_j}}{1+(k-1)\\bar{r}_{x_i x_j}}\n\\]\nThis can be simplified further if we only have two parallel measurements. This formulation is traditionally called split-half reliability:\n\\[\nr_{x_cx_c',\\text{SH}}= \\frac{2r_{x_1 x_2}}{1+r_{x_1 x_2}}\n\\]\nAll of these reliability estimators measure internal consistency, therefore they do not account for error outside of the measurement-specific error. There are other sources of error that internal consistency reliability estimates do not account for, such as transient error or rater-specific error.\n\n\n\n\nRPython\n\n\nSimulate Data: Let us simulate a data set of 50 individuals where each observed score has the same true score with some error.\n\nset.seed(343)\nt = rnorm(50, 0, 1) # simulate 50 true scores\nx1 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 1\nx2 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 2\nx3 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 3\nx4 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 4\nxc = x1 + x2 + x3 + x4 # sum observed scores to create composite score \n\nCalculate Coefficient Alpha Reliability:\n\n# step 1. calculate variance of observed (measured) scores\nvar_xm = c(var(x1),var(x2),var(x3),var(x4))\n# step 2. calculate variance of composite score\nvar_xc = var(xc)\n# step 3. get number of items (k)\nk = length(var_xm)\n# step 4. calculate coefficient alpha reliability\nrel_alpha = k / (k-1) * (1 - sum(var_xm)/var_xc)\n\nprint(round(rel_alpha,3)) \n\n[1] 0.775\n\n\nCalculate Reliability via Spearman-Brown’s Prophecy:\n\n# step 1. get correlation matrix between all observed scores\ncorr_mat = cor(cbind(x1,x2,x3,x4))\n# step 2. average off-diagonal elements of matrix\ndiag(corr_mat) &lt;- NA\nrxixj = mean(corr_mat, na.rm = TRUE)\n# step 3. get number of items (k)\nk = dim(corr_mat)[1]\n# step 4. calculate Spearman-Brown reliability\nrel_SB = k * rxixj / (1 + (k-1) * rxixj)\n\nprint(round(rel_SB,3)) \n\n[1] 0.775\n\n\nCalculate the Split-Half Reliability:\n\n# step 1. make composite scores for each half of the observed scores\nxc1 = x1 + x2\nxc2 = x3 + x4\n# step 2. calculate the correlation between the scores of both halves\nrx1x2 = cor(xc1,xc2)\n# step 3. calculate the split-half reliability\nrel_SH = 2*rx1x2 / (1 + rx1x2)\n\nprint(round(rel_SH,3)) \n\n[1] 0.824\n\n\nTrue Reliability: Lets see how the results compare to the squared correlation of the true scores and our composite score (true reliability).\n\nrxt = cor(xc,t)\nprint(round(rxt^2,3)) \n\n[1] 0.753\n\n\nIn this case, the reliability estimates do a fairly good job of estimating the true reliability of the observed scores.\n\n\nSimulate Data: Let us simulate a data set of 50 individuals where each observed score has the same true score with some error. To calculate the necessary statistics, we will import the numpy package.\n\nimport numpy as np\nnp.random.seed(343)\nt = np.random.normal(0, 1, 50) # simulate 50 true scores\nx1 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 1\nx2 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 2\nx3 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 3\nx4 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 4\nxc = x1 + x2 + x3 + x4 # sum observed scores to create composite score \n\nCalculate Coefficient Alpha Reliability:\n\n# step 1. calculate variance of observed (measured) scores\nvar_xm = [np.var(x1),np.var(x2),np.var(x3),np.var(x4)]\n# step 2. calculate variance of composite score\nvar_xc = np.var(xc)\n# step 3. get number of items (k)\nk = len(var_xm)\n# step 4. calculate coefficient alpha reliability\nrel_alpha = k / (k-1) * (1 - sum(var_xm)/var_xc)\n\nprint(round(rel_alpha,3)) \n\n0.769\n\n\nCalculate Reliability fromSpearman-Brown’s prophecy formula:\n\n# step 1. get correlation matrix between all observed scores\ncorr_mat = np.corrcoef([x1,x2,x3,x4])\n# step 2. average off-diagonal elements of matrix\nrxixj = np.mean(corr_mat[~np.eye(k,dtype=bool)])\n# step 3. get number of items (k)\nk = len(corr_mat)\n# step 4. calculate Spearman-Brown reliability\nrel_SB = k * rxixj / (1 + (k-1) * rxixj)\n\nprint(round(rel_SB,3)) \n\n0.772\n\n\nCalculate Split-Half Reliability:\n\n# step 1. make composite scores for each half of the observed scores\nxc1 = x1 + x2\nxc2 = x3 + x4\n# step 2. calculate the correlation between the scores of both halves\nrx1x2 = np.corrcoef(xc1,xc2)[0,1]\n# step 3. calculate the split-half reliability\nrel_SH = 2*rx1x2 / (1 + rx1x2)\n\nprint(round(rel_SH,3)) \n\n0.772\n\n\nTrue Reliability: Lets see how the results compare to the squared correlation of the true scores and our composite score (true reliability).\n\n# display values\nrxt = np.corrcoef(xc,t)[0,1]\nprint(round(rxt**2,3)) \n\n0.791\n\n\nIn this case, the reliability estimates do a fairly good job of estimating the true reliability of the observed scores. There are also functions within the `psych` package that allow you to easily calculate coefficient alpha () among other reliability estimators\n\n\n\n\n\n\nThere measurement errors that exist outside of the measurement instrument itself. Transient errors represent fluctuations in observed scores over time. These fluctuations (e.g., weight over the course of a single day) add extraneous within-person variance that can mask true scores (i.e., expectation of observed scores). For example, if a researcher wants to investigate how individuals differ in processing speed, then variation within an individual’s scores across multiple testing sessions would be considered error since the goal of the study is to investigate between-person variation. Considering transient fluctuations as error depends on the research goal, so it is important for researchers to take care in considering which variance components should be considered error in their study. To estimate test-retest reliability, we can compute the pearson correlation coefficient between the measurement at time 1 (\\(x_{T_{1}}\\)) and the second measurement at time 2 (\\(x_{T_{2}}\\)).\n\\[\nr_{xx',\\text{TR}}= r_{x_{T_1}x_{T_2}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n(Haertel 2006)\n(Schmidt, Le, and Ilies 2003)\n(Gliem and Gliem 2003)\n(Bobko, Roth, and Bobko 2001)\n(Mendoza and Mumford 1987)\n(Brennan 2010)\n(Viswanathan 2005)\n(Viswesvaran et al. 2014)\n(Sijtsma 2009)\n(Charles 2005)\n(Spearman 1904)\n\n\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001. “Correcting the Effect Size of d for Range Restriction and Unreliability.” Organizational Research Methods - ORGAN RES METHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nBrennan, Robert L. 2010. “Generalizability Theory and Classical Test Theory.” Applied Measurement in Education 24 (1): 1–21. https://doi.org/10.1080/08957347.2011.532417.\n\n\nCharles, Eric. 2005. “The Correction for Attenuation Due to Measurement Error: Clarifying Concepts and Creating Confidence Sets.” Psychological Methods 10 (July): 206–26. https://doi.org/10.1037/1082-989X.10.2.206.\n\n\nGliem, Joseph A., and Rosemary R. Gliem. 2003. “Calculating, Interpreting, And Reporting Cronbach’s Alpha Reliability Coefficient For Likert-Type Scales.” https://scholarworks.iupui.edu/handle/1805/344.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nMendoza, Jorge L., and Michael Mumford. 1987. “Corrections for Attenuation and Range Restriction on the Predictor.” Journal of Educational Statistics 12 (3): 282–93. https://doi.org/10.3102/10769986012003282.\n\n\nSchmidt, Frank L., Huy Le, and Remus Ilies. 2003. “Beyond Alpha: An Empirical Examination of the Effects of Different Sources of Measurement Error on Reliability Estimates for Measures of Individual-Differences Constructs.” Psychological Methods 8: 206–24. https://doi.org/10.1037/1082-989X.8.2.206.\n\n\nSijtsma, Klaas. 2009. “On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s Alpha.” Psychometrika 74 (1): 107–20. https://doi.org/10.1007/s11336-008-9101-0.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” International Journal of Epidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nViswanathan, Madhu. 2005. Measurement Error and Research Design. SAGE.\n\n\nViswesvaran, Chockalingam, Deniz S. Ones, Frank L. Schmidt, Huy Le, and In-Sue Oh. 2014. “Measurement Error Obfuscates Scientific Knowledge: Path to Cumulative Knowledge Requires Corrections for Unreliability and Psychometric Meta-Analyses.” Industrial and Organizational Psychology 7 (4): 507–18. https://doi.org/10.1017/S1754942600006799."
  },
  {
    "objectID": "unreliability.html#introduction",
    "href": "unreliability.html#introduction",
    "title": "Unreliability",
    "section": "",
    "text": "In general terms, measurement is the process of quantifying an attribute or characteristic of something. In scientific measurement, the measurand is the quantity of the attribute we intend to measure. In the psychological sciences, measurands usually take the form of constructs such as intelligence or anxiety. The goal of measurement is to produce quantities (i.e., scores) that accurately reflect the measurand. It is important to note that measures are not all created equal, some perform better than others. Ideally, measures should produce scores that are consistent and repeatable, this is referred to as the reliability of a measure. A high quality measure should produce highly reliable scores. This section will review what reliability is in theory, how to estimate reliability, and how to adjust effect sizes for measurement error."
  },
  {
    "objectID": "unreliability.html#reliability-in-true-score-theory",
    "href": "unreliability.html#reliability-in-true-score-theory",
    "title": "Unreliability",
    "section": "",
    "text": "True score theory (or classical test theory) is a mathematical formalization of scores obtained from measurements. The true score model assumes that each individual has a true score, \\(t\\), that stays constant over measurements. Observed scores, \\(x_m\\), can vary between measurements, \\(m\\). This variation is due to measurement-specific error, \\(e_m\\).\n\\[\nx_{m} = t+e_{m}\n\\]\nThe true score can be defined as the expected value (i.e., the mean) of observed scores over an infinite number of repeated measurements such that, \\(t=\\mathbb{E}_{m\\rightarrow\\infty}[x_{m}]\\). Therefore, it is also assumed that the expectation of measurement-specific error is zero, \\(\\mathbb{E}_{m\\rightarrow\\infty}[e_{m}]=0\\). It follows from these assumptions that the covariance between errors and true scores is zero (\\(\\sigma_{et}=0\\)) and the covariance between error scores in parallel measurements is zero (\\(\\sigma_{e e'}=0\\)). The independence between true scores and errors provide convenient parsing of the variance in observed scores (\\(\\sigma^2_{x_m}\\)) into components of variance in true scores (\\(\\sigma_t^2\\)) and errors (\\(\\sigma_{e_m}^2\\)),\n\\[\n\\sigma_{x_m}^2 = \\sigma_t^2 + \\sigma_{e_m}^2\n\\] If \\(\\sigma_{e_m}^2 &gt; 0\\) then the measurement has imperfect reliability, that is, observed scores are not identical to true scores. In practice, this is almost always the case. Reliability can be defined as the square correlation between observed scores and true scores, \\(r_{xt}^2\\), or the correlation between observed scores in parallel measurements, \\(r_{xx'}=r_{xt}^2\\).\n\n\n\nStructural model illustrating the relationship between true scores, observed scores, and error scores. The pink circle labeled \\(t\\) indicates the true scores, the blue squares labeled with \\(x\\) and \\(x'\\) represent observed scores on parallel measurements, and the red \\(e\\) denotes error. Correlations between \\(t\\), \\(x\\), and \\(x'\\) are in terms of reliability (\\(r_{xx'}\\)). Note that \\(\\sqrt{r_{xx'}}=r_{xt}\\).\n\n\nGiven that errors do not co-vary between parallel measurements and true scores are held constant over measurements, it becomes apparent that the covariance between observed scores produced from parallel measurements must solely be attributable to true score variance, \\(\\sigma_{xx'}=\\sigma_t^2\\). The covariance in observed scores can be standardized to obtain the correlation coefficient between parallel measurements (i.e., the reliability), such that, \\(r_{xx'}=\\sigma_{xx'}/\\sigma_x\\sigma_{x'} = \\sigma_t /\\sigma^2_{x_m}\\). Therefore reliability can be expressed in a few forms different forms\n\\[\nr_{xx'} =r^2_{xt} = \\frac{\\sigma_t^2}{\\sigma_t^2+\\sigma_{e_m}^2} = \\frac{\\sigma_t^2}{\\sigma_{x_m}^2}\n\\]\nIn the literature, the correlation between observed and true scores, \\(r_{xt}\\), is often referred to as the “measure quality index” (Hunter and Schmidt 1990), however measure quality encompasses both reliability and validity. A measure can demonstrate high reliability even though the scores produced by the measure do not accurately reflect the measurand (the quantity that we are intending to measure). For example, if an individual were to step on a weight scale with shoes on, the weight presented on the scale would be highly reliable, namely, if the individual were to repeat this process, they would achieve highly similar results. Nevertheless, the observed weight is systematically biased upward by the weight of the shoes. Therefore if a measure is reliable it does not logically follow that the measure is necessarily valid."
  },
  {
    "objectID": "unreliability.html#estimating-reliability",
    "href": "unreliability.html#estimating-reliability",
    "title": "Unreliability",
    "section": "",
    "text": "In practice, reliability must be estimated through indirect methods, since true scores and errors are unknown. Their are many estimators that can be used however, we will go over three of the most common approaches: coefficient alpha, split-half, and test-retest reliability.\n\n\nMaybe the most conventionally reported reliability estimator in the psychological sciences is coefficient alpha, also referred to as Cronbach’s alpha or internal consistency. Alpha has the benefit of being computationally convenient, but it also brings along many assumptions that are often violated in practice (Haertel 2006; Sijtsma 2009). A common strategy in the social sciences is to take \\(k\\) noisy measurements of some construct (e.g., likert responses in a psychological scale) and combine them to create a more stable composite score (\\(x_C\\)) that better estimates the individlatrue score, \\[\nx_c = x_1 + x_2 +...+x_k= \\sum^k_{m=1}x_m\n\\]Coefficient alpha is represents the reliability of this composite. Coefficient alpha only requires three parameters to calculate, the number of measurements (\\(k\\)), the variances of each items ( \\(\\sigma^2_{x_m}\\)), and the variance of the composite score (\\(\\sigma^2_{x_c}\\)),\n\\[\nr_{x_c x_c',\\alpha} = \\frac{k}{k-1}\\left( 1 - \\frac{\\sum_{m=1}^k \\sigma^2_{x_m}}{\\sigma^2_{x_c}} \\right)\n\\]\nWith tighter assumptions [i.e., strictly parallel forms; Haertel (2006)], the formula for coefficient alpha can be simplified to just two parameters: the number of measurements and the average correlation between measured scores (\\(\\bar{r}_{x_i x_j}\\), where \\(i\\neq j\\)). This formula is known as Spearman-Brown’s prophecy,\n\\[\nr_{x_cx_c',\\text{SB}}= \\frac{k \\bar{r}_{x_i x_j}}{1+(k-1)\\bar{r}_{x_i x_j}}\n\\]\nThis can be simplified further if we only have two parallel measurements. This formulation is traditionally called split-half reliability:\n\\[\nr_{x_cx_c',\\text{SH}}= \\frac{2r_{x_1 x_2}}{1+r_{x_1 x_2}}\n\\]\nAll of these reliability estimators measure internal consistency, therefore they do not account for error outside of the measurement-specific error. There are other sources of error that internal consistency reliability estimates do not account for, such as transient error or rater-specific error.\n\n\n\n\nRPython\n\n\nSimulate Data: Let us simulate a data set of 50 individuals where each observed score has the same true score with some error.\n\nset.seed(343)\nt = rnorm(50, 0, 1) # simulate 50 true scores\nx1 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 1\nx2 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 2\nx3 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 3\nx4 = t + rnorm(50, 0, 1) # simulate observed scores for measurement 4\nxc = x1 + x2 + x3 + x4 # sum observed scores to create composite score \n\nCalculate Coefficient Alpha Reliability:\n\n# step 1. calculate variance of observed (measured) scores\nvar_xm = c(var(x1),var(x2),var(x3),var(x4))\n# step 2. calculate variance of composite score\nvar_xc = var(xc)\n# step 3. get number of items (k)\nk = length(var_xm)\n# step 4. calculate coefficient alpha reliability\nrel_alpha = k / (k-1) * (1 - sum(var_xm)/var_xc)\n\nprint(round(rel_alpha,3)) \n\n[1] 0.775\n\n\nCalculate Reliability via Spearman-Brown’s Prophecy:\n\n# step 1. get correlation matrix between all observed scores\ncorr_mat = cor(cbind(x1,x2,x3,x4))\n# step 2. average off-diagonal elements of matrix\ndiag(corr_mat) &lt;- NA\nrxixj = mean(corr_mat, na.rm = TRUE)\n# step 3. get number of items (k)\nk = dim(corr_mat)[1]\n# step 4. calculate Spearman-Brown reliability\nrel_SB = k * rxixj / (1 + (k-1) * rxixj)\n\nprint(round(rel_SB,3)) \n\n[1] 0.775\n\n\nCalculate the Split-Half Reliability:\n\n# step 1. make composite scores for each half of the observed scores\nxc1 = x1 + x2\nxc2 = x3 + x4\n# step 2. calculate the correlation between the scores of both halves\nrx1x2 = cor(xc1,xc2)\n# step 3. calculate the split-half reliability\nrel_SH = 2*rx1x2 / (1 + rx1x2)\n\nprint(round(rel_SH,3)) \n\n[1] 0.824\n\n\nTrue Reliability: Lets see how the results compare to the squared correlation of the true scores and our composite score (true reliability).\n\nrxt = cor(xc,t)\nprint(round(rxt^2,3)) \n\n[1] 0.753\n\n\nIn this case, the reliability estimates do a fairly good job of estimating the true reliability of the observed scores.\n\n\nSimulate Data: Let us simulate a data set of 50 individuals where each observed score has the same true score with some error. To calculate the necessary statistics, we will import the numpy package.\n\nimport numpy as np\nnp.random.seed(343)\nt = np.random.normal(0, 1, 50) # simulate 50 true scores\nx1 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 1\nx2 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 2\nx3 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 3\nx4 = t + np.random.normal(0, 1, 50) # simulate observed scores for measurement 4\nxc = x1 + x2 + x3 + x4 # sum observed scores to create composite score \n\nCalculate Coefficient Alpha Reliability:\n\n# step 1. calculate variance of observed (measured) scores\nvar_xm = [np.var(x1),np.var(x2),np.var(x3),np.var(x4)]\n# step 2. calculate variance of composite score\nvar_xc = np.var(xc)\n# step 3. get number of items (k)\nk = len(var_xm)\n# step 4. calculate coefficient alpha reliability\nrel_alpha = k / (k-1) * (1 - sum(var_xm)/var_xc)\n\nprint(round(rel_alpha,3)) \n\n0.769\n\n\nCalculate Reliability fromSpearman-Brown’s prophecy formula:\n\n# step 1. get correlation matrix between all observed scores\ncorr_mat = np.corrcoef([x1,x2,x3,x4])\n# step 2. average off-diagonal elements of matrix\nrxixj = np.mean(corr_mat[~np.eye(k,dtype=bool)])\n# step 3. get number of items (k)\nk = len(corr_mat)\n# step 4. calculate Spearman-Brown reliability\nrel_SB = k * rxixj / (1 + (k-1) * rxixj)\n\nprint(round(rel_SB,3)) \n\n0.772\n\n\nCalculate Split-Half Reliability:\n\n# step 1. make composite scores for each half of the observed scores\nxc1 = x1 + x2\nxc2 = x3 + x4\n# step 2. calculate the correlation between the scores of both halves\nrx1x2 = np.corrcoef(xc1,xc2)[0,1]\n# step 3. calculate the split-half reliability\nrel_SH = 2*rx1x2 / (1 + rx1x2)\n\nprint(round(rel_SH,3)) \n\n0.772\n\n\nTrue Reliability: Lets see how the results compare to the squared correlation of the true scores and our composite score (true reliability).\n\n# display values\nrxt = np.corrcoef(xc,t)[0,1]\nprint(round(rxt**2,3)) \n\n0.791\n\n\nIn this case, the reliability estimates do a fairly good job of estimating the true reliability of the observed scores. There are also functions within the `psych` package that allow you to easily calculate coefficient alpha () among other reliability estimators\n\n\n\n\n\n\nThere measurement errors that exist outside of the measurement instrument itself. Transient errors represent fluctuations in observed scores over time. These fluctuations (e.g., weight over the course of a single day) add extraneous within-person variance that can mask true scores (i.e., expectation of observed scores). For example, if a researcher wants to investigate how individuals differ in processing speed, then variation within an individual’s scores across multiple testing sessions would be considered error since the goal of the study is to investigate between-person variation. Considering transient fluctuations as error depends on the research goal, so it is important for researchers to take care in considering which variance components should be considered error in their study. To estimate test-retest reliability, we can compute the pearson correlation coefficient between the measurement at time 1 (\\(x_{T_{1}}\\)) and the second measurement at time 2 (\\(x_{T_{2}}\\)).\n\\[\nr_{xx',\\text{TR}}= r_{x_{T_1}x_{T_2}}\n\\]"
  },
  {
    "objectID": "unreliability.html#unreliability-and-effect-size-bias",
    "href": "unreliability.html#unreliability-and-effect-size-bias",
    "title": "Unreliability",
    "section": "",
    "text": "(Haertel 2006)\n(Schmidt, Le, and Ilies 2003)\n(Gliem and Gliem 2003)\n(Bobko, Roth, and Bobko 2001)\n(Mendoza and Mumford 1987)\n(Brennan 2010)\n(Viswanathan 2005)\n(Viswesvaran et al. 2014)\n(Sijtsma 2009)\n(Charles 2005)\n(Spearman 1904)\n\n\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001. “Correcting the Effect Size of d for Range Restriction and Unreliability.” Organizational Research Methods - ORGAN RES METHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nBrennan, Robert L. 2010. “Generalizability Theory and Classical Test Theory.” Applied Measurement in Education 24 (1): 1–21. https://doi.org/10.1080/08957347.2011.532417.\n\n\nCharles, Eric. 2005. “The Correction for Attenuation Due to Measurement Error: Clarifying Concepts and Creating Confidence Sets.” Psychological Methods 10 (July): 206–26. https://doi.org/10.1037/1082-989X.10.2.206.\n\n\nGliem, Joseph A., and Rosemary R. Gliem. 2003. “Calculating, Interpreting, And Reporting Cronbach’s Alpha Reliability Coefficient For Likert-Type Scales.” https://scholarworks.iupui.edu/handle/1805/344.\n\n\nHaertel, Edward H. 2006. “3. Reliability.” In, 4th ed.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nMendoza, Jorge L., and Michael Mumford. 1987. “Corrections for Attenuation and Range Restriction on the Predictor.” Journal of Educational Statistics 12 (3): 282–93. https://doi.org/10.3102/10769986012003282.\n\n\nSchmidt, Frank L., Huy Le, and Remus Ilies. 2003. “Beyond Alpha: An Empirical Examination of the Effects of Different Sources of Measurement Error on Reliability Estimates for Measures of Individual-Differences Constructs.” Psychological Methods 8: 206–24. https://doi.org/10.1037/1082-989X.8.2.206.\n\n\nSijtsma, Klaas. 2009. “On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s Alpha.” Psychometrika 74 (1): 107–20. https://doi.org/10.1007/s11336-008-9101-0.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” International Journal of Epidemiology 39 (5): 1137–50. https://doi.org/10.1093/ije/dyq191.\n\n\nViswanathan, Madhu. 2005. Measurement Error and Research Design. SAGE.\n\n\nViswesvaran, Chockalingam, Deniz S. Ones, Frank L. Schmidt, Huy Le, and In-Sue Oh. 2014. “Measurement Error Obfuscates Scientific Knowledge: Path to Cumulative Knowledge Requires Corrections for Unreliability and Psychometric Meta-Analyses.” Industrial and Organizational Psychology 7 (4): 507–18. https://doi.org/10.1017/S1754942600006799."
  },
  {
    "objectID": "misclassification.html",
    "href": "misclassification.html",
    "title": "Misclassification",
    "section": "",
    "text": "Misclassification\n(Chyou 2007)\n(Wiernik and Dahlke 2020)\n(Hunter and Schmidt 1990)\n\n\n\n\nChyou, Po-Huang. 2007. “Patterns of Bias Due to Differential Misclassification by Casecontrol Status in a Casecontrol Study.” European Journal of Epidemiology 22 (1): 7–17. https://doi.org/10.1007/s10654-006-9078-x.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "artifact_correction_metaanalysis.html",
    "href": "artifact_correction_metaanalysis.html",
    "title": "Artifact Correction Meta-Analysis",
    "section": "",
    "text": "(Hunter and Schmidt 1990)\n(Wiernik and Dahlke 2020)\n(Schmidt and Hunter 1977)\n(Murphy 2003)\n(Viswesvaran and Ones 1995)\n(Raju and Burke 1983)\n(Callender and Osburn 1980)\n\n\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test of a New Model for Validity Generalization.” Journal of Applied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nMurphy, Kevin R. 2003. Validity Generalization: A Critical Review. Psychology Press.\n\n\nRaju, Nambury, and Michael Burke. 1983. “Two Procedures for Studying Validity Generalization.” Journal of Applied Psychology 68 (August): 382–95. https://doi.org/10.1037/0021-9010.68.3.382.\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General Solution to the Problem of Validity Generalization.” Journal of Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nViswesvaran, Chockalingam, and Deniz S. Ones. 1995. “Theory Testing: Combining Psychometric Meta-Analysis and Structural Equations Modeling.” Personnel Psychology 48 (4): 865–85. https://doi.org/10.1111/j.1744-6570.1995.tb01784.x.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "artifact_correction_metaanalysis.html#alternative-methods",
    "href": "artifact_correction_metaanalysis.html#alternative-methods",
    "title": "Artifact Correction Meta-Analysis",
    "section": "",
    "text": "(Hunter and Schmidt 1990)\n(Wiernik and Dahlke 2020)\n(Schmidt and Hunter 1977)\n(Murphy 2003)\n(Viswesvaran and Ones 1995)\n(Raju and Burke 1983)\n(Callender and Osburn 1980)\n\n\n\n\nCallender, John C., and H. G. Osburn. 1980. “Development and Test of a New Model for Validity Generalization.” Journal of Applied Psychology 65 (5): 543–58. https://doi.org/10.1037/0021-9010.65.5.543.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nMurphy, Kevin R. 2003. Validity Generalization: A Critical Review. Psychology Press.\n\n\nRaju, Nambury, and Michael Burke. 1983. “Two Procedures for Studying Validity Generalization.” Journal of Applied Psychology 68 (August): 382–95. https://doi.org/10.1037/0021-9010.68.3.382.\n\n\nSchmidt, Frank, and John Hunter. 1977. “Development of a General Solution to the Problem of Validity Generalization.” Journal of Applied Psychology 62 (October): 529–40. https://doi.org/10.1037/0021-9010.62.5.529.\n\n\nViswesvaran, Chockalingam, and Deniz S. Ones. 1995. “Theory Testing: Combining Psychometric Meta-Analysis and Structural Equations Modeling.” Personnel Psychology 48 (4): 865–85. https://doi.org/10.1111/j.1744-6570.1995.tb01784.x.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  },
  {
    "objectID": "small_samples.html",
    "href": "small_samples.html",
    "title": "Small Samples",
    "section": "",
    "text": "Small Samples\n(Hedges 1989)\n(Lin 2018)\n(Hedges 1981)\n(Fisher 1915)\n(Olkin and Pratt 1958)\n\n\n\n\nFisher, R. A. 1915. “Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population.” Biometrika 10 (4): 507–21. https://doi.org/10.2307/2331838.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator of Effect Size and Related Estimators.” Journal of Educational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\n———. 1989. “An Unbiased Correction for Sampling Error in Validity Generalization Studies.” Journal of Applied Psychology 74 (3): 469–77. https://doi.org/10.1037/0021-9010.74.3.469.\n\n\nLin, Lifeng. 2018. “Bias Caused by Sampling Error in Meta-Analysis with Small Sample Sizes.” PLOS ONE 13 (9): e0204056. https://doi.org/10.1371/journal.pone.0204056.\n\n\nOlkin, Ingram, and John W. Pratt. 1958. “Unbiased Estimation of Certain Correlation Coefficients.” The Annals of Mathematical Statistics 29 (1): 201–11. https://www.jstor.org/stable/2237306."
  },
  {
    "objectID": "scale_coarseness.html",
    "href": "scale_coarseness.html",
    "title": "Scale Coarseness",
    "section": "",
    "text": "Scale Coarseness\n(Symonds 1924)\n(Aguinis, Pierce, and Culpepper 2009)\n(Krieg 1999)\n\n\n\n\nAguinis, Herman, Charles A Pierce, and Steven A Culpepper. 2009. “Scale Coarseness as a Methodological Artifact,” September.\n\n\nKrieg, Edward F. 1999. “Biases Induced by Coarse Measurement Scales.” Educational and Psychological Measurement 59 (5): 749–66. https://doi.org/10.1177/00131649921970125.\n\n\nSymonds, P. M. 1924. “On the Loss of Reliability in Ratings Due to Coarseness of the Scale.” Journal of Experimental Psychology 7 (6): 456–61. https://doi.org/10.1037/h0074469."
  },
  {
    "objectID": "direct_range_restriction.html",
    "href": "direct_range_restriction.html",
    "title": "Direct Range Restriction/Enhancement",
    "section": "",
    "text": "Direct Range Restriction/Enhancement\n(Bobko, Roth, and Bobko 2001)\n(Sackett and Yang 2000)\n(Law, Schmidt, and Hunter 1994)\n(Thorndike 1949)\n(Hunter and Schmidt 1990)\n(Wiernik and Dahlke 2020)\n\n\n\n\nBobko, Philip, Philip Roth, and Christopher Bobko. 2001. “Correcting the Effect Size of d for Range Restriction and Unreliability.” Organizational Research Methods - ORGAN RES METHODS 4 (January): 46–61. https://doi.org/10.1177/109442810141003.\n\n\nHunter, John E., and Frank L. Schmidt. 1990. Methods of meta-analysis: correcting error and bias in research findings. Newbury Park: Sage Publications.\n\n\nLaw, Kenneth, Frank Schmidt, and John Hunter. 1994. “Nonlinearity of Range Corrections in Meta-Analysis: Test of an Improved Procedure.” Journal of Applied Psychology 79 (December): 978–86. https://doi.org/10.1037/0021-9010.79.6.978.\n\n\nSackett, Paul R., and Hyuckseung Yang. 2000. “Correction for Range Restriction: An Expanded Typology.” Journal of Applied Psychology 85 (1): 112–18. https://doi.org/10.1037/0021-9010.85.1.112.\n\n\nThorndike, Robert L. 1949. Personnel Selection; Test and Measurement Techniques. Personnel Selection; Test and Measurement Techniques. Oxford, England: Wiley.\n\n\nWiernik, Brenton M., and Jeffrey A. Dahlke. 2020. “Obtaining Unbiased Results in Meta-Analysis: The Importance of Correcting for Statistical Artifacts.” Advances in Methods and Practices in Psychological Science 3 (1): 94–123. https://doi.org/10.1177/2515245919885611."
  }
]