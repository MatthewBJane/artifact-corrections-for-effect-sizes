# Group Misclassification

```{r color_scheme,echo = F,warning=F,message=F}

library(raster)
library(metR)
library(isoband)
library(latex2exp)
library(extrafont)
library(ggplot2)
library(ggdist)
library(patchwork)
library(MASS)


text_color_blue      = '#326982ff'
panel_color_blue     = '#f6fafbff'
lightmain_color_blue = '#a4cdd9ff'
main_color_blue      = '#5fa6bcff'
darkmain_color_blue  = '#397689ff'
border_color_blue    = '#5fa6bcff'

text_color_red       = '#a62675ff'
panel_color_red      = '#fdf6faff'
lightmain_color_red  = '#eeb4d7ff'
main_color_red       = '#d74ea2ff'
darkmain_color_red   = '#bf2986ff'
border_color_red     = '#d74ea2ff'




th_blue <- theme(aspect.ratio = 1,
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(color = text_color_blue),
        panel.background = element_rect(fill = panel_color_blue),
        panel.border = element_rect(fill = NA, color = border_color_blue,linewidth=1.2),
        axis.title = element_text(size=15, color = text_color_blue),
        axis.text.x = element_text(size=13, color = text_color_blue),
        axis.text.y = element_text(size=13, color = text_color_blue),
        axis.ticks = element_line(color = border_color_blue,linewidth=1)) 
  
th_red <- theme(aspect.ratio = 1,
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(color = text_color_red),
        panel.background = element_rect(fill = panel_color_red),
        panel.border = element_rect(fill = NA, color = border_color_red,linewidth=1.2),
        axis.title = element_text(size=15, color = text_color_red),
        axis.text.x = element_text(size=13, color = text_color_red),
        axis.text.y = element_text(size=13, color = text_color_red),
        axis.ticks = element_line(color = border_color_red,linewidth=1)) 
```

## Introduction

Group misclassification describes a situation where true group membership (e.g., people with a disorder) does not perfectly match the observed group membership (e.g., people *diagnosed* with a disorder). Group misclassification can be considered a type of measurement error where instead of accounting for errors in continuous variables (i.e., unreliability), group misclassification accounts for errors in categorical variables.

## Defining Group Misclassification

Misclassification can be defined as any deviations between true group membership and observed group membership. Let us imagine two groups, group $A$ and group $B$. In order to identify members of group $A$ and group $B$, we have to use some measurement procedure. We can also suppose that this measurement instrument produces imperfect group assignments, that is, people who are actually in group $A$ are sometimes assigned group $B$ and vice versa. We can visualize the performance of the classification procedure with a contingency table (see @tbl-class) between actual group membership ($G$) and observed group membership ($g$):

::: columns
::: {.column width="25%"}
:::

::: {.column width="50%"}
|                    |                    |                    |
|:------------------:|:------------------:|:------------------:|
|                    | $\boldsymbol{G=A}$ | $\boldsymbol{G=B}$ |
| $\boldsymbol{g=A}$ |      $n_{AA}$      |      $n{BA}$       |
| $\boldsymbol{g=B}$ |      $n_{AB}$      |      $n_{BB}$      |

: Contingency table between assigned group membership and actual group membership. {#tbl-class .hover}
:::

::: {.column width="25%"}
:::
:::

We can see from the contingency table that individual's who were correctly classified, would be labeled in the cell block $AA$ or $BB$ and those who were misclassified would belong to cells $BA$ and $AB$. Therefore we can define the proportion of individuals that are accurately classified as $p_{\text{acc}} = \frac{n_{AA} + n_{BB}}{n_{AA} + n_{BB} + n_{AB} + n_{BA}}$ whereas the proportion of people misclassified can be defined as $p_{\text{mis}} = \frac{n_{AB} + n_{BA}}{n_{AA} + n_{BB} + n_{AB} + n_{BA}}$. A high-quality classifier would would minimize $p_{\text{mis}}$ and maximize $p_{\text{acc}}$. Note that the proportion of people misclassified is inversely proportional to the proportion of people accurately classified such that, $p_{\text{mis}} = 1-p_{\text{acc}}$.

## Classification Reliability

Similar to quantifying reliability in continuous variables by calculating the correlation in parallel sets of observed scores, the same can be done in categorical variables. Instead of a contingency table between observed ($g$) and true ($G$) group membership, we will instead create a contingency table of two measurements producing two sets of observed group assignments ($g$ and $g'$). Measurements often will take the form of inter-rater assessments, for example, two clinician's diagnosis of Major Depressive Disorder (MDD) in the same sample of patients.

::: columns
::: {.column width="25%"}
:::

::: {.column width="50%"}
|                     |                    |                    |
|:-------------------:|:------------------:|:------------------:|
|                     | $\boldsymbol{g=A}$ | $\boldsymbol{g=B}$ |
| $\boldsymbol{g'=A}$ |      $n_{AA}$      |      $n_{BA}$      |
| $\boldsymbol{g'=B}$ |      $n_{AB}$      |      $n_{BB}$      |

: {.hover}
:::

::: {.column width="25%"}
:::
:::

To obtain the reliability of the group assignments, we can calculate the correlation coefficient between $g$ and $g'$. Since both variables are categorical, a Pearson correlation coefficient would be equivalent to a phi coefficient. The phi coefficient is often referred to as Matthew's correlation coefficient and is most frequently used as an index of performance of a binary classifier in machine learning. Let's denote the reliability (i.e., the correlation between $g$ and $g'$) as $\rho_{gg'}$. Remember that reliability from the chapter on unreliability can be defined as the square of the correlation between true scores and observed scores. As is the case here, we can define classification reliability as the square of the correlation between assigned group membership and actual group membership,

$$
\rho_{gg'} = \rho^2_{gG}
$$

There are a few ways to obtain a sample estimate of $\rho_{gg'}$ ($r_{gg'}$). The first way is to calculate the sample estimate directly from a contingency table,

$$
r_{gg'} = \frac{n_{AA}n_{BB}-n_{AB}n_{BA}}{\sqrt{(n_{AA}+n_{BA})(n_{AB}+n_{BB})(n_{AA}+n_{AB})(n_{BA}+n_{BB})}}.
$$

Where $n_{AA}$, $n_{BB}$, $n_{AB}$, and $n_{BA}$ are the number of subjects within their respective cells of the contingency table. If the values of the contingency table are not available, we can calculate the reliability from the $\chi^2$-statistic,

$$
r_{gg'} = \sqrt{\frac{\chi^2}{n}}.
$$

Where $n$ is the total sample size (sum of all cells). If the $\chi^2$-statistic is unavailable, we can approximate the reliability from the accuracy ($p_{\text{acc}}$) or the proportion of people misclassified ($p_{\text{mis}}$),

$$
r_{gg'} = (2p_{\text{acc}}-1)^2 = (1-2p_{\text{mis}})^2.
$$

This approximation assumes that the group sizes are approximately equal *and* the misclassification rates are approximately equal between groups. Otherwise, $r_{gg'}$ will be overestimated [@wiernik2020].

::: {.callout-note appearance="default" icon="false"}
## Applied Example in R

Let's say that a researcher wants to calculate the reliability of a clinician's diagnoses of major depressive disorder. They hire two clinicians to assign a sample of 100 patients to a control group or a major depressive disorder group. The researcher runs a $\chi^2$-test to test the association between the clinicians group assignments and it returns $\chi^2=54.5$. We can then calculate the reliability of classification using base R.

```{r,message=FALSE}
chi2 <- 54.5
n <- 100

# calculate reliability from chi squared statistic
rgg <- sqrt(chi2/n)

# print reliability
rgg
```

The clinicians show a fair level of agreement with a classification reliability of $r_{gg'}=.74$.
:::

## Correcting for Group Misclassification in Standardized Mean Difference

### Defining our Target Quantity

Our quantity of interest is the true score population standardized mean difference, $\delta_{GU}$, between actual members of group $A$ and group $B$ on the true scores of the dependent variable, $U$. However, the observed sample standardized mean difference ($d_{gY}$) is estimating the difference in observed scores between individuals who are assigned group to $A$ and group $B$. Non-differential error in the assignment of groups (i.e., group misclassification) will bias the observed correlation. We can model the observed standardized mean difference as a function of the target quantity, $\delta_{GU}$,

$$
d_{gY} = a\delta_{GU} + e.
$$

Where $a$ is the artifact attenuation factor and $e$ denotes the sampling error. Therefore an unbiased estimate of the target standardized mean difference can be obtained by dividing the observed standardized mean difference by an estimate of the artifact attenuation factor,

$$
d_{GU} = \frac{d_{gY}}{\hat{a}}.
$$

### Artifact Correction for Standardized Mean Difference

The standardized mean differences will become biased when subject's assigned groups differ from their actual group. This is partially due to the fact that the means of each group are driven closer to one another. Let us suppose that, on average, group $A$ and group $B$ score differently on some outcome, $Y$. When some subjects are erroneously assigned to the incorrect group, the observed mean within each group will reflect a weighted average true means of both groups. This is due to the fact that the misclassified individuals are being drawn from a population with a different mean. To calculate the mean of the observed groups we must incorporate the true mean of the correctly classified subjects and the misclassified subjects,

$$
\overline{Y}^\text{obs}_A = \left(\frac{n_{AA}}{n_{AA}+n_{BA}}\right)\overline{Y}^\text{true}_A + \left(\frac{n_{BA}}{n_{AA}+n_{BA}}\right)\overline{Y}^\text{true}_B
$$

$$
\overline{Y}^\text{obs}_A = \left(\frac{n_{BB}}{n_{BB}+n_{AB}}\right)\overline{Y}^\text{true}_B + \left(\frac{n_{AB}}{n_{BB}+n_{AB}}\right)\overline{Y}^\text{true}_A.
$$

From the above equations, it becomes evident that as the number of misclassified individuals increases ($n_{AB}$ and $n_{BA}$), the observed means of each group gradually converge towards each other. As the means converge, the standardized mean difference will correspondingly shift toward zero. To illustrate this phenomenon, @fig-no-mis shows the distributions for groups $A$ and $B$ without any misclassification. In this case, there is no attenuation of the standardized mean difference.

```{r, echo = FALSE,warning=FALSE,fig.height=5}
#| id: fig-no-mis
#| fig-cap: Distributions of scores without misclassification. True mean difference and observed mean differ only due to sampling error. Red squares denote actual group $A$ members, blue circles denote actual group $B$ members.

set.seed(25)
n = 50
P_mis = 5

`Observed Score (Y)` = c(rnorm(n,30,8) , rnorm(n,38,8))

`Assigned Group (g)` =  c(rep('A',n) , rep('B',n))
ord = c(rep(4,P_mis),rep(1,n-P_mis),
        rep(3,P_mis),rep(2,n-P_mis))
`Actual Group` =  c(rep('B',n) , rep('A',n))

DIFF = (mean(`Observed Score (Y)`[`Assigned Group (g)`=='B'])-mean(`Observed Score (Y)`[`Assigned Group (g)`=='A']))
SD = (sd(`Observed Score (Y)`[`Assigned Group (g)` == 'A']) + sd(`Observed Score (Y)`[`Assigned Group (g)` == 'B']))/2
d = round(DIFF / SD,2)

ggplot(data = NULL, aes(y = `Assigned Group (g)`,
                        x = `Observed Score (Y)`,
                        shape = `Actual Group`,
                        fill = `Actual Group`,
                        color = `Actual Group`,
                        order = ord,
                        group = NA)) +
  geom_dots(side = "top", scale = 0.75,stackratio = 1.1,dotsize=.9, 
            position = "dodge",linewidth = 1) + 
  theme_ggdist() + 
  theme(aspect.ratio = 1,
        axis.text.y = element_text(size=14),
        axis.text.x = element_text(size=13),
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
        title = element_text(size=13),
        legend.position = "none") +  
  scale_fill_manual(values = c(main_color_blue,main_color_red)) + 
  scale_color_manual(values = c(main_color_blue,main_color_red)) + 
  scale_shape_manual(values = c(21,22))+
  stat_pointinterval(aes(y = as.numeric(`Assigned Group (g)`=='B')[`Assigned Group (g)`=='B']+.95,
                        x = `Observed Score (Y)`[`Assigned Group (g)`=='B'],
                        shape = `Actual Group`[`Assigned Group (g)`=='B'],
                        fill = `Actual Group`[`Assigned Group (g)`=='B'],
                        color = `Actual Group`[`Assigned Group (g)`=='B'],
                        order = ord[`Assigned Group (g)`=='B'],
                        group = NA),
                     shape = 16,point_size = 3.2,
                     point_color = main_color_blue,
                     interval_color = main_color_blue) +
  stat_pointinterval(aes(y = as.numeric(`Assigned Group (g)`=='B')[`Assigned Group (g)`=='A']+.95,
                        x = `Observed Score (Y)`[`Assigned Group (g)`=='A'],
                        shape = `Actual Group`[`Assigned Group (g)`=='A'],
                        fill = `Actual Group`[`Assigned Group (g)`=='A'],
                        color = `Actual Group`[`Assigned Group (g)`=='A'],
                        order = ord[`Assigned Group (g)`=='A'],
                        group = NA),
                     shape = 15,point_size = 3.2,
                     point_color = main_color_red,
                     interval_color =main_color_red) + 
  annotate('text',x = median(`Observed Score (Y)`[`Assigned Group (g)` == 'B']),
           y = 1.85,
           label = round(median(`Observed Score (Y)`[`Assigned Group (g)` == 'B']),1),
           color=text_color_blue) +
  annotate('text',x = median(`Observed Score (Y)`[`Assigned Group (g)` == 'A']),
           y = 0.85,
           label = round(median(`Observed Score (Y)`[`Assigned Group (g)` == 'A']),1),
           color=text_color_red) +
  annotate('text',
           x = 8,
           y = 2.65,
           label = TeX(paste0('$d_{gY}$ = ',d)) ,
           color="black", hjust='left') +
  annotate('text',
           x = 8,
           y = 2.85,
           label = TeX("$\\delta_{GY}$ = 1.00"),
           color="black", hjust='left') +
  ggtitle("Group Difference", subtitle = "No Misclassification")

```

If some individuals are assigned to the incorrect group, then we will see attenuation in the standardized mean difference as the means converge. @fig-mis is showing what happens when the group misclassification rate is 10%. A group misclassification rate of 10% is equivalent to a classification reliability of $r_{gg'}=.64$.

```{r, echo = FALSE,warning=FALSE,fig.height=5}
#| id: fig-mis
#| fig-cap: Distributions of scores with a 10% misclassification rate. Observed standardized mean differences are biased toward the null (i.e., $\delta$ = 0). Red squares denote actual group $A$ members, blue circles denote actual group $B$ members. Note that a few members of group $A$ (red squares) are within assigned group $B$ and vice versa (indicative of misclassification). 

set.seed(25)
n = 50
P_mis = 5

`Observed Score (Y)` = c(rnorm(n,30,8) , rnorm(n,38,8))

`Assigned Group (g)` = c(rep('B',P_mis) , rep('A',n-P_mis),
                    rep('A',P_mis), rep('B',n-P_mis))
ord = c(rep(4,P_mis),rep(1,n-P_mis),
        rep(3,P_mis),rep(2,n-P_mis))
`Actual Group` =   c(rep('B',n) , rep('A',n))
DIFF = (mean(`Observed Score (Y)`[`Assigned Group (g)`=='B'])-mean(`Observed Score (Y)`[`Assigned Group (g)`=='A']))
SD = (sd(`Observed Score (Y)`[`Assigned Group (g)` == 'A']) + sd(`Observed Score (Y)`[`Assigned Group (g)` == 'B']))/2
d = round(DIFF / SD,2)
ggplot(data = NULL, aes(y = `Assigned Group (g)`,
                        x = `Observed Score (Y)`,
                        shape = `Actual Group`,
                        fill = `Actual Group`,
                        color = `Actual Group`,
                        order = ord,
                        group = NA)) +
  geom_dots(side = "top", scale = 0.75,stackratio = 1.1,dotsize=.9, 
            position = "dodge",linewidth = 1) + 
  theme_ggdist() + 
  theme(aspect.ratio = 1,
        axis.text.y = element_text(size=14),
        axis.text.x = element_text(size=13),
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
        title = element_text(size=13),
        legend.position = "none") +  
  scale_fill_manual(values = c(main_color_blue,main_color_red)) + 
  scale_color_manual(values = c(main_color_blue,main_color_red)) + 
  scale_shape_manual(values = c(21,22))+
  stat_pointinterval(aes(y = as.numeric(`Assigned Group (g)`=='B')[`Assigned Group (g)`=='B']+.95,
                        x = `Observed Score (Y)`[`Assigned Group (g)`=='B'],
                        shape = `Actual Group`[`Assigned Group (g)`=='B'],
                        fill = `Actual Group`[`Assigned Group (g)`=='B'],
                        color = `Actual Group`[`Assigned Group (g)`=='B'],
                        order = ord[`Assigned Group (g)`=='B'],
                        group = NA),
                     shape = 16,point_size = 3.2,
                     point_color = main_color_blue,
                     interval_color = main_color_blue) +
  stat_pointinterval(aes(y = as.numeric(`Assigned Group (g)`=='B')[`Assigned Group (g)`=='A']+.95,
                        x = `Observed Score (Y)`[`Assigned Group (g)`=='A'],
                        shape = `Actual Group`[`Assigned Group (g)`=='A'],
                        fill = `Actual Group`[`Assigned Group (g)`=='A'],
                        color = `Actual Group`[`Assigned Group (g)`=='A'],
                        order = ord[`Assigned Group (g)`=='A'],
                        group = NA),
                     shape = 15,point_size = 3.2,
                     point_color = main_color_red,
                     interval_color =main_color_red) + 
  annotate('text',x = median(`Observed Score (Y)`[`Assigned Group (g)` == 'B']),
           y = 1.85,
           label = round(median(`Observed Score (Y)`[`Assigned Group (g)` == 'B']),1),
           color=text_color_blue) +
  annotate('text',x = median(`Observed Score (Y)`[`Assigned Group (g)` == 'A']),
           y = 0.85,
           label = round(median(`Observed Score (Y)`[`Assigned Group (g)` == 'A']),1),
           color=text_color_red) +
  annotate('text',
           x = 8,
           y = 2.65,
           label = TeX(paste0('$d_{gY}$ = ',d)) ,
           color="black", hjust='left') +
  annotate('text',
           x = 8,
           y = 2.85,
           label = TeX("$\\delta_{GY}$ = 1.00"),
           color="black", hjust='left') +
  ggtitle("Group Difference", subtitle = "10% misclassification rate")

```

It is important to note that for many of the corrections converting the standardized mean difference to a point-biserial correlation is often a necessary step. However, once the corrected point-biserial correlation is obtained, the correlation can then be converted back into a standardized mean difference. To correct for bias induced by misclassification we first need to convert the observed standardized mean difference to a point-biserial correlation coefficient by using the observed proportion of the sample that has been assigned to either group $A$ or group $B$ ($p_g$). The group proportion $p_g$ in the following equations will only show up in the term $p_g(1-p_g)$ so it will not matter which group is used to calculate the proportion. Converting $d_{gY}$ to $r_{gY}$:

$$
r_{gY} = \frac{d_{gY}}{\sqrt{\frac{1}{p_g(1-p_g)}-d_{gY}^2}}.
$$

We can then correct the point-biserial correlation for group misclassification by dividing by the square root of the classification reliability. Since we also want to correct for measurement error in the continuous dependent variable, $Y$, we can simultaneously apply the correction for unreliability:

$$
r_{GU} = \frac{r_{gY}}{\sqrt{r_{gg'}}\sqrt{r_{YY'}}}.
$$

Now we can convert the corrected point-biserial correlation into a corrected standardized mean difference ($d_{GU}$). When converting back to a standardized mean difference, we need to use the true group proportions, $p_G$. Although if we are to assume equal misclassification rates between groups, then the observed proportion can be used $p_g$:

$$
d_{GU} = \frac{r_{GU}}{\sqrt{p_G\left(1-p_G\right)\left(1-r_{GU}^2\right)}}.
$$

This process of converting, correcting, and then back-converting must also be done for the standard error. To avoid redundancy, we can incorporate each step into a single equation:

$$
\small{se(d_{GU}) = \frac {se(d_{gY})\times r_{GU}} {r_{gY}\sqrt{\left(1+d_{gY}^2p[1-p]\right)^2\left(d_{gY}^2+\frac{1}{p_g(1-p_g)}\right)p_G(1-p_G)(1-r_{GU}^2)^3}}.}
$$

::: {.callout-note appearance="default" icon="false"}
## Applied Example in R

A researcher wants to compare the academic performance (measured by a standardized test) of people with and without Major Depressive Disorder (MDD). The researcher reports a classification reliability of $r_{gg'}=.80$ and a reliability of the standardized test as $r_{YY'}=.85$. The researcher than finds a standardized mean difference of $d_{gY}=.30$ favoring controls (i.e., controls had a higher average on the test). Using the `correct_d` function in the `psychmeta` package, we can obtain an unbiased estimate of the target standardized mean difference.

```{r,message=FALSE}
library(psychmeta)

correct_d(correction = "meas",
          d = .30,   # observed standardized mean difference
          ryy = .85, # reliability of dependent variable
          rGg = sqrt(.80), # sqrt of classification reliability
          n1 = 100, # sample size in controls
          n2 = 100) # sample size in people with MDD
```

The corrected standardized mean difference is $d_{GU}=0.37\, [0.02,\, .73]$.
:::


```{=html}
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="matthewbjane" data-description="Support me on Buy me a coffee!" data-message="Thank you for being here! Consider buying me a coffee!" data-color="#eeb4d7ff" data-position="Right" data-x_margin="18" data-y_margin="18"></script>
```
