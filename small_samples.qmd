# Small Samples

```{r color_scheme,echo = F,warning=F,message=F}

library(raster)
library(metR)
library(isoband)
library(latex2exp)
library(extrafont)
library(ggplot2)
library(ggdist)
library(patchwork)
library(MASS)


text_color_blue      = '#326982ff'
panel_color_blue     = '#f6fafbff'
lightmain_color_blue = '#a4cdd9ff'
main_color_blue      = '#5fa6bcff'
darkmain_color_blue  = '#397689ff'
border_color_blue    = '#5fa6bcff'

text_color_red       = '#a62675ff'
panel_color_red      = '#fdf6faff'
lightmain_color_red  = '#eeb4d7ff'
main_color_red       = '#d74ea2ff'
darkmain_color_red   = '#bf2986ff'
border_color_red     = '#d74ea2ff'




th_blue <- theme(aspect.ratio = 1,
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(color = text_color_blue),
        panel.background = element_rect(fill = panel_color_blue),
        panel.border = element_rect(fill = NA, color = border_color_blue,linewidth=1.2),
        axis.title = element_text(size=15, color = text_color_blue),
        axis.text.x = element_text(size=13, color = text_color_blue),
        axis.text.y = element_text(size=13, color = text_color_blue),
        text = element_text(size=13, color = text_color_blue),
        axis.ticks = element_line(color = border_color_blue,linewidth=1)) 
  
th_red <- theme(aspect.ratio = 1,
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(color = text_color_red),
        panel.background = element_rect(fill = panel_color_red),
        panel.border = element_rect(fill = NA, color = border_color_red,linewidth=1.2),
        axis.title = element_text(size=15, color = text_color_red),
        axis.text.x = element_text(size=13, color = text_color_red),
        axis.text.y = element_text(size=13, color = text_color_red),
        text = element_text(size=13, color = text_color_red),
        axis.ticks = element_line(color = border_color_red,linewidth=1)) 
```

## Introduction

The purpose of sample statistics is to draw meaningful inferences about the population. However, effect size estimators such as Pearson's correlation coefficient and Cohen's $d$ are biased in small sample sizes. This small sample bias is an artifact and can be adjusted with the appropriate correction factor.

## When Correcting alongside other Artifacts

The small sample bias should always be corrected for prior to applying any other artifact correction. It is independent of all other artifact corrections and therefore the corrected effect sizes in this section can be treated as the uncorrected effect sizes in other sections.

## Correcting Standardized Mean Differences for Small Sample Bias

### Defining the Target Quantity

Our quantity of interest is the population standardized mean difference, $\delta$, between groups $A$ and $B$. We can model the relationship between the population standardized mean difference and the estimate ($d$),

$$
d = a\delta+e.
$$

Where $a$ is an attenuation/inflation factor and $e$ is our sampling error term. Ultimately, we can obtain an unbiased estimate of the population standardized mean difference by correcting the sample standardized mean difference as follows,

$$
d_c = \frac{d}{\hat{a}}.
$$

### Artifact Correction for *d*

As the sample size approaches infinity, Cohen's estimator of the standardized mean difference is unbiased [@hedges1981; @cohen1977]. However, in small sample sizes Cohen's estimator is inflated, that is, on average, it overestimates the population standardized mean difference. To see why this is the case, we can first define the population standardized mean difference between group $A$ and group $B$ such that,

$$
\delta = \frac{\mu_A-\mu_B}{\sigma}.
$$

Where $\bar{y}_A$ and $\bar{y}_B$ are the observed arithmetic means of group $A$ and group $B$, respectively. A sample estimate of the standardized mean difference is,

$$
d = \frac{\overline{Y}_A-\overline{Y}_B}{S_p}
$$ {#eq-d}

Where $S_p$ is the pooled standard deviation (i.e., weighted average within-group standard deviation). The estimator, $d$, is an asymptotically unbiased estimate of $\delta$. We can denote this asymptotic relationship as,

$$
\mathbb{E}_i[d_i] \overset{_n}{\asymp} \delta.
$$

However, $d$ is a biased estimator of $\delta$ when the sample size is finite. Particularly, the smaller the sample size, the larger the bias. We can see that in @fig-d-bias, $d$ tends to over-estimate $\delta$. Therefore, we can apply an artifact inflation factor, $a$, to capture this over-estimation,

$$
\mathbb{E}_i[d_i] = a\delta.
$$

The reason for this bias in $d$ values is two-fold:

-   Standard deviations tend to be attenuated in small sample sizes. This is due to the fact that although variance (squared standard deviation) is an unbiased estimator of the population variance, the square root of the variance (i.e., the standard deviation, $S_p$) is a biased estimate of the population standard deviation [$\mathbb{E}_i[\sqrt{S_{p_i}}] \neq \sqrt{\sigma}$, @holtzman1950].
-   A ratio is biased in small sample sizes [@vankempen2000], therefore the ratio between the mean difference and the standard deviation (see @eq-d) will likewise be biased.

To obtain an unbiased estimate of the population standardized mean difference, we need to first estimate the artifact inflation factor, $a$. In this case, the artifact inflation factor has been mathematically derived previously by @hedges1989. For other types of artifacts, $a$ is unknown in practice and must be estimated, however, for small sample bias the exact value of $a$ is known. The precise value of $a$ is a function of sample size [equation 6e, @hedges1989],

$$
a = \frac{\Gamma\left(\frac{n-3}{2}\right)\sqrt{\frac{n-2}{2}}}{\Gamma\left(\frac{n-2}{2}\right)}.
$$ Where $\Gamma(\cdot)$ denotes the gamma function. The gamma function is factorial function generalized to non-integers [note that a factorial function on integers would look something like: $3! = 3 \cdot 2 \cdot 1$, @marco2021]. There is also an approximation of $a$ that is more computationally trivial [re-arrangement of the first formula on pp. 114, @hedges1989]:

$$
a \approx \frac{4n-9}{4n-12}
$$

However, with the advent of computers, this approximation formula is unnecessary. We can see in @fig-r-bias that there is notable bias when sample size is below 20. Furthermore, the bias is most pronounced when the sample $d$ value is larger (there is no bias at $d=0$).

```{r,echo=FALSE}
#| id: fig-d-bias
#| fig-cap: Plot showing the bias in the standardized mean difference computed in small samples. The X-axis is the sample size ($n$, the vertical bars are indicative of each integer). The Y-axis is the the estimated standardized mean difference ($d$). The dark pink coloring indicates more bias. 
n <- 5:40
d <- seq(0,1.0,length.out=100)

sims <- expand.grid(n,d)
a <- (sqrt((sims$Var1-2)/2) * gamma((sims$Var1-3)/2)/gamma((sims$Var1-2)/2))


ggplot(data = NULL, aes(x = sims$Var1, 
                        y = sims$Var2,
                        fill=sims$Var2-sims$Var2/a,
                        color=sims$Var2-sims$Var2/a)) +
    geom_point(shape = 15,size=2.3) +
    scale_fill_gradient2(high = main_color_red, 
                         low = 'white',
                         midpoint = .15,
                         mid = main_color_red,
                         limits=c(0,.3),
                         breaks = c(0,.3)) +
    scale_color_gradient2(high = main_color_red, 
                         low = 'white',
                         midpoint = .15,
                        mid = main_color_red,
                         limits=c(0,.3),
                         breaks = c(0,.3)) +
    scale_x_continuous(limits = c(4,40), expand = c(0, 0),
                       labels = seq(0,40,by=5),
                       breaks = seq(0,40,by=5)) +
    scale_y_continuous(limits = c(0,1), expand = c(0, 0),
                       labels = c('0','','.20','','.40','','.60','','.80','','1.00'),
                       breaks = seq(0,1,by=.1)) +
  xlab(TeX("Sample Size (n)"))+
  ylab(TeX("Estimated SMD (d)")) + 
  th_red + theme(aspect.ratio = 1,panel.background = element_rect(fill='white')) +
  ggtitle('Small Sample Bias in d values', 
          subtitle = TeX("$bias = \\it{E}[d_i]-\\delta$")) +
  labs(fill = "bias",col = "bias")


```

Using $a$, we can correct the $d$ value such that,

$$
d_c = \frac{d}{a} = \frac{d}{ \left[\frac{\Gamma\left(\frac{n-3}{2}\right)\sqrt{\frac{n-2}{2}}}{\Gamma\left(\frac{n-2}{2}\right)}\right]}.
$$ {#eq-d-corr}

To obtain the standard error of $d_c$ we can apply the same correction as above to the standard error of $d$ ($se$),

$$
se(d_c) = \frac{se(d)}{a} = \frac{se(d)}{ \left[\frac{\Gamma\left(\frac{n-3}{2}\right)\sqrt{\frac{n-2}{2}}}{\Gamma\left(\frac{n-2}{2}\right)}\right]}.
$$ {#eq-se-corr-d}

::: {.callout-note appearance="default" icon="false"}
## Applied Example in R

Let's say we want to compute the mean difference of test scores between two classes. Class 1 has 11 students and class 2 has 10 students. We then obtain a standardized mean difference between the two classes of $d=.43$. In R, we can use the `correct_d_bias()` function to correct the point estimate (using @eq-d-corr) and then `var_error_g` function to correct the error variance and thus the standard error (using @eq-se-corr-d). Both of these functions are found in the `psychmeta` package [@dahlke2019].

```{r,message=FALSE}
library(psychmeta)

d <- .43
n1 <- 11
n2 <- 10

# correct
dc <- correct_d_bias(d = d,
                     n = n1+n2)

var_corrected <- var_error_g(g = dc, 
                             n1 = n1, 
                             n2 = n2)

# print results
cbind(dc=dc, se = sqrt(var_corrected))
```

The output shows a corrected standardized mean difference of $d_c = 0.41$ ($se(d_c) = 0.42$)
:::

## Correcting for Small Sample Bias in Correlations

### Defining the Estimand

Our quantity of interest is the population correlation, $\rho$. We can model the relationship between the population correlation and our sample estimate ($r$) with,

$$
r = a\rho+e
$$

Where $a$ is our small sample biasing factor and $e$ is our sampling error term. Ultimately, we can obtain an unbiased estimate of the population correlation by correcting the observed correlation as follows,

$$
r_c = \frac{r}{a}
$$

### Artifact Correction for *r*

Let's first define the correlation in the population as the covariance between $X$ and $Y$ ($\sigma_{XY}$) standardized by the product of the standard deviation of $X$ ($\sigma_X$) and $Y$ ($\sigma_Y$):

$$
\rho = \frac{\sigma_{XY}}{\sigma_{X}\sigma_Y}
$$

The sample estimate can be defined as ($S$ denoting the sample estimates of $\sigma$),

$$
r = \frac{S_{XY}}{S_{X}S_Y}
$$

Asymptotically, the expectation of a sample correlation is equal to the population correlation,

$$
\mathbb{E}_i[r_i] \overset{_n}{\asymp} \rho.
$$

However, similar to standardized mean differences, correlations are also biased in small sample sizes [@olkin1958]. Unlike standardized mean differences, Pearson correlations are *under*-estimated, rather than over-estimated. Therefore, an artifact *attenuation* factor, $a$, can account for this bias,

$$
\mathbb{E}_i[r_i] = a\rho.
$$

Because the attenuation factor, $a$, has been mathematically derived directly Olkin and Pratt [equation 2.3, -@olkin1958], there is no need to obtain a noisy estimate of $a$. Therefore $a$ can be defined exactly as,

$$
a = \frac{1}{F\left(\frac{1}{2},\frac{1}{2};\frac{n-1}{2};1-r^2\right)}
$$ {#eq-a-r}

Where $F(\cdot)$ is the hypergeometric function (for background see the Wikipedia page: ). The hypergeometric function is a complicated and iterative function which which can be defined in terms of $\Gamma(\cdot)$ functions [plugging in values into equation 2.2, @olkin1958]

$$
F\left(\frac{1}{2},\frac{1}{2};\frac{n-1}{2};1-r^2\right) = \sum^{\infty}_{z=0} \frac{\Gamma\left(\frac{1}{2} + z\right)\Gamma\left(\frac{n-1}{2}\right)\left(1-r^2\right)^z}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{n-1}{2}+z\right)z!}
$$

Although this formula is complex, it can be easily done in R so do not worry! There is also an approximation of $a$ that is much simpler than [@eq-a-r; simplified from the reciprocal of equation 2.7, @olkin1958],

$$
a \approx \frac{2(n-3)}{2n-r^2-5}
$$

Then we can correct the point-estimate the sampling variance for small sample bias. I will emphasize that approximations are not necessary if a computer is available. We can see in @fig-r-bias that there is notable bias when sample size is below 15. Furthermore, the bias is most pronounced when the sample correlation around .60 (there is no bias at $r=0$ and $r=\pm 1$).

```{r,echo=FALSE}
#| id: fig-r-bias
#| fig-cap: Plot showing the bias in the correlations computed in small samples. The X-axis is the sample size ($n$, the vertical bars are indicative of each integer). The Y-axis is the the estimated Pearson correlation ($r$). The dark pink coloring indicates more bias. 

n <- 5:40
r <- seq(0,1,length.out=100)

sims <- expand.grid(n,r)
a <- 2*(sims$Var1-3)/(2*sims$Var1-sims$Var2^2-5)


ggplot(data = NULL, aes(x = sims$Var1, 
                        y = sims$Var2,
                        fill=sims$Var2-sims$Var2/a,
                        color=sims$Var2-sims$Var2/a)) +
    geom_point(shape = 15,size=2.3) +
    scale_fill_gradient(low = main_color_red, 
                        high = 'white',
                         limits=c(-.1,0),
                         breaks = c(-.1,0)) +
    scale_color_gradient(low = main_color_red, 
                         high = 'white',
                        limits=c(-.1,0),
                         breaks = c(-.1,0)) +
    scale_x_continuous(limits = c(4,40), expand = c(0, 0),
                       labels = seq(0,40,by=5),
                       breaks = seq(0,40,by=5)) +
    scale_y_continuous(limits = c(0,1), expand = c(0, 0),
                       labels = c('0','','.20','','.40','','.60','','.80','','1.00'),
                       breaks = seq(0,1,by=.1)) +
  xlab(TeX("Sample Size (n)"))+
  ylab(TeX("Estimated Correlation (r)")) + 
  th_red + theme(aspect.ratio = 1,panel.background = element_rect(fill='white')) +
  ggtitle('Small Sample Bias in Correlations', 
          subtitle = TeX("$bias = \\it{E}[r_i]-\\rho$")) +
  labs(fill = "bias",col = "bias")


```

To correct for small sample bias, we can divide the sample correlation $r$ by the attenuation factor $a$,

$$
r_c = \frac{r}{a} = \frac{r}{\left[\frac{1}{F\left(\frac{1}{2},\frac{1}{2};\frac{n-1}{2};1-r^2\right)}\right]}
$$ {#eq-r-corr}

Where the standard error of $r_c$ can be obtained by correcting the observed standard error ($se$),

$$
se(r_c) = \frac{se(r)}{a} = \frac{se(r)}{\left[\frac{1}{F\left(\frac{1}{2},\frac{1}{2};\frac{n-1}{2};1-r^2\right)}\right]}
$$ {#eq-se-corr}

::: {.callout-note appearance="default" icon="false"}
## Applied Example in R

Let's say we compute a correlation of $r=.43$ between test scores and self-reported motivation in a small sample of students. In R, we can use the `escalc` function in the `metafor` package [@viechtbauer2010]. Setting `measure = "UCOR"` will apply the small sample correction for the point estimate (@eq-r-corr) and the error variance (i.e., square of the standard error, @eq-se-corr). Note in order to run this function you must have the `gsl` package already installed.

```{r,message=FALSE}
# install.packages('gsl')
library(metafor)

r <- .43
n <- 11

# correct the correlation
escalc(measure = 'UCOR',
       ri = r,
       ni = n,
       var.names = c("rc", "se.2"),
       digits = 3)
```

The output shows a corrected correlation between test scores and motivation of $r_c=.454$ ($se(r)^2=.063$).
:::



```{=html}
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="matthewbjane" data-description="Support me on Buy me a coffee!" data-message="Thank you for being here! Consider buying me a coffee!" data-color="#eeb4d7ff" data-position="Right" data-x_margin="18" data-y_margin="18"></script>
```